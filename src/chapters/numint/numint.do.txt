###### Content provided under a Creative Commons Attribution license, CC-BY 4.0; code under MIT License. (c)2018 Aksel Hiorth

======= Algorithmic thinking =======

The only way to improve in coding and algorithmic thinking is practice. The concept of one dimensional numerical integration is easy to understand, i.e. to calculate the area under a curve. In this chapter we will implement several numerical methods, and it will serve as a very simple playground that illustrates the key aspects of numerical modeling

o We start with a mathematical model (in this case an integral)
o The mathematical model is formulated in discrete form 
o Then we design an algorithm to solve the model 
o The numerical solution for a test case is compared with the true solution (could be an analytical solution or data)
o Error analysis: we investigate the accuracy of the algorithm by changing the number of iterations and/or make changes to the implementation or algorithm

The main point of this chapter is not to develop your own integration methods, the built in methods in Scipy will work in most cases. However, the way to break down the main task of calculating an integral into smaller tasks that is understandable by a computer, may work as a template for many different problems you would typically solve using a computer. A second motivation is that by analyzing the origin of numerical errors gives ideas for improving the algorithm, which is transferable to other problems.      

===== The midpoint rule =====
idx{midpoint method}
Numerical integration is encountered in numerous applications in physics and engineering sciences. 
Let us first consider the most simple case, a function $f(x)$, which is a function of one variable, $x$. The most straight forward way of calculating the area $\int_a^bf(x)dx$ is 
simply to divide the area under the function into $N$ equal rectangular slices with size $h=(b-a)/N$, as illustrated in figure ref{fig:numint:mid}. The area of one box is:
!bt
\begin{equation}
M(x_k,x_k+h)=f(x_k+\frac{h}{2}) h,\label{eq:numint:mid0}
\end{equation}
!et
and the area of all the boxes is:
!bt
\begin{align}
I(a,b)&=\int_a^bf(x)dx\simeq\sum_{k=0}^{N-1}M(x_k,x_k+h)\nonumber\\
&=h\sum_{k=0}^{N-1}f(x_k+\frac{h}{2})=h\sum_{k=0}^{N-1}f(a+(k+\frac{1}{2})h).
\label{eq:numint:mid1}
\end{align}
!et
Note that the sum goes from $k=0,1,\ldots,N-1$, a total of $N$ elements. We could have chosen to let the sum go from $k=1,2,\ldots,N$. 
In Python, C, C++ and many other programming languages the arrays start by indexing the elements from $0,1,\ldots$ to $N-1$, 
therefore we choose the convention of having the first element to start at $k=0$.

FIGURE: [fig-numint/func_sq.png, width=800] Integrating a function with the midpoint rule. label{fig:numint:mid}

Below is a Python code, where this algorithm is implemented for $\int_0^\pi\sin (x)dx$
@@@CODE src-numint/midpoint.py fromto: import@#%%

!bnotice
In the implementation above, we have taken advantage of Numpys ability to pass a vector to a function. This greatly enhances the speed and makes clean, readable code. If you were coding in a lower level programming language like Fortran, C or C++, you would probably implement the loop like (in Python syntax):
!bc pycod
for k in range(0,N): # loop over k=0,1,..,N-1
    val = lower_limit+(k+0.5)*h # midpoint value
    area += func(val)
return area*h
!ec
!enotice


% if FORMAT == 'ipynb':
By increasing $N$ the numerical result will get closer to the true answer. How much do you need to increase $N$ in order to reach an accuracy higher than $10^{-8}$.
 What happens when $N$ increases?
% endif

===== The trapezoidal rule =====
idx{trapezoidal method}
The numerical error in the above example is quite low, only about 2$\%$ for $N=5$. 
However, by just looking at the graph above it seems likely that we can develop a better algorithm by using trapezoids instead of rectangles, 
see figure ref{fig:numint:trap}.

FIGURE: [fig-numint/func_tr.png, width=800] Integrating a function with the trapezoidal rule. label{fig:numint:trap}

Earlier we approximated the area using the midpoint value: $f(x_k+h/2)\cdot h$. Now we use $A=A_1+A_2$, where $A_1=f(x_k)\cdot h$ 
and $A_2=(f(x_k+h)-f(x_k))\cdot h/2$, hence the area of one trapezoid is:
!bt
\begin{equation}
A\equiv T(x_k,x_k+h)=(f(x_k+h)+f(x_k))h/2.
\end{equation}
!et
This is the trapezoidal rule, and for the whole interval we get:
!bt
\begin{align}
I(a,b)&=\int_a^bf(x)dx\simeq\frac{1}{2}h\sum_{k=0}^{N-1}\left[f(x_k+h)+f(x_k)\right] \nonumber \\
&=h\left[\frac{1}{2}f(a)+f(a+h) + f(a+2h) +\nonumber\right. \\
&\left.\qquad\cdots + f(a+(N-2)h)+\frac{1}{2}f(b)\right]\nonumber \\
&=h\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=1}^{N-2}f(a+k h)\right].
\end{align}
!et
Note that this formula was bit more involved to derive, but it requires only one more function evaluations compared to the midpoint rule. 
Below is a python implementation:
@@@CODE src-numint/trapez.py fromto: def trap@N=

In the table below, we have calculated the numerical error for various values of $N$.

|--------c-------------------c-------------------c-------------------c----------|
| $N$               | $h$               | Error Midpoint    | Error Trapezoidal |
|--------c-------------------c-------------------c-------------------c----------|
| 1                 | 3.14              | -57\%             | 100\%             |
| 5                 | 0.628             | -1.66\%           | 3.31\%            |
| 10                | 0.314             | -0.412\%          | 0.824\%           |
| 100               | 0.031             | -4.11E-3\%        | 8.22E-3\%         |
|-------------------------------------------------------------------------------|


Note that we get the surprising result that this algorithm performs poorer, a factor of 2 than the midpoint rule.
How can this be explained? By just looking at figure ref{fig:numint:mid}, we see that the midpoint rule actually over predicts the area from $[x_k,x_k+h/2]$ 
 and under predicts in the interval $[x_k+h/2,x_{k+1}]$ or vice versa. The net effect is that for many cases the midpoint rule give a slightly better 
 performance than the trapezoidal rule. In the next section we will investigate this more formally.

===== Numerical errors on integrals =====
idx{numerical integrals, error}
It is important to know the accuracy of the methods we are using, otherwise we do not know if the
computer produce correct results. In the previous examples we were able to estimate the error because we knew the analytical result. However, if we know the 
analytical result there is no reason to use the computer to calculate the result(!). Thus, we need a general method to estimate the error, and let the computer 
run until a desired accuracy is reached. 

In order to analyze the midpoint rule in more detail we approximate the function by a Taylor 
series at the midpoint between $x_k$ and $x_k+h$: 
!bt
\begin{align}
f(x)&=f(x_k+h/2)+f^\prime(x_k+h/2)(x-(x_k+h/2))\nonumber\\ 
&+\frac{1}{2!}f^{\prime\prime}(x_k+h/2)(x-(x_k+h/2))^2+\mathcal{O}(h^3)
\end{align}
!et
Since $f(x_k+h/2)$ and its derivatives are constants it is straight forward to integrate $f(x)$:
!bt
\begin{align}
I(x_k,x_k+h)&=\int_{x_k}^{x_k+h}\left[f(x_k+h/2)+f^\prime(x_k+h/2)(x-(x_k+h/2))\right.\nonumber\\
&\left.+\frac{1}{2!}f^{\prime\prime}(x_k+h/2)(x-(x_k+h/2))^2+\mathcal{O}(h^3)\right]dx
\end{align}
!et
The first term is simply the midpoint rule, to evaluate the two other terms we make the substitution: $u=x-x_k$:
!bt
\begin{align}
I(x_k,x_k+h)&=f(x_k+h/2)\cdot h+f^\prime(x_k+h/2)\int_0^h(u-h/2)du\nonumber\\
&+\frac{1}{2}f^{\prime\prime}(x_k+h/2)\int_0^h(u-h/2)^2du+\mathcal{O}(h^4)\nonumber\\
&=f(x_k+h/2)\cdot h-\frac{h^3}{24}f^{\prime\prime}(x_k+h/2)+\mathcal{O}(h^4).
\end{align}
!et
Note that all the odd terms cancels out, i.e $\int_0^h(u-h/2)^m=0$ for $m=1,3,5\ldots$. Thus the error for the midpoint rule, $E_{M,k}$, on this particular interval is:
!bt
\begin{equation}
E_{M,k}=I(x_k,x_k+h)-f(x_k+h/2)\cdot h=-\frac{h^3}{24}f^{\prime\prime}(x_k+h/2),
\end{equation}
!et
where we have ignored higher order terms. We can easily sum up the error on all the intervals, but clearly $f^{\prime\prime}(x_k+h/2)$ will 
not, in general, have the same value on all intervals. However, an upper bound for the error can be found by replacing $f^{\prime\prime}(x_k+h/2)$ 
with the maximal value on the interval $[a,b]$, $f^{\prime\prime}(\eta)$:
!bt
\begin{align}
E_{M}&=\sum_{k=0}^{N-1}E_{M,k}=-\frac{h^3}{24}\sum_{k=0}^{N-1}f^{\prime\prime}(x_k+h/2)\leq-\frac{Nh^3}{24}f^{\prime\prime}(\eta),\label{eq:numint:em}\\
E_{M}&\leq-\frac{(b-a)^3}{24N^2}f^{\prime\prime}(\eta),
\end{align}
!et
where we have used $h=(b-a)/N$. We can do the exact same analysis for the trapezoidal rule, but then we expand the function around $x_k-h$ instead of the midpoint. 
The error term is then:
!bt
\begin{equation}
E_T=\frac{(b-a)^3}{12N^2}f^{\prime\prime}(\overline{\eta}).
\end{equation}
!et
At the first glance it might look like the midpoint rule always is better than the trapezoidal rule, but note that the second derivative is 
evaluated in different points ($\eta$ and $\overline{\eta}$). Thus it is possible to construct examples where the midpoint rule performs poorer 
than the trapezoidal rule.

Before we end this section we will rewrite the error terms in a more useful form as it is not so easy to evaluate 
$f^{\prime\prime}(\eta)$ (since we do not know which value of $\eta$ to use). By taking a closer look at equation (ref{eq:numint:em}), 
we see that it is closely related to the midpoint rule for $\int_a^bf^{\prime\prime}(x)dx$, hence:
!bt
\begin{align}
E_{M}&=-\frac{h^2}{24}h
\sum_{k=0}^{N-1}f^{\prime\prime}(x_k+h/2)\simeq-\frac{h^2}{24}\int_a^b
f^{\prime\prime}(x)dx\\
E_M&\simeq\frac{h^2}{24}\left[f^\prime(b)-f^\prime(a)\right]=-\frac{(b-a)^2}{24N^2}\left[f^\prime(b)-f^\prime(a)\right]
\end{align}
!et
The corresponding formula for the trapezoid formula is:
!bt
\begin{equation}
E_T\simeq \frac{h^2}{12}\left[f^\prime(b)-f^\prime(a)\right]=\frac{(b-a)^2}{12N^2}\left[f^\prime(b)-f^\prime(a)\right]
\end{equation}
!et
##Now, we can make an algorithm that automatically choose the number of steps to reach (at least) a predefined accuracy:
##@@@CODE src-numint/adaptive_midpoint.py
##!bnotice
##In Python it is sometimes convenient to enter default values for the arguments in a ##function. In the above example, we could also have written the function definition as\\ ##`def int_adaptive_midpoint(func, lower_limit, upper_limit,` \\ `tol=1e-8):`. If the ##`tol` parameter is not given the code will assume an accuracy of $10^{-8}$. 
##!enotice
===== Practical estimation of errors on integrals (Richardson extrapolation) =====
idx{Richardson extrapolation}
label{sec:numint:parct}
From the example above we were able to estimate the number of steps needed to reach (at least) a certain precision. 
In many practical cases we do not deal with functions, but with data and it can be difficult to evaluate the derivative. 
We also saw from the example above that the algorithm gives a higher precision than what we asked for. 
How can we avoid doing too many iterations? A very simple solution to this question is to double the number of intervals until 
a desired accuracy is reached. The following analysis holds for both the trapezoid and midpoint method, because in both cases 
the (global) error scale as $h^2$[^trapez].
[^trapez]: You can do the following analysis by assuming that the local error is $h^3$, but then you need to take into account that you need to take twice as many steps, which will give the same result.

Assume that we have evaluated the integral with a step size $h_1$, and the computed result is $I_1$. 
Then we know that the true integral is $I=I_1+c h_1^2$, where $c$ is a constant that is unknown. If we now half the step size: $h_2=h_1/2$, 
then we get a new (better) estimate of the integral, $I_2$, which is related to the true integral $I$ as: $I=I_2+c h_2^2$. 
Taking the difference between $I_2$ and $I_1$ give us an estimation of the error:
!bt
\begin{equation}
I_2-I_1=I-c h_2^2-(I-ch_1^2)=3c h_2^2,
\end{equation}
!et
where we have used the fact that $h_1=2h_2$, Thus the error term is:
!bt
\begin{equation}
E(a,b)=c h_2^2=\frac{1}{3}(I_2-I_1).
\end{equation}
!et
This might seem like we need to evaluate the integral twice as many times as needed. This is not the case, by choosing to exactly 
half the spacing we only need to evaluate for the values that lies halfway between the original points. We will demonstrate how 
to do this by using the trapezoidal rule, because it operates directly on the $x_k$ values and not the midpoint values. 
The trapezoidal rule can now be written as:
!bt
\begin{align}
I_2(a,b)&=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=1}^{N_2-1}f(a+k h_2)\right],\\
&=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=\text{even values}}^{N_2-1}f(a+k h_2)\right.\nonumber\\
&\left.\qquad+\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2)\right],
\end{align}
!et
in the last equation we have split the sum into odd an even values. The sum over the even values can be rewritten:
!bt
\begin{equation}
\sum_{k=\text{even values}}^{N_2-1}f(a+k h_2)=\sum_{k=0}^{N_1-1}f(a+2k h_2)=\sum_{k=0}^{N_1-1}f(a+k h_1),
\end{equation}
!et
note that $N_2$ is replaced with $N_1=N_2/2$, we can now rewrite $I_2$ as:
!bt
\begin{align}
I_2(a,b)&=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=0}^{N_1-1}f(a+k h_1)\right.\nonumber\\
&\left.+\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2)\right]
\end{align}
!et
Note that the first terms are actually the trapezoidal rule for $I_1$, hence:
!bt
\begin{equation}
I_2(a,b)=\frac{1}{2}I_1(a,b)+h_2\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2).
\end{equation}
!et
The factor $1/2$ in front of $I_1(a,b)$, appears because $h_2=h_1/2$. 
A possible algorithm is then:
o Choose a low number of steps to evaluate the integral, $I_0$, the first time, e.g. $N_0=1$
o Double the number of steps, $N_1=2N_0$ 
o Calculate the missing values by summing over the odd number of steps $\sum_{k=\text{odd values}}^{N_1-1}f(a+k h_1)$
o Check if $E_1(a,b)=\frac{1}{3}(I_1-I_0)$ is lower than a specific tolerance
o If yes quit, if not, return to 2, and continue until $E_i(a,b)=\frac{1}{3}(I_{i+1}-I_{i})$ is lower than the tolerance  

Below is a Python implementation:
@@@CODE src-numint/adaptive_trapez.py fromto:def int_@prec
% if FORMAT == 'ipynb':
What is a good number to start with, what happens if we choose $N_0$ too large? Compare the adaptive midpoint rule with the adaptive 
trapezoidal rule, is it possible to get the same accuracy with the same number of iterations? Check the expected number of 
iterations with the theoretical value $N=\sqrt{\frac{(b-a)^2}{12E_T}\left[f^\prime(b)-f^\prime(a)\right]}$.
% endif

If you compare the number of terms used in the adaptive trapezoidal rule, which was developed by halving the step size, and the adaptive midpoint rule that was derived on the basis of the theoretical error term, you will find the adaptive midpoint rule is more efficient. So why go through all this trouble? In the next section we will see that the development we did for the adaptive trapezoidal rule is closely related to Romberg integration, which is *much* more effective.


 
======= Romberg integration =======
idx{Romberg integration}
The adaptive algorithm for the trapezoidal rule in the previous section can be easily improved by remembering 
that the true integral was given by[^romerr] : $I=I_i+ch_i^2+\mathcal{O}(h^4)$. The error term was in the previous example only used to 
check if the desired tolerance was achieved, but we could also have added it to our estimate of the integral to reach an accuracy to fourth order:

[^romerr]: Note that all odd powers of $h$ is equal to zero, thus the corrections are always in even powers.  

!bt
\begin{equation}
I=I_{i+1}+ch^2+\mathcal{O}(h^4)=I_{i+1}+\frac{1}{3}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^4).
\end{equation}
!et
As before the error term $\mathcal{O}(h^4)$, can be written as: $ch^4$. Now we can proceed as in the previous section: First we estimate the 
integral by one step size $I_i=I+ch_i^4$, next we half the step size $I_{i+1}=I+ch_{i+1}^4$ and use these two estimates to calculate the error term:
!bt
\begin{align}
I_{i+1}-I_{i}&=I-c h_{i+1}^4-(I-ch_i^4)=-c h_{i+1}^4+c(2h_{i+1})^4=15c h_{i+1}^4,\nonumber\\
ch_{i+1}^4&=\frac{1}{15}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^6).
\end{align}
!et
but now we are in the exact situation as before, we have not only the error term but the correction up to order $h^4$ for this integral:
!bt
\begin{equation}
I=I_{i+1}+\frac{1}{15}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^6).\label{eq:numint:rom}
\end{equation}
!et
Each time we half the step size we also gain a higher order accuracy in our numerical algorithm. Thus, there are two iterations going on at the same time; 
one is the iteration that half the step size ($i$), and the other one is the increasing number of higher order terms added (which we will denote $m$). 
We need to improve our notation, and replace the approximation of the integral ($I_i$) with $R_{i,m}$. Equation (ref{eq:numint:rom}), can now 
be written:
!bt
\begin{equation}
I=R_{i+1,2}+\frac{1}{15}\left[R_{i+1,2}-R_{i,2}\right]+\mathcal{O}(h^6).
\end{equation}
!et
A general formula valid for any $m$ can be found by realizing:
!bt
\begin{align}
I&=R_{i+1,m+1}+c_mh_i^{2m+2}+\mathcal{O}(h_i^{2m+4})\label{eq:numint:rom0}\\
I&=R_{i,m+1}+c_mh_{i-1}^{2m+2}+\mathcal{O}(h_{i-1}^{2m+4})\nonumber\\
&=R_{i,m+1}+2^{2m+2}c_mh_{i}^{2m+2}+\mathcal{O}(h_{i-1}^{2m+4}),\label{eq:numint:rom1}
\end{align}
!et
where, as before $h_{i-1}=2h_i$. Subtracting equation (ref{eq:numint:rom0}) and (ref{eq:numint:rom1}), we find an expression for the error term:
!bt
\begin{align}
c_mh_{i}^{2m+2}&=\frac{1}{4^{m+1}-1}(R_{i,m}-R_{i-1,m})\label{eq:numint:rom2}
\end{align}
!et
Then the estimate for the integral in equation (ref{eq:numint:rom1}) is:
!bt
\begin{align}
I&=R_{i,m+1}+\mathcal{O}(h_i^{2m+2})\\
R_{i,m+1}&=R_{i,m}+\frac{1}{4^{m+1}-1}(R_{i+1,m}-R_{i,m}).
\end{align}
!et
A possible algorithm is then:

o Evaluate $R_{0,0}=\frac{1}{2}\left[f(a)+f(b)\right](b-a)$ as the first estimate
o Double the number of steps, $N_{i+1}=2N_i$ or half the step size $h_{i+1}=h_i/2$ 
o Calculate the missing values by summing over the odd number of steps $\sum_{k=\text{odd values}}^{N_1-1}f(a+k h_{i+1})$
o Correct the estimate by adding *all* the higher order error term $R_{i,m+1}=R_{i,m}+\frac{1}{4^m-1}(R_{i+1,m+1}-R_{i,m+1})$
o Check if the error term is lower than a specific tolerance $E_{i,m}(a,b)=\frac{1}{4^{m+1}-1}(R_{i,m}-R_{i-1,m})$, if yes quit, if no goto 2, increase $i$ and $m$ by one
The algorithm is illustrated in figure ref{fig:numint:romberg}.
FIGURE: [fig-numint/romberg.png, width=400 frac=0.5] Illustration of the Romberg algorithm. Note that for each new evaluation of the integral $R_{i,0}$, all the correction terms $R_{i,m}$ (for $m>0$) must be evaluated again. label{fig:numint:romberg}

Note that the tolerance term is not the correct one as it uses the error estimate for the current step, 
which we also use correct the integral in the current step to reach a higher accuracy. 
Thus the error on the integral will always be lower than the user specified tolerance.
Below is a Python implementation:
@@@CODE src-numint/romberg.py fromto: def int_romberg@#end

Note that the Romberg integration only uses 32 function evaluations to reach a precision of $10^{-8}$, whereas the adaptive midpoint and trapezoidal rule in the previous
section uses 20480 and 9069 function evaluations, respectively. 

===== Alternative implementation of adaptive integration =====
Before we proceed, we will consider an alternative implementation of the adaptive method presented in the previous sections, with the following modification
o We will use Simpsons rule (see the exercise at the end), which takes the following form $\int_a^bf(x)dx\simeq\frac{h}{6}\left[f(a)+4f(a+\frac{h}{2})+2f(a+h)+ 4f(a+3\frac{h}{2})+2f(a+2h)+\cdots+f(b)\right]$
o We only divide the intervals needed to reach the desired accuracy.
Simpsons rule is accurate up to $\mathcal{O}(h^4)$, and by following the same arguments as above we can estimate the error as $E_i(a,b)=\frac{1}{15}(I_{i+1}-I_{i})$. The factor 1/15 (as opposed to 1/3) originates from the higher order accuracy. The integration proceeds as follows
* `S` is an empty list
* `S.append([a,b])`
* $I=0$
* `while S not empty do:`
  * `[a,b]=S.pop(-1)`
  * $m=(b+a)/2$
  * $I_1=$ `simpson_step(a,b)`
  * $I_2=$ `simpson_step(a,m)+simpson_step(m,b)`
  * if $|I_1-I_2|<15|b-a|\cdot tol$
    * $I+=I_2$
  * else:
    * `S.append([a,m])`
    * `S.append([m,b])`
  * return $I$
Note the use of the list `S`, we remove the interval $[a,b]$ from the list and calculates the integral. If the integral is not accurate enough we add to new intervals to the list, and continue until we reach the desired accuracy, then we proceed with the next interval. Since we remove (`pop`) the element from the list, we know that we will finish the evaluation once the list is empty. This algorithm allows for different sub interval to have different degrees of subdivisions, contrary to Rombergs algorithm. The full python implementation is shown below
@@@CODE src-numint/adaptive_trapez.py fromto: simpson_step@Area =


======= Gaussian quadrature =======
idx{Gaussian quadrature}
Many of the methods we have looked into are of the type:
!bt
\begin{align}
	\int_a^b f(x) dx = \sum_{k=0}^{N-1} \omega_k f(x_k),\label{eq:numint:qq1}
\end{align}
!et
where the function is evaluated at fixed interval. For the midpoint rule $\omega_k=h$ for all values of $k$, for the trapezoid rule 
$\omega_k=h/2$ for the endpoints and $h$ for all the interior points. 
For the Simpsons rule (see exercise) $\omega_k=h/3, 4h/3,2h/3,4h/3,\ldots,4h/3,h/3$. 
Note that all the methods we have looked at so far samples the function in equal spaced points, $f(a+k h)$, 
for $k=0, 1, 2\ldots, N-1$. If we now allow for the function to be evaluated at unevenly spaced points, we can do a lot better. 
This realization is the basis for Gaussian Quadrature. We will explore this in the following, 
but to make the development easier and less cumbersome, we transform the integral from the domain $[a,b]$ to $[-1,1]$:
!bt
\begin{align}
\int_a^bf(t)dt&=\frac{b-a}{2}\int_{-1}^{1}f(x)dx\text{ , where:}\\
x&=\frac{2}{b-a}t-\frac{b+a}{b-a}.
\end{align}
!et
The factor in front comes from the fact that $dt=(b-a)dx/2$, thus we can develop our algorithms on the domain $[-1,1]$, 
and then do the transformation back using: $t=(b-a)x/2+(b+a)/2$.

!bnotice
The idea we will explore is as follows:
If we can approximate the function to be integrated on the domain $[-1,1]$ (or on $[a,b]$) as a 
polynomial of as *large a degree as possible*, then the numerical integral of this polynomial will be very close to the integral of the 
function we are seeking.
!enotice
This idea is best understood by a couple of examples. Assume that we want to use $N=1$ in equation (ref{eq:numint:qq1}):
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx\simeq\omega_0f(x_0).
\end{equation}
!et
We now choose $f(x)$ to be a polynomial of as large a degree as possible, but with the requirement that the integral is exact. If $f(x)=1$, we get:
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx=\int_{-1}^{1}1\,dx=2=\omega_0,
\end{equation}
!et
hence $\omega_0=2$. If we choose $f(x)=x$, we get:
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx=\int_{-1}^{1}x\,dx=0=\omega_0f(x_0)=2x_0,
\end{equation}
!et
hence $x_0=0$. 
!bnotice The Gaussian integration rule for $N=1$ is:

!bt
\begin{align}
&\int_{-1}^{1}f(x)\,dx\simeq 2f(0)\text{, or: }\nonumber\\
&\int_{a}^{b}f(t)\,dt\simeq\frac{b-a}{2}\,2f(\frac{b+a}{2})=(b-a)f(\frac{b+a}{2}).
\end{align}
!et

!enotice

This equation is equal to the midpoint rule, by choosing $b=a+h$ we reproduce equation (ref{eq:numint:mid0}). If we choose $N=2$:
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx\simeq\omega_0f(x_0)+\omega_1f(x_1),
\end{equation}
!et
we can show that now $ f(x)=1,\,x,\,x^2\,x^3$ can be integrated exact:
!bt
\begin{align}
\int_{-1}^{1}1\,dx&=2=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0+\omega_1\,,\\
\int_{-1}^{1}x\,dx&=0=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0+\omega_1x_1\,,\\
\int_{-1}^{1}x^2\,dx&=\frac{2}{3}=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0^2+\omega_1x_1^2\,,\\
\int_{-1}^{1}x^3\,dx&=0=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0^3+\omega_1x_1^3\,,
\end{align}
!et
hence there are four unknowns and four equations. The solution is: $\omega_0=\omega_1=1$ and $x_0=-x_1=1/\sqrt{3}$.

!bnotice The Gaussian integration rule for $N=2$ is:
!bt
\begin{align}
\int_{-1}^{1}f(x)\,dx&\simeq f(-\frac{1}{\sqrt{3}})+f(\frac{1}{\sqrt{3}})\, \text{, or:}\\
\int_{a}^{b}f(x)\,dx&\simeq \frac{b-a}{2}\left[f(-\frac{b-a}{2}\frac{1}{\sqrt{3}}+\frac{b+a}{2})
+f(\frac{b-a}{2}\frac{1}{\sqrt{3}}+\frac{b+a}{2})\right].
\end{align}
!et
!enotice

@@@CODE src-numint/gaussquad2.py fromto: def int_@a=0

=== The case N=3 ===
For the case $N=3$, we find that $f(x)=1,x,x^2,x^3,x^4,x^5$ can be integrated exactly:
!bt
\begin{align}
\int_{-1}^{1}1\,dx&=2=\omega_0+\omega_1+\omega_2\,,\\
\int_{-1}^{1}x\,dx&=0=\omega_0x_0+\omega_1x_1+\omega_2x_2\,,\\
\int_{-1}^{1}x^2\,dx&=\frac{2}{3}=\omega_0x_0^2+\omega_1x_1^2+\omega_2x_2^2\,,\\
\int_{-1}^{1}x^3\,dx&=0=\omega_0x_0^3+\omega_1x_1^3+\omega_2x_2^3\,,\\
\int_{-1}^{1}x^4\,dx&=\frac{2}{5}=\omega_0x_0^4+\omega_1x_1^4+\omega_2x_2^4\,,\\
\int_{-1}^{1}x^5\,dx&=0=\omega_0x_0^5+\omega_1x_1^5+\omega_2x_2^5\,,
\end{align}
!et
the solution to these equations are $\omega_{0,1,2}=5/9, 8/9, 5/9$ and $x_{1,2,3}=-\sqrt{3/5},0,\sqrt{3/5}$. Below is a Python implementation:
@@@CODE src-numint/gaussquad3.py fromto: def int_@a=0

Note that the Gaussian quadrature converges very fast. From $N=2$ to $N=3$ function evaluation we reduce the error (in this specific case) 
from 6.5% to 0.1%. Our standard trapezoidal formula needs more than 20 function evaluations to achieve this, the Romberg method uses 4-5 function
evaluations. How can this be? If we use the standard Taylor formula for the function to be integrated, we know that for $N=2$ the Taylor 
formula must be integrated up to $x^3$, so the error term is proportional to $h^4f^{(4)}(\xi)$ (where $\xi$ is some x-value in $[a,b]$). 
$h$ is the step size, and we can replace it with $h\sim (b-a)/N$, thus the error scale as $c_N/N^4$ (where $c_N$ is a constant). 
Following the same argument, we find for $N=3$ that the error term is $h^6f^{(6)}(\xi)$ or that the error term scale as $c_N/N^6$. 
Each time we increase $N$ by a factor of one, the error term reduces by $N^2$. Thus if we evaluate the integral for $N=10$, 
increasing to $N=11$ will reduce the error by a factor of $11^2=121$.

===== Error term on Gaussian integration =====
idx{Gaussian quadrature, error term}
The Gaussian integration rule of order $N$ integrates exactly a polynomial of order $2N-1$. 
%if book:
From Taylors error formula, see equation (ref{eq:taylor:error}) in Chapter ref{ch:taylor},
%else:
From Taylors error formula, 
%endif
we can easily see that the error term must be of order $2N$, and be proportional to $f^{(2N)}(\eta)$, see cite{stoer2013} for more details on the derivation of error terms. The drawback with an analytical error term derived from series expansion is that it involves the derivative of the function. As we have already explained, this is very unpractical and it is much more practical to use the methods described in section ref{sec:numint:parct}. Let us consider this in more detail, assume that we evaluate the integral using first a Gaussian integration rule with $N$ points, and then $N+1$ points. Our estimates of the "exact" integral, $I$,  would then be:
!bt
\begin{align}
 I&=I_N+ch_{N}^{2N},label{eq:numint:gerr1}\\
 I&=I_{N+1}+ch_{N+1}^{2N+1}.
label{eq:numint:gerr2}
\end{align}
!et
In principle $h_{N+1}\neq h_{N}$, but in the following we will assume that $h_N\simeq h_{N+1}$, and $h\ll 1$. Subtracting equation (ref{eq:numint:gerr1}) and (ref{eq:numint:gerr2}) we can show that a reasonable estimate for the error term $ch^{2N}$ would be:
!bt
\begin{equation}
ch^N= I_{N+1}-I_N.
\end{equation}
!et
If this estimate is lower than a given tolerance we can be quite confident that the higher order estimate $I_{N+1}$ approximate the true integral within our error estimate. This is the method implemented in SciPy, "`integrate.quadrature`":"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.quadrature.html"

===== Common weight functions for classical Gaussian quadratures =====
##https://pomax.github.io/bezierinfo/legendre-gauss.html
======= Integrating functions over an infinite range  =======
idx{numerical integral, infinite}
Integrating a function over an infinite range can be done by the following trick. Assume that we would like to evaluate
!bt
\begin{equation}
\int_a^\infty f(x) dx.
label{eq:numint:inf}
\end{equation}
!et
If we introduce the following substitution
!bt
\begin{equation}
z=\frac{x-a}{1+x-a},
label{eq:numint:infs}
\end{equation}
or equivalently
\begin{equation}
x=a+\frac{z}{1-z},
label{eq:numint:infs2}
\end{equation}
!et
then if $x=a$, $z=0$, and if $x\to\infty$ then $z\to1$, hence:
!bt
\begin{equation}
\int_a^\infty f(x) dx = \int_0^1 f(a+\frac{z}{1-z}) \frac{dz}{(1-z)^2}.
label{eq:numint:infs3}
\end{equation}
!et
# #ifdef PROGRESS
===== Which method to use in a specific case? (NOT COMPLETED) =====
There are no general answers to this question, and one need to decide from case to case. If computational speed is not an issue, 
and the function to be integrated can be evaluated at any points all the methods above can be used. If the function to be integrated 
is a set of observations at different times, that might be unevenly spaced, I would use the midpoint rule:
!bt
\begin{equation}
I(a,b)=\int_a^bf(x)dx\simeq\sum_{k=0}^{N-1}M(x_k,x_k+h)=\sum_{k=0}^{N-1}h_if(x_k+\frac{h_i}{2})
\end{equation}
!et
This is because we do not know anything about the function between the points, only when it is observed, and the formula uses only 
the information at the observation points. There is a second more subtle reason, and that is the fact that in many cases the 
observations a different times are the {\it average} value of the observable quantity and it those cases the midpoint 
rule would be the exact answer. 

# #endif
##===== Exercises =====

===== Exercise: Numerical Integration =====
!bsubex
Show that for a linear function, $y=a\cdot x+b$ both the trapezoidal rule and the rectangular rule are exact
!esubex
!bsubex
Consider $I(a,b)=\int_a^bf(x)dx$ for $f(x)=x^2$. The analytical result is $I(a,b)=\frac{b^3-a^3}{3}$. Use the Trapezoidal and 
  Midpoint rule to evaluate these integrals and show that the error for the Trapezoidal rule is exactly twice as big as the Midpoint rule.
!esubex
!bsubex
Use the fact that the error term on the trapezoidal rule is twice as big as the midpoint rule to derive Simpsons formula: $I(a,b)=\sum_{k=0}^{N-1}I(x_k,x_k+h)=\frac{h}{6}\left[f(a)+ 4f(a+\frac{h}{2})+2f(a+h)+4f(a+3\frac{h}{2})+2f(a+2h)+\cdots+f(b)\right]$ Hint: $I(x_k,x_k+h)=M(x_k,x_k+h)+E_M$ (midpoint rule) and $I(x_k,x_k+h)=T(x_k,x_k+h)+E_T=T(x_k,x_k+h)-2E_M$ (trapezoidal rule).

!bsol
Simpsons rule is an improvement over the midpoint and trapezoidal rule. It can be derived in different ways, we will make use of 
the results in the previous section. If we assume that the second derivative is reasonably well behaved on the interval $x_k$ 
and $x_k+h$ and fairly constant we can assume that $f^{\prime\prime}(\eta)\simeq f^{\prime\prime}(\overline{\eta})$, hence $E_T=-2E_M$.
!bt
\begin{align}
I(x_k,x_k+h)&=M(x_k,x_k+h)+E_M\text{ (midpoint rule)}\\
I(x_k,x_k+h)&=T(x_k,x_k+h)+E_T\nonumber\\
&=T(x_k,x_k+h)-2E_M\text{ (trapezoidal rule)},
\end{align}
!et
we can now cancel out the error term by multiplying the first equation with 2 and adding the equations:
!bt
\begin{align}
3I(x_k,x_k+h)&=2M(x_k,x_k+h)+T(x_k,x_k+h)\\
&=2f(x_k+\frac{h}{2}) h+\left[f(x_k+h)+f(x_k)\right] \frac{h}{2}\\
I(x_k,x_k+h)&=\frac{h}{6}\left[f(x_k)+4f(x_k+\frac{h}{2})+f(x_k+h)\right].
\end{align}
!et
Now we can do as we did in the case of the trapezoidal rule, sum over all the elements:
!bt
\begin{align}
I(a,b)&=\sum_{k=0}^{N-1}I(x_k,x_k+h)\nonumber\\
&=\frac{h}{6}\left[f(a)+ 4f(a+\frac{h}{2})+2f(a+h)+4f(a+3\frac{h}{2})\right.\nonumber\\
&\left.\qquad+2f(a+2h)+\cdots+f(b)\right]\\
&=\frac{h^\prime}{3}\left[f(a)+ f(b) + 4\sum_{k= \text{odd}}^{N-2}f(a+k h^\prime)+2\sum_{k= \text{even}}^{N-2}f(a+k h^\prime)\right],
\end{align}
!et
note that in the last equation we have changed the step size $h=2h^\prime$.
!esol



!esubex
!bsubex
Show that for $N=2$ ($f(x)=1,x,x^3$), the points and Gaussian quadrature rule for $\int_{0}^{1}x^{1/2}f(x)\,dx$
is $\omega_{0,1}=-\sqrt{70}{150} + 1/3, \sqrt{70}{150} + 1/3$
##=\simeq 0.27755599823106164, 0.38911066843560504$
and $x_{0,1}=-2\sqrt{70}{63} + 5/9, 2\sqrt{70}{63} + 5/9$
##\simeq 0.27755599823106164, 0.38911066843560504$
o Integrate $\int_0^1x^{1/2}\cos x\,dx$ using the rule derived in the exercise above and compare with the standard Gaussian quadrature rule for ($N=2$, and $N=3$).
!esubex
!bsubex
Make a Python program that uses the Midpoint rule to integrate experimental data that are unevenly spaced and given in the form of two arrays.  
!esubex


