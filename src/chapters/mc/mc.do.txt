###### Content provided under a Creative Commons Attribution license, CC-BY 4.0; code under MIT License. (c)2018 Aksel Hiorth

======= Monte Carlo methods =======
Monte Carlo methods are named after the Monte Carlo Casino in Monaco,
this is because at its core it uses random numbers to solve problems.
Monte Carlo methods are quite easy to program, and they are
usually much more intuitive than a theoretical approach. 
# If we would
# like to find the probability to get at least 5 on three dices after 5 throws
# there are methods from statistics that could tell us the probability.
# Using the Monte Carlo method, we would get the computer to pick a
#random integer between 1 and 6, three times, to represent one throw of
#the dices bla bla.
# later in this chapter 
# Usually  Usually we use differential equations to describe physical systems, the solution to these equations are continuous functions. In order for these solutions 
# to be useful, they require that the differential equation describes our physical sufficiently. In many practical cases we have no control over many

# of the parameters entering the differential equation, or stated differently *our system is not deterministic*. This means that there could be some random
# fluctuations, occurring at different times and points in space, that we have no control over. In a practical situation we might would like to investigate how these fluctuations would
# affect the behavior of our system. A 

# o "ray  tracing":"https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-rendering-practical-example"

======= Monte Carlo integration  ''hit and miss'' =======
idx{Monte Carlo integration}
Let us start with a simple illustration of the Monte Carlo Method (MCM), Monte Carlo integration. To the left
in figure ref{fig:mc:mci} there is a shape of a pond. Imagine that we wanted to estimate the area of the pond, how could
we do it? Assume further that you did not have your phone or any other electronic devices to help you. 

FIGURE: [fig-mc/mci, width=400 frac=1.0] Two ponds to illustrate the MCM. label{fig:mc:mci}

One possible approach is: First to walk around it, and put up some bands (illustrated by the black dotted line).
Then estimate the area inside the bands (e.g. 4$\times$3 meters). Then
we would know that the area was less than e.g. 12m$^2$. Finally,
and this is the difficult part, throw rocks *randomly* inside the
bands. The number of rocks hitting the pond divided by the total
number rocks thrown should be equal to the area of the pond divided by
the total area inside the bands, i.e. the area of the pond should be
equal to:
!bt
\begin{equation}
A\simeq\text{Area of rectangle}\times\frac{\text{Number of rocks hitting the pond}}{\text{Number of rocks thrown}}.
label{eq:mc:mci}
\end{equation}
!et
It is important that we throw the rocks randomly, otherwise  equation (ref{eq:mc:mci}) is not correct. Now, let us
investigate this in more detail, and use the idea of throwing rocks to estimate $\pi$. To the right in figure ref{fig:mc:mci},
there is a well known shape, a circle. The area of the circle is $\pi d^2/4$, and the shape is given by $x^2+y^2=d^2/4$. Assume that
the circle is inscribed in a square with sides of $d$. To throw rocks randomly inside the square, is equivalent pick random numbers
with coordinates $(x,y)$, where $x\in[0,d]$ and $y\in[0,d]$. We want all the $x-$ and $y-$values to be chosen with equal probability,
which is equivalent to pick random numbers from a *uniform* distribution. Below is a Python implementation:
@@@CODE src-mc/pi.py fromto:import@NN

In the table below, we have run the code for $d=1$ and different values of $N$. 

|------c--------------c--------------c--------------c-------|
| MC estimate  | Error        | $N$          | $1/\sqrt{N}$ |
|------c--------------c--------------c--------------c-------|
| 3.04         | -0.10159     | 10$^2$       | 0.100        |
| 3.176        | $\,$0.03441  | 10$^3$       | 0.032        |
| 3.1584       | $\,$0.01681  | 10$^4$       | 0.010        |
| 3.14072      | -0.00087     | 10$^5$       | 0.003        |
|-----------------------------------------------------------|

We clearly see that a fair amount of rocks or numbers needs to be used in order to get a good estimate. If you run this code several
times you will see that the results changes from time to time. This
makes sense as the coordinates $x$ and $y$ are chosen at random.

!bnotice A note on performance
The code above is not efficient and for MC simulations we usually have to use many random numbers. Instead of
!bc pypro
for k in range(0,N):
        x=np.random.uniform(0,d)
        y=np.random.uniform(0,d)
!ec
one should avoid loops, and take advantage of build in functions in Numpy
!bc pypro
x=np.random.uniform(0,d,size=N)
y=np.random.uniform(0,d,size=N)
!ec
!enotice
===== Random number generators =====
idx{random number generators}
idx{Mersenne Twister}
There are much to be said about random number generators. The MCM depends on a good random number generator, otherwise we cannot use the results from
statistics to develop our algorithms. Below, we briefly summarize some important points that you should be aware of:

o Random number generators are generally of two types: *hardware random number generator* (HRNG) or *pseudo random number generator* (PRNG).
o HRNG uses a physical process to generate random numbers, this could atmospheric noise, radioactive decay, microscopic fluctuations, which is translated to an electrical signal. The electrical signal is converted to a digital number (1 or 0), by sampling the random signal random numbers can be generated. The HRNG are often named *true random number generators*, and their main use are in *cryptography*.
o PRNG uses a mathematical algorithm to generate an (apparent) random sequence. The algorithm uses an initial number, or a *seed*,  to start the sequence of random number. The sequence is deterministic, and it will generate the same sequence of numbers if the same seed is used. At some point the algorithm will reproduce itself, i.e. it will have certain period. For some seeds the period may be much shorter.
o Many of the PRNG are not considered to be cryptographically secure, because if a sufficiently long sequence of random numbers are generated from them, the rest of the sequence can be predicted. 
o Python uses the "Mersenne Twister":"https://en.wikipedia.org/wiki/Mersenne_Twister" algorithm to generate random numbers, and has a period of $2^{19937}âˆ’1\simeq4.3\cdot10^{6001}$. It is not considered to be cryptographically secure.

In Pythons `random.uniform` function, a random seed is chosen each time the code is run, but
if we set e.g. `random.seed(2)`, the code will generate the same sequence of numbers each time it is called. 

===== Encryption  =====
idx{encryption}
This section can be skipped as it is not relevant for development of
the numerical algorithms, but it is a good place to explain the basic
idea behind encryption of messages. A very simple, but not a very good
encryption, is to replace all the letters in the alphabet with a
number, e.g. A=1, B=2, C=3, etc. This is what is know as a
*substitution cipher*, it does not need to be a number it could be a
letter, a sequence of letters, letters and numbers etc. The receiver
can solve the code by doing the reverse operation.

The
weakness of this approach is that it can fairly easily be cracked, by
the following approach: First we analyze the encrypted message and find the frequency of each of the symbols.
Assume that we know that the message is written in English, then the
frequency of symbols can be compared with the frequency of
letters from a known English text (the most common is `E` (12$\%$), then `T`
(9$\%$), etc.). We would then guess that the most occurring symbol
probably is an `E` or `T`. When some of the letters are in place, we
can compare with the frequency of words, and so on. By the help of
computers this process can easily be automated.

A much better algorithm is *to not replace a letter with the same
symbol*. To make it more clear, consider our simple example where A=1, B=2,
C=3, $\ldots$. If we know say that A=1 but we add a *random number*,
then our code would be much harder to crack. Then the letter A
could be several places in the message but represented as a complete different
number. Thus we could not use the frequency of the various symbols to
crack the message.

How can the receiver decrypt the message? Obviously, it can be done if
both the sender and receiver have the same sequence of random numbers (or the *key*).
This can be achieved quite simple with random number generators, if we
know the seed  used we can generate the same sequence of
random numbers. If Alice where to send a message to Bob without Eve
knowing what it is, Alice and Bob could agree to send a message that
was scrambled using Pythons Mersenne-Twister algorithm with seed=2.

The weakness of this approach is of course that Eve could convince
Alice or Bob to give her the seed or the key. Another possibility is
that Eve could write a program that tested
different random number generators and seeds to decipher the message.
How to avoid this?

Let us assume that Alice and Bob each had their own
hardware random generator. This generator generated random numbers that was truly
random, and the sequence could not be guessed by any outsider. Alice
do not want to share her key (sequence of random numbers) with Bob,
and Bob would not share his key with Alice. How can they send a
message without sharing the key? One possible way of doing it is as
follows: Alice write a message and encrypt it with her key, she send
the message to Bob. Bob then encrypt the message with his key, he
sends it back to Alice. Alice then decrypt the message with her key
and send it back to Bob. Now, Bob can decrypt it with his own key and
read the message. The whole process can be visualized by thinking of
the message as box with the message. Alice but her padlock on the box
(keeps her key for her self), she sends the message to Bob. Bob locks
the box with his padlock, now there are two padlocks on the box. He
sends the box back to Alice, Alice unlocks her padlock with her key,
and sends it back to Bob. The box now only has Bob's key, he can
unlock the box and read the message. The important point is that the
box was never unlocked throughout the transaction, and Alice and Bob
never had to share the key with anyone. 

===== Errors on Monte Carlo integration and the binomial distribution  =====
idx{Monte Carlo Integration, error}
idx{binomial distribution}
How many rocks do we need to throw in order to reach a certain accuracy? To answer this question we need some results from statistics. Our problem of calculating the integral is closely related to the *binomial distribution*. When we throw a rock one of two things can happen i) the rock falls into the water, or ii) it falls outside the pond. If we denote the probability that the rock falls into the pond as $p$, then the probability that it falls outside the pond, $q$, has to be $q=1-p$.
This is simply because there are no other possibilities and the sum of the two probabilities has to be one: $p+q=p+(1-p)=1$. The binomial distribution is given by:
!bt
\begin{equation}
p(k)=\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}.
label{eq:mc:bin}
\end{equation}
!et
$p(k)$ is the probability that an event happens $k$ times after $n$ trials. The mean, $\mu$, and the variance, $\sigma^2$, of the binomial distribution is:
!bt
\begin{align}
\mu&=\sum_{k=0}^{n-1}kp(k)=np, label{eq:mc:binm}\\
\sigma^2&=\sum_{k=0}^{n-1}(k-\mu)^2p(k)=np(1-p). label{eq:mc:bins}
\end{align}
!et
!bnotice Mean and variance
idx{mean}
idx{variance}
The mean of a distribution is simply the *sum* divided by the *count*,
the symbol $\mu$ or $\overline{x}$ is usually used. For $N$ observations, $x_i$,
$\mu=\sum_i x_i/N$. The mean is just an average, it could e.g. be the sum of all the heights of
students in the class divided by the number of students. The mean
would then be the average height of all the students in the class.

The variance is calculated by taking the difference between each of
the data points and the mean, square it, and sum over all data points.
Usually the symbol $\sigma^2$ is used, $\sigma^2=\sum_i(\mu-x_i)^2/N$.
The variance measures the spread in the data. Furthermore, it squares the
distance between the mean and the individual observations, meaning
that the points lying far a way from $\mu$ contributes more to the
variance. 
!enotice
Before we proceed, we should take a moment and look a little more into
the meaning of equation (ref{eq:mc:bin}) to appreciate its usefulness.  A classical example of the use of the binomial formula is to toss a coin, if the coin is fair it will have an equal probability of giving us a head or tail, hence $p=1/2$. Equation (ref{eq:mc:bin}), can answer questions like: ''What is the probability to get only heads after 4 tosses?''. Let us calculate this answer using equation (ref{eq:mc:bin}), the number of tosses is 4, the number of success is 4 (only heads each time)
!bt
\begin{equation}
p(k=4)=\frac{4!}{4!(4-4)!}\frac{1}{2}^4(1-\frac{1}{2})^{4-4}=\frac{1}{2^4}=\frac{1}{16}.
label{eq:mc:coin}
\end{equation}
!et
''What is the probability to get three heads in four tosses?'', using
the same equation, we find:
!bt
\begin{equation}
p(k=3)=\frac{4!}{3!(4-3)!}\frac{1}{2}^3(1-\frac{1}{2})^{4-3}=\frac{4}{2^4}=\frac{1}{4}.
label{eq:mc:coin2}
\end{equation}
!et
In figure ref{fig:mc:coin}, all the possibilities are shown. The
number of possibilities are 16, and there are only one possibility
that we get only heads, i.e. the probability is 1/16 as calculated in
equation (ref{eq:mc:coin}). In the figure we also see that there are 4
possible ways we can get three heads, hence the probability is
4/16=1/4 as calculated in equation (ref{eq:mc:coin2}).

FIGURE: [fig-mc/coin, width=400 frac=1.0] The famous Norwegian Moose coin, and possible outcomes of four coin flips in a row. label{fig:mc:coin}

Now, let us return to our original question, ''What is the error on our
estimate of the integral, when using the MCM?''. Before we continue we
should also clean up our notation, let $I$ be the value of the true
integral, $A$ is our *estimate* of the integral, and $I_N$ is the area
of the rectangle. First, let us show
that the mean or expectation value of the binomial distribution is
related to our estimate of the area of the pond or the circle, $A$. In our case we draw $n=N$
random numbers, and $k$ times the coordinate falls inside the circle,
equation (ref{eq:mc:binm}) tells us that the mean value is $np$. $p$
is the probability that the coordinate is within the area to be
integrated, hence as before $p$ is equal to the area to be integrated
divided by the area of the total domain, thus:
!bt
\begin{equation}
\mu=np=N\frac{A}{I_N},
\end{equation}
!et
or
!bt
\begin{equation}
A=I_N\frac{\mu}{N}.
\end{equation}
!et
Equation (ref{eq:mc:bins}), gives us an estimate of the variance of
the mean value. Assume for simplicity that we can replace $1-p\simeq
p$, this is of course only correct if the area of the rectangle is
twice as big as our pond, but we are only interested in an
estimate of the error, hence $\sigma^2\simeq np^2$. We can now use the
standard deviation as an estimate of the error of our integral:
!bt
\begin{align}
I&\simeq I_N\frac{\mu\pm\sigma}{n}=I_N\frac{Np\pm \sqrt{N}p}{N}\nonumber\\
&\simeq I_N(p\pm \frac{p}{\sqrt{N}})=A\pm \frac{A}{\sqrt{N}}.
label{eq:mc:mcmf}
\end{align}
!et
In the last equation we have replaced $p$ with $A/I_N$. 
Hence, the error of our integral is inversely proportional to the
square root of the number of points. 

===== The mean value method =====
idx{Monte Carlo integration, mean value}
How does our previous method compare with some of our standard methods,
like the midpoint rule? The error for the MC method scales as
$1/\sqrt{N}$, in our previous error estimates we used the step length,
$h$, as an indicator of the accuracy, and not $N$. The s$N$ is
related to the number of points as $h=(b-a)/n$, where $b$ and $a$ are
the integration limit. Thus our MCM scales as $1/\sqrt{n}\sim
h^{1/2}$, this is actually worse than the midpoint or trapezoidal
rule, which scaled as $h$.

The MCM can be improved. We will first describe the mean value method.
In the last section we calculated the area of
a circle by picking random numbers inside a square and estimated the
fraction of points inside the circle. This is equivalent to calculate
the area of a half circle, and multiply with 2:
!bt
\begin{equation}
I=2\int_{-d/2}^{d/2}\sqrt{(d/2)^2-x^2}dx=\frac{\pi d^2}{4}.
label{eq:mc:Is}
\end{equation}
!et
The half-circle is now centered at the origin. Before we proceed we
write our integral in a general form as:
!bt
\begin{equation}
I=\int_a^bf(x)dx.
label{eq:mc:I1}
\end{equation}
!et
Instead of counting the number of points inside the curve given by
$f(x)$, we could instead use the mean of
the function, which we will define as $\overline{f}=\sum_k f(x_k)/N$:
!bt
\begin{equation}
I=\int_a^bf(x)dx\simeq\overline{f}\int_a^bdx=(b-a)\overline{f}
=\frac{(b-a)}{N}\sum_{k=0}^{N-1}f(x_k).
label{eq:mc:I2}
\end{equation}
!et
Note that this formula is similar to the midpoint rule, but now the
function is not evaluated at the midpoint, but at several points and
we use the average value. 

FIGURE: [fig-mc/mcint, width=400 frac=1.0] Illustration of MC integration for $N=4$. label{fig:mc:int}

Below is an implementation:
@@@CODE src-mc/pi2.py fromto: import@N=

In the table below we have compared the mean value method with the
''hit and miss'' method. We see
that the mean value method performs somewhat better, but there are
some random fluctuations and in some cases it performs poorer. 

|---c---------c---------c---------c---------c-----|
| MC-mean | Error   | MC      | Error   | $N$     |
|---c---------c---------c---------c---------c-----|
| 3.1706  | 0.0290  | 3.1600  | 0.0184  | 10$^2$  |
| 3.1375  | -0.0041 | 3.1580  | 0.0164  | 10$^3$  |
| 3.1499  | 0.0083  | 3.1422  | 0.0006  | 10$^4$  |
| 3.1424  | 0.0008  | 3.1457  | 0.0041  | 10$^5$  |
| 3.1414  | -0.0002 | 3.1422  | 0.0006  | 10$^6$  |
|-------------------------------------------------|

We also see that in this case the error scales as $1/\sqrt{N}$.
!bnotice 
At first sight it might be a little counter intuitive that if we
multiply the average value of the function with the size of the
integration domain we get an estimate for the integral, as illustrated
in the top figure in figure ref{fig:mc:int}. A different, but
equivalent way, of viewing the mean value method is the lower figure
in figure ref{fig:mc:int}. For each random point we choose, we
multiply with the area $(b-a)/N$, as $N$ increases the area decreases
and the mean value method approaches the midpoint algorithm. The
reason the mean value method performs poorer is that we do not sample
the function at regular intervals. The "law of large
numbers":"https://en.wikipedia.org/wiki/Law_of_large_numbers", ensures
that our estimate approach the true value of the integral.
!enotice

===== Basic properties of probability distributions =====
The MCM is closely tied to statistics, and it is important to have a
basic understanding of probability density functions (PDF). In
the previous section, we used a random number generator to give us
random numbers in an interval. All the numbers are picked with an
equal probability. Another way to state this is to say that: we *draw*
random numbers from an *uniform* distribution. Thus all the numbers
are drawn with an equal probability $p$. What is the value of $p$?
That value is given from another property of PDF's, all PDF's must
be *normalized* to 1. This is equivalent to state that the sum of all
probabilities must be equal to one. Thus for a general PDF, $p(x)$, we
must have:
!bt
\begin{equation}
\int_{-\infty}^{\infty}p(x)dx=1.
label{eq:mc:pdf1}
\end{equation}
!et
A uniform distribution, $p(x)=U(x)$, is given by:
!bt
\begin{equation}
U(x)=\begin{cases} \frac{1}{b-a}, \text{ for }x\in[a,b]\\
0, \text{ for } x<a \text{ or }x>b,
\end{cases}
label{eq:mc:pdfu}
\end{equation}
!et
you can easily verify that $\int_{-\infty}^{\infty}U(x)=1$. In the MCM
we typically evaluate *expectation values*. The expectation
value, $E[f]$, for a function is defined:
!bt
\begin{equation}
E[f]\equiv\int_{-\infty}^{\infty}f(x)p(x)dx\simeq\frac{1}{N}\sum_{k=0}^{N-1}f(x_k),
label{eq:mc:ef}
\end{equation}
!et
specializing to a uniform distribution, $p(x)=U(x)$, we get:
!bt
\begin{equation}
E[f]=\int_{-\infty}^{\infty}f(x)U(x)dx=\frac{1}{b-a}\int_a^bf(x)dx.
label{eq:mc:efu}
\end{equation}
!et
Rearranging this equation, we see that we can write the above equation
as:
!bt
\begin{equation}
\int_a^bf(x)dx=(b-a)E[f]\simeq(b-a)\frac{1}{N}\sum_{k=0}^{N-1}f(x_k).
label{eq:mc:efu2}
\end{equation}
!et
This equation is the same as equation (ref{eq:mc:I2}), but in the
previous section we never explained why the expectation value of
$f(x_k)$ was equal to the integral. The derivation above shows that
$\int_a^bf(x)dx$ is equal to the expectation value of $f(x)$ 
only under the condition that *we pick numbers from a uniform distribution*.

To make this a bit more clearer, let us specialize to $f(x)=x$. In
this case the expectation value is equal to the mean:
!bt
\begin{equation}
E[x]=\mu=\int_{-\infty}^\infty xp(x)=\frac{1}{N}\sum_kx_k.
label{eq:mc:mean}
\end{equation}
!et
In this case we also find that the numerical error scales as $N^{-1/2}$, from the definition of the variance
!bt
\begin{equation}
\sigma=\sqrt{\frac{1}{N}\sum_k(f(x_i)-\langle f\rangle)^2}\sim\frac{1}{\sqrt{N}},
label{eq:mc:varf}
\end{equation}
!et
!bnotice Why would we or anyone use MC integration?
Monte Carlo integration performs much poorer than any of our previous
methods. So why should we use it, or when should we use it? The
strength of MC integration is only apparent when there is a large
number of dimensions, as we will see in the next section. 
!enotice

===== Example: Monte Carlo integration of a hyper sphere =====
The volume of a hyper sphere is known:
!bt
\begin{equation}
V(R)=\frac{\pi^{D/2}}{\Gamma(D/2+1)}R^D,
label{eq:mc:hyp}
\end{equation}
!et
where $D$ is the number of dimensions $\Gamma(D/2+1)$ is the gamma
function, if $n$ is an integer then $\Gamma(n)=(n-1)!$ and
$\Gamma(n+1/2)=(2n)!/(4^nn!)\sqrt{\pi}$. You can easily verify that
for $D=2,3$, $V(R)=\pi R^2, 4/3\pi R^3$, respectively.

In the case of MC integration, we simply place the sphere inside a cube, and then count
the number of points that hits inside the hyper sphere:

@@@CODE src-mc/hypersphere3.py fromto: def mc_nballII@def mc_nball_sampling

Notice how easy it is to do MC integration in any number of dimensions. If you run the code above for $D=3,\ldots,9$ you will see that the error is the same regardless of dimensions i.e. it only scales proportional to $N^{-1/2}$ and *not the number of dimensions*.  

How can we extend the traditional methods to more dimensions? One way of doing it is
to call a one dimensional integration routine several times. 
The volume of a hypersphere can be written
!bt
\begin{equation}
V(R)=\int_{-R}^{R}\int_{-\sqrt{R^2-x_0^2}}^{\sqrt{R^2-x_0^2}}
\int_{-\sqrt{R^2-x_0^2-x_1^2}}^{\sqrt{R^2-x_0^2-x_1^2}}\cdots
\int_{-\sqrt{R^2-x_0^2-x_1^2-\cdots -x_{n-2}^2}}^{\sqrt{R^2-x_0^2-x_1^2-\cdots -x_{n-2}^2}}dx_0dx_1dx_2\cdots dx_{n-1}.
label{eq:mc:hypV}
\end{equation}
!et
To simplify the notation lets look a little bit closer at $D=3$ ($n=2$). We can always do the last integration
regardless of the number of dimensions
!bt
\begin{equation}
V(R)=2\int_{-R}^{R}\int_{-\sqrt{R^2-x_0^2}}^{\sqrt{R^2-x_0^2}}
\sqrt{R^2-x_0^2-x_1^2}dx_0dx_1.
label{eq:mc:hypV2}
\end{equation}
!et
This equation can be rewritten as
!bt
\begin{align}
V(R)&=2\int_{-R}^{R}F(x_0)dx_0,\nonumber\\
F(x_0)&\equiv\int_{-\sqrt{R^2-x_0^2}}^{\sqrt{R^2-x_0^2}}\sqrt{R^2-x_0^2-x_1^2}dx_1,
label{eq:mc:hypV3}
\end{align}
!et
when integrating $F(x_0)$, we do it by dividing the x-axis from $-R$
to $R$ into $N$ equal slices as before. We also need to evaluate
$F(x_0)$ for each value of $x_0$, which is slightly more tricky, see
figure ref{fig:mc:2Dint} for an illustration. 

FIGURE: [fig-mc/2Dint, width=400 frac=1.0] Illustration of a 2D integration to evaluate the volume of a sphere.label{fig:mc:2Dint}

The multi dimensional
integral is done by placing a box around the sphere, and divide this
box into $N\times N$ equal boxes. If start the integration
at $x=-R$, $F(-R)=0$, because the integrand is zero. If we move one
step to the left, we need to integrate from $y=-R$ to $y=R$. We see
from the figure to the right in figure ref{fig:mc:2Dint} that the
function is not defined for two first points. Thus we need to make
sure that if we are outside the integration bounds the function is
zero. Below is an implementation that uses recursive function calls

@@@CODE src-mc/hypersphere4.py fromto: def dd_trapez@def mc_n

!bwarning Recursive functions
idx{recursive functions}
Recursive implementation is very elegant, and more transparent, but it comes with a price. The reason is that when a function is called additional memory is allocated to store the local variables. If we where to calculate $100!$, 100 copies of the variable $n$ are created, whereas using a loop only one variable is created. Each time a function is called more memory is allocated, and if the recursive calls are too many it might cause memory overflow. If you try to call `fact_rec(1000)`, Python will give an error, because the maximum number of recursions are reached, it can be changed by:
!bc
import sys
sys.setrecursionlimit(1500)
!ec
!ewarning

!bnotice Error Analysis in higher dimensions
In the chapter about numerical integration, we did an error analysis
on the trapezoidal rule and found that it scaled as $h^2$. As we see
from the example above, a higher order integration is simply to do a
series of 1D integrations in all the dimensions, thus the error term
should be $h_{x_0}^2+h_{x_1}^2+\cdots+h_{x_{d-1}}^2$. If we use the
same spatial resolution in all dimensions, then the overall error
scale as $h^2$. If we let $n$ denote the number of points in each
directions, $h\sim 1/n$, the total number of points used is $N=n\times
n\cdots n=n^d$. Thus, the error term scales as $h^2\sim N^{-2/d}$, and
we see that if $d\geq 4$, the MC integration is expected to perform
better. 
!enotice
# #ifdef PROGRESS
======= Importance sampling  =======
idx{importance sampling} 
In many practical examples, MC integration is used to calculate expectation values. When calculating expectation values we usually have to integrate over all possibilities. That means that our integrand is most likely zero or close to zero in a large part of the domain. Consider the evaluation of the integral $\int_{-\infty}^{\infty}\cos(x)e^{-x^2/2} dx$, a plot of the integrand can be seen in figure ref{fig:mc:gauss_imp}. It is clear that the main contribution to the integral is in a small range about $x=0$. To sample the function from $-\infty$ to $\infty$ seems like a waste, it would be much better to sample the function where it gives an important contribution to the integral. This is where *importance sampling* comes into play. In figure ref{fig:mc:gauss_imp} we have also plotted the normal distribution, it has a similar shape as our integrand and we would clearly be better off sampling $x$-values from a normal distribution. How do we achieve this, the key point is equation (ref{eq:mc:ef}). Let us assume that the typical behavior of the integrand is captured by the function $q(x)$, in our example it would be $q(x=)e^{-x^2/2}$ we can then do
!bt
\begin{equation}
\int f(x)q(x)dx=\int f(x)\frac{1}{N}\sum_{k=0}^{N-1}f(x_k).
label{eq:mc:imp}
\end{equation}
!et
Note that the equation above is only exact in the limit of $N\to\infty$. The $x_k$ values have to be chosen from the probability distribution $p(x)$. Mathematically what we are doing is a change of integration variables,
$p(x)dx=dz$. $p(x)$ has to fulfill the basic properties 

would be 
What would be the optimal shape of the function for our MC method? To illustrate what we mean, we can consider a very trivial example a function that is constant over the integration domain, i.e. $f(x)= 1$. Using the MC technique on this function, we only need to sample *a single point* to get the true answer. Clearly, we cannot expect functions to be constant over the integration domain, but we can try to make them *as constant as possible*. Another way of viewing the same problem is to consider the distributions in figure ref{fig:mc:gauss_imp}

FIGURE: [fig-mc/gauss_imp.png, width=400 frac=1.0] A plot of the integrand and the normal distribution. label{fig:mc:gauss_imp}

======= Accept reject (To be completed) =======

======= Example: random walk (To be completed) =======

======= Simulated annealing (To be completed) =======

# #endif 

===== Exercise: The central limit theorem =====
idx{central limit theorem}
label{ex:mc:norm}
##file=solution.pdf
The central limit theorem is a corner stone in statistics, and it is
the reason for why the normal distribution is so widely used. The
central limit theorem states that if we calculate the average of an
independent random variable, the *average will be distributed
according to a normal distribution*. Not that the central limit
theorem does not state anything about the distribution of the original
variable. We will not prove the central limit theorem, but illustrate
it with two examples. 

# !bsol
# !esol

!bsubex
First we will investigate a random variable that follows a *uniform
distribution*. Write a Python function that returns the average of
$N$ uniformly distributed numbers in $[0,1]$.
!bsol
@@@CODE src-mc/clt.py fromto: def@def hist
!esol
!esubex

!bsubex
Calculate the average $M$ times and make a histogram of the values.
!bsol
@@@CODE src-mc/clt.py fromto: def hist@average3
!esol
!esubex

!bsubex
Repeat the above exercise for a Poisson distribution.
!bsol
@@@CODE src-mc/clt.py fromto: def average3@hist2
!esol
!esubex

!bremarks
It is quite remarkable that the distribution of the average values
from both a uniform and Poisson distribution follows a normal
distribution. The general
"proof":"https://en.wikipedia.org/wiki/Central_limit_theorem"
is not that complicated, but the ramifications are large. The central
limit theorem explains why it makes sense to use the standard
deviation as a measure of confidence for the mean value.
!eremarks
===== Exercise: Birthday Paradox =====
idx{birthday paradox}
label{ex:mc:BP}
##file=solution.pdf
The human mind is not good at logical thinking, and if we use our
intuition we often get into trouble. A well known example is the
''Birthday Paradox'', it is simply to answer the following question:
''How many randomly selected people do we need in order that there is
a 50\% chance that two of them have birthday on the same date?'' 

# !bsol
# !esol

!bsubex
Write a Python function that pick a random date
# subexercise...

!bsol
Below are two examples, the first one picks a date, while the second
one just picks a random day at year.  
@@@CODE src-mc/rd.py fromto: from@def No
!esol
!esubex

!bsubex
Write a function that takes as argument, number of persons in a group,
and returns 1 if two of them has birthday on the same date and 0
otherwise.

!bsol
. 
@@@CODE src-mc/rd.py fromto: def No@def BP

!esol

!esubex

!bsubex

Write a function that returns the probability that two people in a
group of $p$ persons have birthday on the same day, and determine how
many people we need to have a probability of 50\%.

!bsol
In order to get some statistics, we need to sample $N$ groups and
return the fraction of groups that had two persons with the same birthday.
@@@CODE src-mc/rd.py fromto: def BP@def /N
By trial an error, we find that 23 persons is needed in order to have
a probability of 

!esol

!esubex
##https://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb
##https://nbviewer.jupyter.org/url/norvig.com/ipython/ProbabilityParadox.ipynb
## standard deviation, the variance, the confidence interval, 95% confidence
## how to compute confidence interval - emperical rule - assume mean
## estimate is zero, no bias, distribution of errors are normal
