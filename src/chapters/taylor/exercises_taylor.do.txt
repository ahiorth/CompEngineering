TITLE: Finite Differences 
AUTHOR: Prepared as part of MOD510 Computational Engineering and Modeling
DATE: today

======= Getting familiar with Taylors formula =======

Learning objectives:
o Understand how Taylors formula can be used to approximate a function, for more information on Taylors formula, see "the book":"http://www.ux.uis.no/~ah/CompEng/._book002.html#___sec3" or "Wikipedia":"https://en.wikipedia.org/wiki/Taylor%27s_theorem"  
o Understand approximation and numerical error 

Below are *suggestions* for Python code, it is not the most optimal or elegant code, but meant as an inspiration. As a general rule, we *always* make the code as simple as possible (and as a consequence, usually, inefficient) the first time. When the code is working we can make it more elegant and/or efficient. The most important point is always to get a code that does what it is supposed to do.

Efficiency is usually lost whenever you write a for loop. For loops usually looks the same in any language and are easy to understand. As an example, let us consider the classical example of adding all integers from 1 to 100 (the answer is $n(n+1)/2=5050$ for $n=100$), we can do this in Python several ways:
@@@CODE src-taylor/summing.py fromto: import@N=100
If you are unfamiliar with `numpy.arange`, we refer to the official documentation "here":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html".  
The first implementation is how you would to the summation in most low level languages as C, C++ or Fortran. It is straight forward to read, and it will give you the correct answer. How fast are the methods? This can be checked with the following code:
@@@CODE src-taylor/summing.py fromto: N=100@#end
The command `%timeit` is a built-in magic command, and will work in Jupyter notebooks and with iPython. You can read more about `%timeit` "here":"https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-timeit". Notice that there is hardly any difference in speed between the methods. However if you increase $N$ to e.g. 1000, you will see that the NumPy implementation is superior.

!bnotice Observe:
Testing the efficiency of code should always be done on *large* samples.
!enotice
===== Exercise: Plot $\sin x$ =====
Run the script below, and you will see a plot of $\sin x$ on the interval $[-\pi,\pi]$.
@@@CODE src-taylor/func_plot_nb.py

Is this a good plot of $\sin x$? Improve the code above to produce a
better plot of $\sin x$.
!bc pycod
# enter code here 
!ec
o How many points are needed?
o Do we need the same number of points in the whole domain? Which properties of the function determines where we need more points?

===== Exercise: Taylor Polynomial Approximation =====
There are many ways of representing a function, but perhaps one of the most widely used is Taylor polynomials. 
Taylor series are the basis for solving ordinary and differential equations, simply because it makes it possible to evaluate any function with a set
of limited operations: *addition, subtraction, and multiplication*. The Taylor polynomial, $P_n(x)$ of degree $n$ of a function $f(x)$ at the point $c$ is defined as:
!bnotice Taylor polynomial:
!bt
\begin{align}
 P_n(x) &= f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots+\frac{f^{(n)}(c)}{n!}(x-c)^n\nonumber\\
&=\sum_{k=0}^n\frac{f^{(n)}}{k!}(x-c)^k.\label{eq:taylor:taylori}
\end{align}
!et
!enotice
If the series is around the point $c=0$, the Taylor polynomial $P_n(x)$ is often called a Maclaurin polynomial, more examples can be found 
"here":"https://en.wikipedia.org/wiki/Taylor_series". If the series converge (i.e. that the higher order terms approach zero), then we can represent the
function $f(x)$ with its corresponding Taylor series around the point $x=c$:
!bt
\begin{align}
 f(x) &= f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots
=\sum_{k=0}^\infty\frac{f^{(n)}}{k!}(x-c)^k.\label{eq:taylor:taylor}
\end{align}
!et
===== Exercise: Maclaurin series of $\sin x$ =====
Use equation (ref{eq:taylor:taylor}) to show that the Maclaurin series ($c=0$) of $\sin x$ is:
!bt
\begin{align}
\sin x = x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\cdots=\sum_{k=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}.
label{sin}
\end{align}
!et
Make a plot of $\sin x$, and the first 7 terms of the Maclaurin series for $\sin x$:
!bc pycod
import matplotlib.pyplot as plt
import numpy as np

#Maclaurin Serie of sin(x) at the power n
def mac_sin(x,n):
    """Returns the Maclaurin series up to order
       n for any x"""
    for element 0, 1, 2, ..., n-1:
    return x - x**3/3! + x**5/5! ...

# make the plot:
x  = np.arange(-np.pi,np.pi,0.01)
f1 = mac_sin(x,1)
f2 = mac_sin(x,1)
...
plt.plot(x,f1)
plt.plot(x,f1)
...
plt.show()
!ec

===== Exercise: Calculating $\sin x$ up to a certain error  =====
The error term in Taylors formula, when we represent a function with a finite number of polynomial elements is given by:
!bt
\begin{align}
R_n(x)&=f(x)-P_n(x)=\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-c)^{n+1}\nonumber\\
      &=\frac{1}{n!}\int_c^x(x-\tau)^{n}f^{(n+1)}(\tau)d\tau,\label{error}
\end{align}
!et 
for some $\eta$ in the domain $[x,c]$.

Write an implementation of $\sin x$, that calculates $\sin x$ up to a
certain accuracy, using the error formula in equation (ref{error}).

===== Exercise: Calculating derivative of functions =====
The derivative of a function can be calculated using the definition from calculus:
!bt
\begin{align}
f^\prime(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\simeq \frac{f(x+h)-f(x)}{h}.\label{eq:taylor:der1}
\end{align}  
!et
In the computer we cannot take the limit, $h\to 0$, a natural question is then: What value to use for $h$?

* Make a Python script that calculates the derivative of $\sin x$ for different step sizes $h$. Vary $h$ between $10^{-15}$ and 0.1, and make a plot of the error versus the step size.

!bc pycod
import matplotlib.pyplot as plt
import numpy as np

def f(x):
    return np.sin(x)

def df(x,h):
    return numerical derivative of f(x)

def err(x,h):
    return np.abs(df(x,h)-np.cos(x))

# to generate plot
step_size = [1/(1e1*10**h) for h in range(16)]
error =  estimate error for various step sizes
plt.loglog( ...)

plt.grid()

plt.show()
!ec

* Explain the shape of the plot you have made. Why does the numerical *not go to zero* as the step size is decreasing?

* Use equation (ref{eq:taylor:taylor}), expand about $x\pm h$ and show that you can derive a higher order formula for the numerical derivative:
!bt
\begin{align}
f^\prime(x)&=\frac{f(x+h)-f(x-h)}{2h},\label{eq:taylor:der2}
\end{align}
!et

* Make a similar plot as before but this time also with equation (ref{eq:taylor:der2}).

===== Exercise: Round off errors vs numerical errors =====
The numerical error is given in equation (ref{error}), show that for the numerical derivatives in the previous exercise:
!bt
\begin{align}
f^\prime(x)&=\frac{f(x+h)-f(x)}{h}-\frac{h}{2}f^{\prime\prime}(\eta),label{eq:taylor:derrb}\\
f^\prime(x)&=\frac{f(x+h)-f(x-h)}{2h} -\frac{h^2}{6}f^{(3)}(\eta),\label{eq:taylor:der2b}
\end{align}
!et
o What is the value of $\eta$? Is it the same for equation (ref{eq:taylor:derrb}) and (ref{eq:taylor:der2b})?
o What is round off errors?
o Show that the total error, including round off errors for (ref{eq:taylor:derrb}) and (ref{eq:taylor:der2b}) is:
!bt
\begin{align}
R_3&=\frac{\epsilon|f(x)|}{h}+\frac{h^2}{6}f^{(3)}(\eta),\label{eq:taylor:derr3b}\\
R_4&=\frac{4\epsilon|f(x)|}{h^2}+\frac{h^2}{12}f^{(4)}(\eta),\label{eq:taylor:derr4b}
\end{align}
!et

Which value of $h$ would minimize $R_3$ and $R_4$? Compare with the plot in the previous exercise. 