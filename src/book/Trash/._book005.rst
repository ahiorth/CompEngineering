.. !split

.. _ch:nlin:

Optimization and nonlinear systems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

.. Contrary to linear equations, you will most likely find that the functions available in

.. various Python library will *not* cover your needs and in many cases fail to give you

.. the correct solution. The reason for this is that the solution of a nonlinear equation # is greatly

.. dependent on the starting point, and a combination of various techniques  must be used.

In this chapter we will cover some theory related to the solution of nonlinear equations, and introduce the most used methods. A nonlinear problem is represented as a single equation or a system of equations, where the response is not changing proportionally to the input.  Almost all physical systems are nonlinear, and one frequent use of the methods presented in this chapter is to determine model parameters by matching a nonlinear model to data. 

Numerical methods that is guaranteed to find a solution (if it exists) are called *closed methods*, and *open* other vise. In many cases the closed methods requires more iterations for well behaved functions than the open methods. For one dimensional problems we will cover: fixed point iteration, bisection, Newton's method, and the secant method.
For  multidimensional problems we will cover Newton-Rapson method, which is a direct extension of Newton's method in one
dimension, and the steepest decent. The main challenge is that there are (usually) more than one solution, the solution that
*you* want for a specific problem is usually dictated by the underlying physics. If computational speed is not an issue, the
 method of choice is usually the bisection method. It is guaranteed to give an answer, but it might be slow. If speed is an issue, usually Newton's or the secant method will be the fastest (but it depends on the starting point). The secant method is sometimes preferred if the derivative of the function is costly to evaluate. Brents method is a method that combine the secant and bisection method (not covered), and is guaranteed to find a solution if the root is bracketed. 

In many practical, engineering, applications one usually implements some of the methods described below directly inside functions. This is because it is usually faster than calling a separate all purpose nonlinear solver, and that one usually has a very good idea of what a good starting point for the nonlinear solver is. 

Nonlinear equations
===================
A nonlinear equation is simply an equation that is not linear. That means that when the variables changes the response is not changing proportional to the values of the variables. Solving a nonlinear equation always proceeds by *iterations*, we start with one or several initial guesses and then search for the solution. In many cases we do not know beforehand if the equation actually has a solution, or multiple solutions. An example of a nonlinear problem is:

.. _Eq:eq:nlin:exp:

.. math::

    \tag{135}
    e^{-x}=x^2.
        
        

Traditionally one collect all the terms on one side, to solve an equation of the form

.. _Eq:eq:nlin:fx:

.. math::

    \tag{136}
    f(x)=x^2-e^{-x}=0.
        
        

In figure :ref:`fig:nlin:fx`, the solution is shown graphically. Note that in one case the solution is when the graph of :math:`e^{-x}`, and :math:`x^2` intersect, whereas in the other case the root is located when :math:`x^2-e^{-x}` intersect the $x-$axis. 

.. _fig:nlin:fx:

.. figure:: fig-nlin/fx.png
   :width: 400

   *Notice that the root is located at the same place (:math:`x=0.703467417`)*

In the case of more than one unknown, or a set of equations that must be satisfied simultaneously, equation :ref:`(136) <Eq:eq:nlin:fx>` is replaced with a vector equation

.. _Eq:eq:nlin:fvec:

.. math::

    \tag{137}
    \mathbf{f}(\mathbf{x})=\mathbf{0}.
        
        

Although this equation looks quite similar to equation :ref:`(136) <Eq:eq:nlin:fx>`, this equation is *much* harder to solve. The only methods we will cover is the Newton Rapson method, which is a very good method if a good starting point is given. If you have a multidimensional problem, the advice is to try Newton-Raphson, if this method fails you need to try more advanced method, see e.g. [Ref09]_.

Example: van der Waals equation of state
========================================
Before we begin with the numerical algorithms, let us consider an example: the van der Waals equation of state. The purpose is to illustrate some of the typical challenges. You are probably familiar with the ideal gas law:

.. _Eq:eq:nlin:pvt:

.. math::

    \tag{138}
    P\nu=R_gT,
        
        

where :math:`\nu=V/n` is the molar volume of the gas, :math:`P` is the pressure, :math:`V` is the volume, :math:`T` is the temperature, :math:`n` is the number of moles of the gas, and :math:`R_g` is the ideal gas constant.  This equation is an example of an *equation of state* (EOS), it relates :math:`P`, :math:`T`, and :math:`\nu`. Thus if we know the pressure and temperature of the gas, we can calculate :math:`\nu`. Equation :ref:`(138) <Eq:eq:nlin:pvt>` assumes that there are no interactions between the molecules in the gas. Clearly, this is too simplistic, and because of this one normally uses an EOS that better reflect the physical properties of the substance. A very famous EOS is the van der Waal EOS, which is a slight modification of equation :ref:`(138) <Eq:eq:nlin:pvt>`:

.. _Eq:eq:nlin:vdw:

.. math::

    \tag{139}
    \left(P+\frac{a}{\nu^2}\right)\left(\nu-b\right)=R_gT.
        
        

:math:`a` and :math:`b` are material constants that needs to be determined experimentally. This equation is *not* used in industrial design, but most equations used in practice are based on equation :ref:`(139) <Eq:eq:nlin:vdw>`. Multiplying equation :ref:`(139) <Eq:eq:nlin:vdw>` with :math:`\nu^2`, we get a non linear equation that is cubic in the molar volume. It turns out that cubic EOS are a class of equations that are quite successful in modeling the behavior of real systems [Ref10]_. However equation :ref:`(139) <Eq:eq:nlin:vdw>` is a good starting point for more complex and realistic equations.

It is common practice to rescale EOS with respect to the critical point. At the critical point we have [ref]:

.. _Eq:eq:nlin:crit1:

.. math::

    \tag{140}
    \left.\frac{\partial P}{\partial \nu}\right|_{T_c,P_c} =0
         
        

.. _Eq:eq:nlin:crit2:

.. math::

    \tag{141}
    \left.\frac{\partial^2 P}{\partial \nu^2}\right|_{T_c,P_c} =0
         
        

From equation :ref:`(140) <Eq:eq:nlin:crit1>`,  :ref:`(141) <Eq:eq:nlin:crit2>`, and :ref:`(139) <Eq:eq:nlin:vdw>`, it follows:

.. _Eq:eq:nlin:crit3:

.. math::

    \tag{142}
    \nu_c=3b\quad,P_c=\frac{a}{27b^2}\quad,R_gT_c=\frac{8a}{27b^2}.
        
        

Inserting these equations into equation :ref:`(139) <Eq:eq:nlin:vdw>`, and defining the *reduced* quantities :math:`\hat{P}=P/P_c`, :math:`\hat{T}=T/T_c`, :math:`\hat{\nu}=\nu/\nu_c`, we get

.. _Eq:eq:nlin:vdwr:

.. math::

    \tag{143}
    \left(\hat{P}+\frac{3}{\hat{\nu}^2}\right)\left(3\hat{\nu}-1\right)=8\hat{T}.
        
        

.. _fig:nlin:vdw:

.. figure:: fig-nlin/vdw.png
   :width: 400

   *van der Waal isotherms*

In figure :ref:`fig:nlin:vdw`, we have plotted the isotherms. Note that if :math:`\hat{T}<1` (:math:`T<T_c`), there might be more than one solution for the molar volume. This is clearly unphysical and additional constraints are needed. For the curve :math:`\hat{T}=0.9`, the dashed lined shows that for :math:`\hat{P}=0.7`, there are three solutions. This is a typical behavior of the cubic EOS, and physically it corresponds to the saturated case, where the vapor and liquid phase co-exist. The left root is the liquid state and the right root is the vapor state. The root in the middle represents a meta stable state.


.. admonition:: It never hurts to look at your function

   The example in figure :ref:`fig:nlin:vdw` illustrates some important points. Solving a nonlinear problem might be very easy in part of the parameter space (e.g. when :math:`T>T_c` there are only one solution), but extremely hard in other part of the parameter space (e.g. when :math:`T<T_c`, where there are multiple solutions). However, much of the trick to find a solution is to choose a good starting point. When there are multiple solutions we need to start close to the physical solution.




.. --- begin exercise ---

Exercise 5.1: van der Waal EOS and CO$_2$
-----------------------------------------

Use equation :ref:`(139) <Eq:eq:nlin:vdw>`, and the parameters for CO$_2$: a=3.640 L$^2$bar/mol, and b=0.04267 L/mol, to test the van der Waal EOS in equation :ref:`(139) <Eq:eq:nlin:vdw>`. Use that at 2 MPa and 100 $^\circ$C, CO$_2$ has a specific volume of 0.033586 m$^3$/kg.

.. --- begin solution of exercise ---

**Solution.**
The calculation is straight forward, but it is easy to get an error due to units. We will use SI units: a=0.3640 m$^6$Pa/mol, b=4.267$\cdot10^{-5}$ m$^3$/mol, $R$=8.314J/mol K.  The molar volume is obtained by multiplying by the molar weight of CO$_2$: :math:`M_w` = 44 g/mol, hence $\nu=1.478\cdot10^{-3}$m$^3$/mol. Using :math:`P=RT/(\nu-b)-a/\nu^2=1.993` MPa, or an error of :math:`0.3\%`.

.. --- end solution of exercise ---

.. --- end exercise ---

Fixed-point iteration
=====================

.. index:: fixed-point iteration

A simple (but not always possible) way of solving a nonlinear equation is to reformulate the problem :math:`f(x)=0` to a problem of the form

.. _Eq:eq:nlin:g:

.. math::

    \tag{144}
    x=g(x).
        
        

The algorithm for solving this equation is to guess at a starting point, :math:`x_0`, evaluate :math:`x_1=g(x_0)`, :math:`x_2=g(x_1)`, and so on. In some circumstances we might end up at a stable point, where :math:`x` does not change. This point is termed a *fixed point*.

Note that the form of :math:`g(x)` is not uniquely determined. For our function defined in equation :ref:`(135) <Eq:eq:nlin:exp>`, we can solve for :math:`x` directly

.. _Eq:eq:nlin:g2:

.. math::

    \tag{145}
    x=e^{-x/2},
        
        

or we could write:

.. _Eq:eq:nlin:g3:

.. math::

    \tag{146}
    x=x-x^2+e^{-x}.
        
        

These functions are illustrated in figure :ref:`fig:nlin:fg`, by visual inspection they look very similar, but as we will show in the next exercise the convergence is quite different. 

.. _fig:nlin:fg:

.. figure:: fig-nlin/f_g_comb.png
   :width: 400

   *Two examples of iterative functions, that will give the same solution*

.. --- begin exercise ---

Exercise 5.2: Implement the fixed point iteration
-------------------------------------------------

Write a Python function that utilizes the fixed point algorithm in the previous section, find the root of :math:`f(x)=x^2-e^{-x}`. In one case use :math:`g(x)=e^{-x/2}`, and in the other case use :math:`g(x)=x-x^2+e^{-x}`. How many iterations does it take in each case?

.. --- begin solution of exercise ---

**Solution.**
Below is a straight forward (vanilla) implementation:

.. code-block:: python

    def iterative(x,g,prec=1e-8, MAXIT=1000):
        '''Approximate solution of x=g(x) by fixed point iterations.
        x : starting point for iterations 
        eps : desired precision
        Returns x when x does not change more than prec
        and number of iterations MAXIT are not exceeded
        '''
        eps = 1
        n=0
        while eps>prec and n < MAXIT:
            x_next = g(x)
            eps = np.abs(x-x_next)
            x = x_next
            n += 1
            if(np.isinf(x)):
                print('Quitting .. maybe bad starting point?')
                return x
        if (n<MAXIT):
            print('Found solution: ', x, ' After ', n, 'iterations')
        else:
            print('Max number of iterations exceeded')
        return x

If we start at :math:`x=0`, it will take 174 iterations using :math:`x-x^2+e^{-x}` (:math:`g(x)`) and only 19 for :math:`e^{-x/2}` (:math:`h(x)`), the root is $x$=0.70346742.

.. --- end solution of exercise ---

.. --- end exercise ---

.. --- begin exercise ---

Exercise 5.3: Finding the molar volume from the van der Waal EOS by fixed point iteration
-----------------------------------------------------------------------------------------

Extend the code above to take as argument the van der Waal EOS. For simplicity we will use the rescaled EOS in equation :ref:`(143) <Eq:eq:nlin:vdwr>`. Show that for the reduced temperature, $\hat{T}$=1.2, and pressure, $\hat{P}$=1.5, the reduced molar volume :math:`\hat{nu}` is 1.3522091.

.. --- begin solution of exercise ---

**Solution.**
First we rewrite equation :ref:`(143) <Eq:eq:nlin:vdwr>` in a more useful form

.. _Eq:eq:nlin:sp:

.. math::

    \tag{147}
    \hat{\nu}=\frac{1}{3}(1+\frac{8\hat{T}}{\hat{P}+3/\hat{\nu}^2})
        
        

The right hand side will play the same role as :math:`g(x)` above, where :math:`x` now is the reduced molar volume, and can be implemented in Python as:

.. code-block:: python

    def dvdwEOS(nu,t,p):
        return (1+8*t/(p+3/nu**2))/3

Note that this function requires the values of :math:`\hat{P}` and :math:`\hat{T}`, in addition to :math:`\hat{\nu}` to return a value. Thus in order to use the fixed point iteration method implemented above, we need to pass arguments to our function. This can easily be achieved by taking advantage of Pythons ``*args`` functionality. By simply rewriting our implementation slightly:

.. code-block:: python

    def iterative(x,g,*args,prec=1e-8):
        MAX_ITER=1000
        eps = 1
        n=0
        while eps>prec and n < MAX_ITER:
            x_next = g(x,*args)
            eps = np.abs(x-x_next)
            x = x_next
            n += 1
        print('Number of iterations: ', n)
        return x

We can find the root by calling the function as:

.. code-block:: python

    iterative(1,dvdwEOS,1.2,1.5)

The program returns the correct solution after 71 iterations.

.. --- end solution of exercise ---

.. --- end exercise ---

.. _sec:nlin:fp:

When does the fixed point method fail?
--------------------------------------
If we replace :math:`e^{-x}` with :math:`e^{1-x^2}` in equation :ref:`(146) <Eq:eq:nlin:g3>`, our method will not give a solution. You can easily verify that the :math:`x=1` is a solution, so why does our method fail? To investigate this in a bit more detail, we turn to Taylors formula (once again). Assume that the root is located at :math:`x^*`, and our guess is :math:`x_k`, then the next :math:`x`-value will be

.. _Eq:eq:nlin:t1:

.. math::

    \tag{148}
    x_{k+1}=g(x_0)=g(x^*)+g^\prime(x^*)(x_k-x^*)+\cdots
        
        

The true solution is :math:`x^*`, hence :math:`x^*=f(x^*)`, and we can write

.. _Eq:eq:nlin:t2:

.. math::

    \tag{149}
    x_{k+1}-x^*=g^\prime(x^*)(x_k-x^*),
        
        

where we have neglected higher order terms. The point is: at each iteration we want the distance :math:`x_1-x^*` to decrease, i.e. to be smaller than :math:`x_0-x^*`. This can only be achieved if

.. _Eq:eq:nlin:fpi:

.. math::

    \tag{150}
    |g^\prime(x^*)|<1. 
        
        

In our example above we saw that if :math:`g(x)=x-x^2+e^{-x}`, we used 172 iterations and only 19 iterations if we replaced :math:`g(x)` with :math:`h(x)=e^{-x/2}` to converge to the *same* root $x$=0.70346742. We can now understand this, because :math:`g^\prime(x)=1-2x-e^{-x}` and :math:`g(x^*)\simeq-0.90`, whereas :math:`h^\prime(x)=-e^{-x/2}/2`, and :math:`h^\prime(x^*)\simeq0.35`. We expect the number of iterations, :math:`n`, needed to reach a certain precision, :math:`\varepsilon`, to scale as

.. _Eq:eq:nlin:scale:

.. math::

    \tag{151}
    |g^\prime(x^*)|^n=\varepsilon.
        
        

We expect to use :math:`\log|h^\prime(x^*)|/\log|g^\prime(x^*)|\simeq10` more iterations using :math:`g(x)` compared to :math:`h(x)`, which is close to the observed value of 172/19$\simeq 9$.
What to do when the fixed point method fails
--------------------------------------------
As discussed in [Ref11]_, there might be an elegant solution whenever :math:`|g^\prime(x^*)|>1`. If it is possible to invert the :math:`g(x)`, we can show that the derivative of the inverse function
$ { g^\prime }^{-1} (x^*)  = 1/g^\prime (x^*) $. Why is this useful? Because if :math:`x^*=g(x^*)` is the solution we are searching for, then this is equivalent to :math:`x^*={g}^{-1}(x^*)` *if and only if* we can invert :math:`g(x)`. Note that in many cases it is not possible to invert :math:`g(x)`. Let us first show that $ { g^\prime }^{-1} (x^*)  = 1/g^\prime (x^*) $. For simplicity write

.. _Eq:_auto35:

.. math::

    \tag{152}
    y = g(x)\Leftarrow x=g^{-1}(y),
        
        

taking the derivative with respect to x gives

.. _Eq:eq:nlin:fpi1:

.. math::

    \tag{153}
    \frac{d}{dx}g^{-1}(y)=\frac{dx}{dx}=1,
        

.. _Eq:eq:nlin:fpi2:

.. math::

    \tag{154}
    \frac{dg^{-1}(y)}{dy}\frac{dy}{dx}=\frac{dx}{dx}=1,
        

.. _Eq:eq:nlin:fpi3:

.. math::

    \tag{155}
    \frac{dg^{-1}(y)}{dy}=\frac{1}{\frac{dy}{dx}}=\frac{1}{g^{\prime}(x)}
        =\frac{1}{g^{\prime}(g^{-1}(y))}.
        

Going from equation :ref:`(153) <Eq:eq:nlin:fpi1>` to :ref:`(154) <Eq:eq:nlin:fpi2>`, we have used the chain rule. Equation :ref:`(155) <Eq:eq:nlin:fpi3>` is general, let us now specify to our fixed point iteration. Then we can use :math:`x^*=g(x^*)=y^*`, and :math:`x^*=g^{-1}(y^*)=g^{-1}(x^*)` hence we can write the last equation as

.. _Eq:eq:nlin:fpif:

.. math::

    \tag{156}
    \frac{d}{dx}g^{-1}(x^*)=\frac{1}{g^{\prime}(x^*)}.
        
        

.. --- begin exercise ---

Exercise 5.4: Solve :math:`x=e^{1-x^2}` using fixed point iteration
-------------------------------------------------------------------

The solution to :math:`x=e^{1-x^2}` is clearly :math:`x=1`.

* First try the fixed point method using :math:`g(x)=e^{1-x^2}` to find the root :math:`x=1`. Try to start very close to the true solution :math:`x=1`. What is the value of :math:`g^\prime(x^*)`?

* Next, invert :math:`g(x)`, what is the derivative of :math:`g^{-1}(x^*)`? Try the fixed point method using :math:`g^{-1}(x^*)`

.. --- begin solution of exercise ---

**Solution.**
First, we calculate the derivative of :math:`g(x)`, :math:`g^\prime(x)=-2xe^{1-x^2}`, hence :math:`g^\prime(x^*)=-2` and :math:`|g^\prime(x^*)|>1`. This is an unstable fixed point, and if we start a little bit off from this point we will spiral away from it.

Inverting :math:`y=g(x)` gives us $ g^{-1} (y)=\sqrt{1-\ln y}$. Note that :math:`y^*=x^*=1` is a solution to this equation as it should be. The derivative is

.. _Eq:_auto36:

.. math::

    \tag{157}
    {g^{-1}}^\prime(y)=-\frac{1}{2\sqrt{1-\ln y}},
        
        

and $ {g^{-1}}^\prime(y^*)=-1/2 $.
It takes about 30 iterations to reach the correct solution :math:`y^*=1`, when the starting point is :math:`y=0`.

.. --- end solution of exercise ---

.. --- end exercise ---

Rate of convergence          (1)
================================

.. index:: rate of convergence

The rate of convergence is the speed at which a *convergent* sequence approach the limit. Assume that our sequence :math:`x_{k}` converges to the number :math:`x^*`, the sequence is said to *converge linearly* to :math:`x^*` if there exists a number :math:`\mu\in<0,1>`, such that

.. _Eq:eq:nlin:linconv:

.. math::

    \tag{158}
    \lim_{k\to\infty}=\frac{|x_{k+1}-x^*|}{|x_k-x^*|}=\mu
        
        

Inserting equation :ref:`(149) <Eq:eq:nlin:t2>` in equation :ref:`(158) <Eq:eq:nlin:linconv>`, we get:

.. _Eq:eq:nlin:ling:

.. math::

    \tag{159}
    \lim_{k\to\infty}=\frac{|x_{k+1}-x_k|}{x_k-x^*}
        =\frac{|g^\prime(x^*)(x_k-x^*)|}{|x_k-x^*|}=|g^\prime(x^*)|.
        
        

Hence the fixed point iteration is expected to converge *linearly* to the correct solution. The definition in equation :ref:`(158) <Eq:eq:nlin:linconv>`, can be extended to include the definition of quadratic, cubic, etc. convergence:

.. _Eq:eq:nlin:qconv:

.. math::

    \tag{160}
    \lim_{k\to\infty}=\frac{|x_{k+1}-x^*|}{|x_k-x^*|^q}=\mu.
        
        

If :math:`q=2` the convergence is said to be quadratic and so on.

The bisection method
====================

.. index:: bisection method

The idea behind bisection is that the root is bracketed, i.e. that there exists two points :math:`a` and :math:`b`, such that :math:`f(a)\cdot f(b)<0`. In practice it might be a challenge to find these two points. However, if you know that the function has a only root between two values, and that speed is not a big issue this method guarantees that the root will be found within a finite number of steps. The basic idea behind the method is to divide the interval into two (i.e. bisecting the interval). The method only works if the function is continuous on the interval. 

.. _fig:nlin:bisection:

.. figure:: fig-nlin/bisection.png
   :width: 400

   *Illustration of the bisection method for the van der Waal EOS*

The algorithm is as follows:
* Test if :math:`f(a)\cdot f(b)<0`, if not return an error message

* Calculate the midpoint :math:`c=(a+b)/2`. If :math:`f(a)\cdot f(c)<0` the root is in the interval :math:`[a,c]`, else the root is in the interval :math:`[c,b]`

* Half the interval, and test in which interval the root lies, and continue until a convergence criterion.

In figure :ref:`fig:nlin:bisection`, there is a graphical illustration.
Below is an implementation of the bisection method.

.. code-block:: python

    def bisection(f,a,b,PREC=1e-8,MAXIT=100):
        '''Approximate solution of f(x)=0 on interval [a,b] by bisection.
    
        f   : f(x)=0.
        a,b : brackets the root f(a)*f(b) has to be negative 
        PREC: desired precision
        
        Returns the midpoint when it is closer than eps to the root, 
        unless MAXIT are not exceeded
        '''
        if f(a)*f(b) > 0:
            print('You need to bracket the root, f(a)*f(b) >= 0')
            return None
        prec=10*PREC
        c = 0.5*(a + b)
        for n in range(MAXIT):
            c_old = c 
            fc = f(c)
            if fc == 0:
                print('Found exact solution ', c, 
                        ' after ', n, 'iterations' )
                return c
            if f(a)*fc < 0:
                b = c
            else:
                a = c
            c = 0.5*(a+b)
            prec=np.abs(c_old-c)
        if n<MAXIT-1:
            print('Found solution ', c,' after ', n, 'iterations' )
            return c
        else:
            print('Max number of iterations: ', MAXIT, ' reached.') 
            print('Try to increase MAXIT')
            print('Returning best guess, value of function is: ', fc)
        return c


.. admonition:: Warnings

   Note that the implementation of the bisection algorithm is only a few lines of code, and most of the code is to give warnings to the user. In this case it is important to do additional checking, and give the user warnings. If $f(c)$=0, then we must stop and return the exact solution. If we only test if :math:`f(a)\cdot f(c)` is greater or lower than zero the algorithm would fail.




Rate of convergence          (2)
--------------------------------

.. index:: rate of convergence

If :math:`c_n` is the midpoint after :math:`n` steps, the difference between the solution :math:`x^*` and :math:`c_n` is

.. _Eq:eq:nlin:bisec:

.. math::

    \tag{161}
    |c_n-x^*| \le \frac{|b-a|}{2^n}
        
        

Using our previous definition in equation :ref:`(160) <Eq:eq:nlin:qconv>`, we find that

.. _Eq:eq:nlin:bsc1:

.. math::

    \tag{162}
    \lim_{k\to\infty}=\frac{|c_{k+1}-x^*|}{|c_k-x^*|}\le\frac{|b-a|/2^{n+1}}{|b-a|/2^n}=\frac{1}{2},
        
        

hence the bisection method converges linearly.
Newton's method
===============

.. index:: Newtons method

Newtons method is one of the most used methods. If it converges, it converges quadratically to the correct solution. The drawback is that contrary to the bisection method it may fail if a bad starting point is given. Newtons method for finding the root of a function :math:`f(x)=0` is illustrated in figure :ref:`fig:nlin:newton`. The main idea is to use more information about the function in the search of the root. In this case we want to find the point where the tangent of the function in :math:`x_k` intersect the $x-$axis, and take that as our next point, :math:`x_{k+1}`. 

.. _fig:nlin:newton:

.. figure:: fig-nlin/newton_comb.png
   :width: 400

   *Illustration of Newtons method for the van der Waals EOS*

We can easily derive the algorithm by finding the formula for the tangent line. Using :math:`y=ax+b` for the tangent line, we immediately know that :math:`a=f^\prime(x_k)`. :math:`b` can be found as we know that the line intersects :math:`(x_k,f(x_k))`: :math:`f(x_k)=f^\prime(x_k)x_k+b`, hence the equation for the tangent line is :math:`y=f^\prime(x_k)x+f(x_k)-f^\prime(x_k)x_k`. The next point is located where :math:`y` crosses the :math:`x`-axis, hence :math:`0=f^\prime(x_k)x_{k+1}+f(x_k)-f^\prime(x_k)x_k`. Rearranging this equation, we can write Newtons method in the standard form

.. _Eq:eq:nlin:newton:

.. math::

    \tag{163}
    x_{k+1}=x_k-\frac{f(x_k)}{f^\prime(x_k)}.
        
        

Note that the derivative of :math:`f(x)` enters in equation :ref:`(163) <Eq:eq:nlin:newton>`, which means that if our function has a extremal value in our search domain, Newtons method most likely will fail. In particular :math:`x_1`, and :math:`x_4` in the figure to the right in figure :ref:`fig:nlin:newton2` are bad starting point for Newtons method.

.. _fig:nlin:newton2:

.. figure:: fig-nlin/newton2.png
   :width: 400

   *Illustration of some of the possible challenges with Newtons method. Note that if the derivative is zero somewhere in the search interval, Newtons method will fail*

An implementation is shown below.

.. code-block:: python

    def newton(f,x, prec=1e-8,MAXIT=500):
        '''Approximate solution of f(x)=0 by Newtons method.
        The derivative of the function is calculated numerically
        f   : f(x)=0.
        x   : starting point  
        eps : desired precision
        
        Returns x when it is closer than eps to the root, 
        unless MAX_ITERATIONS are not exceeded
        '''
        MAX_ITERATIONS=MAXIT
        x_old = x
        h     = 1e-4
        for n in range(MAX_ITERATIONS):
            x_new = x_old - 2*h*f(x_old)/(f(x_old+h)-f(x_old-h))
            if(abs(x_new-x_old)<prec):
                print('Found solution:', x_new, 
                      ', after:', n, 'iterations.' )
                return x_new
            x_old=x_new
        print('Max number of iterations: ', MAXIT, ' reached.') 
        print('Try to increase MAXIT or decrease prec')
        print('Returning best guess, value of function is: ', f(x_new))
        return x_new

Comparing figure :ref:`fig:nlin:bisection` and :ref:`fig:nlin:newton`, you immediately get the sense that Newtons method converges faster, and indeed it does. 

Rate of convergence          (3)
--------------------------------

.. index::
   single: Newtons method, rate of convergence

 Newtons method is similar to the fixed point method, but where we do not use :math:`g(x)=x-f(x)`, but :math:`g(x)=x-\frac{f(x)}{f^\prime(x)}`. We will now analyze Newtons method, using the same approach as in the section :ref:`sec:nlin:fp`. First we expand :math:`g(x)` around the root :math:`x^*`

.. _Eq:eq:nlin:nsec:

.. math::

    \tag{164}
    x_{k+1}=g(x_k)=g(x^*)+g^\prime(x^*)(x_k-x^*)+\frac{1}{2}g^{\prime\prime}(x^*)(x_k-x^*)^2,
        
        

where we have skipped all higher order terms. You can easily verify that

.. _Eq:eq:nlin:gn2:

.. math::

    \tag{165}
    g^\prime(x) =\frac{f^{\prime\prime}(x)f(x)}{f^\prime(x)^2}
         
        

.. _Eq:eq:nlin:gn3:

.. math::

    \tag{166}
    g^{\prime\prime}(x) =\frac{(f^{\prime\prime\prime}(x)f^\prime(x)-2f^{\prime\prime}(x)^2f^\prime(x))f(x)
        +f^{\prime\prime}(x)f^\prime(x)^2}{f^\prime(x)^4}.
        
        

:math:`x^*` is a solution, hence :math:`f(x^*)=0`, we then find from equation :ref:`(165) <Eq:eq:nlin:gn2>` and :ref:`(166) <Eq:eq:nlin:gn3>` that :math:`g^\prime(x^*)=0`, and :math:`g^{\prime\prime}(x^*)=f^{\prime\prime}(x^*)/f^{\prime}(x^*)^2`. Thus from equation :ref:`(164) <Eq:eq:nlin:nsec>` we get

.. _Eq:eq:nlin:nsecn:

.. math::

    \tag{167}
    x_{k+1}=x^*+\frac{1}{2}\frac{f^{\prime\prime}(x^*)}{f^{\prime}(x^*)^2}(x_k-x^*)^2,
        
        

or equivalently:

.. _Eq:eq:nlin:nsecn2:

.. math::

    \tag{168}
    \frac{x_{k+1}-x^*}{(x-x^*)^2}=\frac{1}{2}\frac{f^{\prime\prime}(x^*)}{f^{\prime}(x^*)^2}.
        
        

The denominator has a power of two, and hence Newtons method is *quadratic* convergent (assuming that the sequence :math:`x_{k+1}` is a convergent sequence). Note that it also follows from the analyses above that Newtons method will fail if the derivative at the root, :math:`f^\prime(x^*)`, is zero.

.. --- begin exercise ---

Exercise 5.5: Compare Newtons, Bisection and the Fixed Point method
-------------------------------------------------------------------

Find the root of :math:`f(x)=x^2-e^{-x}` using bisection, fixed point,  and Newtons method, start at :math:`x=0`. How many iterations do you need to use reach a precision of :math:`10^{-8}`? What happens if you widen the search domain or start further away from the root?

.. --- begin solution of exercise ---

**Solution.**
The root is located at :math:`x^*=0.70346742`.
* Fixed point method: we saw earlier that using :math:`g(x)=x-f(x)` used 174 iterations, and :math:`g(x)=\sqrt{x^2-f(x)}` used 19 iterations. If we start at :math:`x=-100`, :math:`g(x)=x-f(x)` fails, and  :math:`g(x)=\sqrt{x^2-f(x)}` uses only 21 iterations, and at :math:`x=100` we use 20 iterations.

* Bisection method: it use 25 iterations for :math:`a=0`, and :math:`b=1` (implementation shown earlier in the chapter). Choosing :math:`a=-b=-100` we use 33 iterations.

* Newtons method: it use only 5 function evaluations (implementation above) starting at  :math:`x=0`. Starting at :math:`x=-100`, it uses 106 iterations. Newtons method is slow in this case because the function is very steep around the starting point, see figure :ref:`fig:nlin:newton_bad`. Starting at :math:`x=100`, we only use 10 iterations.

.. _fig:nlin:newton_bad:

.. figure:: fig-nlin/newton_bad.png
   :width: 400

   *Newtons method performs poorly far away due to the shape of the function close to :math:`x=-100`, bisection performs much better while the fixed point method fails*


.. admonition:: A good starting point is crucial

   Note that it is not given which method is best, but if we are ''close'' to the root Newtons method is usually superior. If we are far away, other methods might work better. In many cases one uses a more stable method far away from the root, and then ''polish up'' the root by a couple of Newton iterations [Ref09]_. See also Brents method which combines bisection and linear interpolation (secant method) [Ref09]_.




.. --- end solution of exercise ---

.. --- end exercise ---

Secant method
=============

.. index:: secant method

The Newtons method is very good if you can choose a good starting point, and you can give in an analytical formula for the derivative. In some cases it is not possible to calculate the derivative analytically, then a very good method of choice is the secant method. It can be derived by simply replacing the derivative in Newtons method by the finite difference approximation

.. _Eq:eq:nlin:sec1:

.. math::

    \tag{169}
    f^\prime(x_k)\to \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}.
        
        

Inserting this equation into equation :ref:`(163) <Eq:eq:nlin:newton>`, we get

.. _Eq:_auto37:

.. math::

    \tag{170}
    x_{k+1}=x_k-f(x_k)\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}{\nonumber}
        
        

.. _Eq:eq:nlin:sec2:

.. math::

    \tag{171}
    =\frac{x_{k-1}f(x_k)-x_kf(x_{k-1})}{f(x_k)-f(x_{k-1})}. 
        

For a graphical illustration see figure :ref:`fig:nlin:secant`

.. _fig:nlin:secant:

.. figure:: fig-nlin/secant.png
   :width: 400

   A graphical illustration of the secant method. Note that the starting points :math:`x_0` and :math:`x_1` do not need to be close. The next point is where the (secant) line crosses the :math:`x`-axis

Rate of convergence          (4)
--------------------------------

.. index::
   single: secant method, rate of convergence

The derivation of the rate of convergence for the secant method is a bit more involved. To simplify the notation we introduce the notation :math:`\varepsilon_k\equiv x_k-x^*`, where :math:`x^*` is the exact solution. Subtracting :math:`x^*` from each side of equation :ref:`(171) <Eq:eq:nlin:sec2>` we get

.. _Eq:_auto38:

.. math::

    \tag{172}
    \varepsilon_{k+1}=x_{k+1}-x^*=\frac{x_{k-1}f(x_k)-x_kf(x_{k-1})}{f(x_k)-f(x_{k-1})}-x^*, {\nonumber}
        
        

.. _Eq:eq:nlin:sec3:

.. math::

    \tag{173}
    \varepsilon_{k+1}=\frac{\varepsilon_{k-1}f(x_k)-\varepsilon_k f(x_{k-1})}{f(x_k)-f(x_{k-1})},
        
        

we now make a Taylor expansion of :math:`f(x_k)` and :math:`f(x_{k-1})` about the root :math:`x^*`

.. _Eq:_auto39:

.. math::

    \tag{174}
    f(x_k) =f(x^*)+f^\prime(x^*)(x_k-x^*)+\frac{1}{2}f^{\prime\prime}(x^*)(x_k-x^*)^2+\cdots ,{\nonumber}
        
        

.. _Eq:_auto40:

.. math::

    \tag{175}
    =f^\prime(x^*)\varepsilon_k+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_k^2+\cdots .
        
        

.. _Eq:_auto41:

.. math::

    \tag{176}
    f(x_{k-1}) =f(x^*)+f^\prime(x^*)(x_{k-1}-x^*)+\frac{1}{2}f^{\prime\prime}(x^*)(x_{k-1}-x^*)^2+\cdots,{\nonumber}
        
        

.. _Eq:_auto42:

.. math::

    \tag{177}
    =f^\prime(x^*)\varepsilon_{k-1}+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_{k-1}^2+\cdots ,
        
        

where we have used the fact that :math:`f(x^*)=0`. Inserting these equations into equation :ref:`(173) <Eq:eq:nlin:sec3>` and neglecting terms of order :math:`\varepsilon_k^3` we get

.. _Eq:_auto43:

.. math::

    \tag{178}
    \varepsilon_{k+1}=\frac{\varepsilon_{k-1}\left[f^\prime(x^*)\varepsilon_k+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_k^2\right] -\varepsilon_k\left[ f^\prime(x^*)\varepsilon_{k-1}+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_{k-1}^2\right]}{f^\prime(x^*)\varepsilon_k+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_k^2-\left[ f^\prime(x^*)\varepsilon_{k-1}+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_{k-1}^2\right]},{\nonumber}
        
        

.. _Eq:_auto44:

.. math::

    \tag{179}
    =\frac{\varepsilon_k\varepsilon_{k-1}\left[\varepsilon_k-\varepsilon_{k-1}\right]}{\left[f^\prime(x^*)+\frac{1}{2}f^{\prime\prime}(x^*)(\varepsilon_k+\varepsilon_{k-1})\right](\varepsilon_k-\varepsilon_{k-1})},{\nonumber}
        
        

.. _Eq:eq:nlin:sec4:

.. math::

    \tag{180}
    =\frac{f^{\prime\prime}(x^*)}{2f^\prime(x^*)}\varepsilon_k\varepsilon_{k-1},
        

where we have neglected higher powers of :math:`\varepsilon`. We are searching for a solution of the form :math:`\varepsilon_{k+1}=K\varepsilon_k^q`, :math:`q` is the rate of convergence. We can invert this equation to get :math:`\varepsilon_k=K^{-1/q}\varepsilon_{k+1}^{1/q}`, or alternatively :math:`\varepsilon_{k-1}=K^{-1/q}\varepsilon_{k}^{1/q}` (just set :math:`k\to k-1`). Inserting these equations into equation :ref:`(180) <Eq:eq:nlin:sec4>`

.. _Eq:eq:nlin:sec5:

.. math::

    \tag{181}
    \varepsilon_k^q=\frac{f^{\prime\prime}(x^*)}{2f^\prime(x^*)}\varepsilon_kK^{-1/q}\varepsilon_{k}^{1/q}.
        
        

Clearly, if this equation is to have a solution we must have

.. _Eq:_auto45:

.. math::

    \tag{182}
    \frac{f^{\prime\prime}(x^*)}{2f^\prime(x^*)}K^{-1/q} =1{\nonumber}
        
        

.. _Eq:_auto46:

.. math::

    \tag{183}
    \varepsilon_k^q=\varepsilon_k\varepsilon_{k}^{1/q}=\varepsilon_{k}^{1+1/q},
        
        

or :math:`q=1+1/q`. Solving this equation we get :math:`q=(1\pm\sqrt{5})/2`, neglecting the negative solution, we find the rate of convergence for the secant method :math:`q=(1+\sqrt{5})/2\simeq 1.618`.

Newton Rapson method
====================

.. index:: Newton Rapson method

The derivation of Newtons method, equation :ref:`(163) <Eq:eq:nlin:newton>`, done in the previous section was based on figure :ref:`fig:nlin:newton`. We will now derive it using a slightly different approach, but which lends itself easier to extend Newtons method to higher dimensions. The starting point is to expand the function around :math:`x_k`, using Taylors formula

.. _Eq:eq:nlin:nt:

.. math::

    \tag{184}
    f(x)=f(x_k)+f^\prime(x_k)(x-x_k) + \cdots\,.
        
        

Equation :ref:`(163) <Eq:eq:nlin:newton>` can be derived from equation :ref:`(184) <Eq:eq:nlin:nt>` by simply demanding that we keep the linear terms, and that the next point :math:`x_{k+1}` is located where the linear approximation intersects the :math:`x`-axis, i.e. simply set :math:`f(x)=0`, and :math:`x=x_{k+1}` in equation :ref:`(184) <Eq:eq:nlin:nt>`.

In higher order dimensions, we solve equation :ref:`(137) <Eq:eq:nlin:fvec>`, and equation :ref:`(184) <Eq:eq:nlin:nt>` is

.. _Eq:eq:nlin:ntd:

.. math::

    \tag{185}
    \mathbf{f}(\mathbf{x})=\mathbf{f}(\mathbf{x}_k)+ \mathbf{J}(\mathbf{x}_k)(\mathbf{x}-\mathbf{x}_k) + \cdots\,.
        
        

:math:`\mathbf{J}(\mathbf{x}_k)` is the Jacobian. As before, we simply set  :math:`\mathbf{f}(\mathbf{x})=\mathbf{0}`, :math:`\mathbf{x}=\mathbf{x}_{k+1}`, and keep the linear terms, hence

.. _Eq:eq:nlin:ntd2:

.. math::

    \tag{186}
    \mathbf{x}_{k+1}=\mathbf{x}_k-\mathbf{J}^{-1}(\mathbf{x}_k)\mathbf{f}(\mathbf{x}_k). 
        
        

To make the mathematics a bit more clear, let us specify to :math:`2D`. Assume that
:math:`\mathbf{f}(\mathbf{x})=[f_x(x,y),f_y(x,y)]`, then the Jacobian is

.. _Eq:eq:nlin:jac:

.. math::

    \tag{187}
    \mathbf{J}(\mathbf{x}_k)=
        \left(
        \begin{array}{cc}
        \frac{\partial f_x}{\partial x}&\frac{\partial f_x}{\partial y}\\ 
        \frac{\partial f_y}{\partial x}&\frac{\partial f_y}{\partial y}
        \end{array}
        \right).
        
        

Gradient descent
================

.. index:: gradient descent

This method used is to minimize functions (does not work for root finding). In many nonlinear problems, we would like to minimize (or maximize) a function. An ideal 2D example is shown in figure :ref:`fig:nlin:grad`. The algorithm moves in the direction of steepest descent. Note that the step size might change towards the search. 

.. _fig:nlin:grad:

.. figure:: fig-nlin/steepest_descent.png
   :width: 400

   *A very simple example of the gradient descent method*

Assume that we have a function :math:`\mathbf{f}(\mathbf{x})`, that we would like to minimize. The gradient descent algorithm is simply to update parameters according to the derivative (gradient) of :math:`\mathbf{f}`

.. _Eq:eq:nlin:stpdc:

.. math::

    \tag{188}
    \mathbf{x}_{k+1}=\mathbf{x}_{k}-\gamma\nabla\mathbf{f}.
        
        

:math:`\gamma` is the learning rate, and a good choice of :math:`\gamma` is important. :math:`\gamma` might also change from one iteration to the other, and does not have to be constant.  

.. --- begin exercise ---

Exercise 5.6: Gradient descent solution of linear regression
------------------------------------------------------------

A very typical example is if we have a model and we would like to fit some parameters of the model to a data set (e.g. linear regression). Assume that we have observations :math:`(x_i,y_i)` and model predictions :math:`f(x_i,\mathbf{\beta})`, the model parameters are contained in the vector :math:`\mathbf{\beta}`. The *least square*, :math:`S`, is the square of the sum of all the *residuals*, i.e. the difference between the observations and model predictions 

.. _Eq:eq:nlin:lsq:

.. math::

    \tag{189}
    S=\sum_i(y_i-f(x_i,\mathbf{\beta}))^2.
        
        

Specializing to linear regression, we choose the model to be linear

.. _Eq:eq:nlin:lin:

.. math::

    \tag{190}
    f(x_i,\mathbf{\beta})=b_0+b_1x_i.
        
        

Equation :ref:`(189) <Eq:eq:nlin:lsq>` now takes the form

.. _Eq:eq:nlin:lsq2:

.. math::

    \tag{191}
    S=\sum_i(y_i-b_0+b_1x_i)^2.
        
        

The gradients are:

.. _Eq:_auto47:

.. math::

    \tag{192}
    \frac{\partial S}{\partial b_0}=-2\sum_i(y_i-b_0+b_1x_i),{\nonumber}
        
        

.. _Eq:eq:nlin:dlsq:

.. math::

    \tag{193}
    \frac{\partial S}{\partial b_1}=-2\sum_i(y_i-b_0+b_1x_i)x_i,.
        
        

* Implement the gradient descent method using a constant learning rate of :math:`10^{-3}`, to minimize the least square function

* Test the linear regression on the data set :math:`x_i=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`, and :math:`y=[1, 3, 2, 5, 7, 8, 8, 9, 10, 12]`, choose a starting value :math:`(b_0,b_1)=(0,0)`. What happens if you increase the learning rate?

.. --- begin solution of exercise ---

**Solution.**
Below is an implementation of the gradient descent method with a constant learning rate

.. code-block:: python

    def gradient_descent(f,x,df, g=.001, prec=1e-8,MAXIT=10):
        '''Minimize f(x) by gradient descent.
        f   : min(f(x))
        x   : starting point 
        df  : derivative of f(x)
        g   : learning rate
        prec: desired precision
        
        Returns x when it is closer than eps to the root, 
        unless MAXIT are not exceeded
        '''
        x_old = x
        for n in range(MAXIT):
            plot_regression_line(x_old)  
            x_new = x_old - g*df(x_old)
            if(abs(np.max(x_new-x_old))<prec):
                print('Found solution:', x_new, 
                      ', after:', n, 'iterations.' )
                return x_new
            x_old=x_new
        print('Max number of iterations: ', MAXIT, ' reached.') 
        print('Try to increase MAXIT or decrease prec')
        print('Returning best guess, value of function is: ', f(x_new))
        return x_new

The linear regression is implemented as below

.. code-block:: python

    x_obs_ = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 
    y_obs_ = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12]) 
    def plot_regression_line(b,x=x_obs_, y=y_obs_): 
        global N_
        # plotting the actual points as scatter plot 
        plt.scatter(x, y, color = "m", 
                   marker = "o", s = 30,label="data") 
      
        # predicted response vector 
        y_pred = b[0] + b[1]*x
      
        # plotting the regression line
        if(len(b)>1):
    #        plt.plot(x, y_pred, color = "g", label = "R-squared = {0:.3f}".format(b[2]))
            plt.plot(x, y_pred, color = "g", label = "iteration:" + str(N_) +", (b[0],b[1])= ({0:.3f}".format(b[0]) + ", {0:.3f})".format(b[1]))
            plt.legend()
        else:
            plt.plot(x, y_pred, color = "g")
      
        # putting labels 
        plt.xlabel('x') 
        plt.ylabel('y') 
        plt.grid()
        plt.legend()
    #    plt.savefig('../fig-nlin/stdec'+str(N_)+'.png', bbox_inches='tight',transparent=True)
        N_=N_+1  
        # function to show plot 
        plt.show() 
    
    
    def Jacobian(x,f,dx=1e-5):
        N=len(x)
        x0=np.copy(x)
        f0=f(x)
        J=np.zeros(shape=(N,N))
        for j in range(N):
            x[j] = x[j] +  dx
            for i in range(N):   
                J[i][j] = (f(x)[i]-f0[i])/dx
            x[j] = x[j] -  dx
        return J
    
    
    
    
    def newton_rapson(x,f,J=None, jacobian=False, prec=1e-8,MAXIT=100):
        '''Approximate solution of f(x)=0 by Newtons method.
        The derivative of the function is calculated numerically
        f   : f(x)=0.
        J   : Jacobian
        x   : starting point  
        eps : desired precision
        
        Returns x when it is closer than eps to the root, 
        unless MAX_ITERATIONS are not exceeded
        '''
        MAX_ITERATIONS=MAXIT
        x_old = np.copy(x)
        for n in range(MAX_ITERATIONS):
            plot_regression_line(x_old) 
            if not jacobian:
                J_=Jacobian(x_old,f)
            else:
                J_=J(x_old)
            z=np.linalg.solve(J_,-f(x_old))
            x_new=x_old+z
            if(np.sum(abs(x_new-x_old))<prec):
                print('Found solution:', x_new, 
                      ', after:', n, 'iterations.' )
                return x_new
            x_old=np.copy(x_new)
        print('Max number of iterations: ', MAXIT, ' reached.') 
        print('Try to increase MAXIT or decrease prec')
        print('Returning best guess, value of function is: ', f(x_new))
        return x_new
    
    
    def gradient_descent(f,x,df, g=.001, prec=1e-8,MAXIT=10):
        '''Minimize f(x) by gradient descent.
        f   : min(f(x))
        x   : starting point 
        df  : derivative of f(x)
        g   : learning rate
        prec: desired precision
        
        Returns x when it is closer than eps to the root, 
        unless MAXIT are not exceeded
        '''
        x_old = x
        for n in range(MAXIT):
            plot_regression_line(x_old)  
            x_new = x_old - g*df(x_old)
            if(abs(np.max(x_new-x_old))<prec):
                print('Found solution:', x_new, 
                      ', after:', n, 'iterations.' )
                return x_new
            x_old=x_new
        print('Max number of iterations: ', MAXIT, ' reached.') 
        print('Try to increase MAXIT or decrease prec')
        print('Returning best guess, value of function is: ', f(x_new))
        return x_new
    #end
    
    def S(b,x=x_obs_,y=y_obs_):
        return np.sum((y-b[0]-b[1]*x)**2)
    
    def dS(b,x=x_obs_,y=y_obs_):
        return np.array([-2*np.sum(y-b[0]-b[1]*x),
                         -2*np.sum((y-b[0]-b[1]*x)*x)])
    
    def J(b,x=x_obs_,y=y_obs_):
        N=len(b)
        J=np.zeros(shape=(N,N))
        xs=np.sum(x)
        J[0][0]=2*len(x)
        J[0][1]=2*xs
        J[1][0]=2*xs
        J[1][1]=2*np.sum(x*x)
        return J
    N_=0
    print('Gradient ')
    b=np.array([0,0])

The first four iterations are shown in figure :ref:`fig:nlin:grsc`. If we choose a learning rate that is too high, we will move past the minimum, and the solution will oscillate. This can be avoided by lowering the learning rate as we iterate, by e.g. replacing ``g`` with ``g/(n+1)`` in the implementation above.

.. _fig:nlin:grsc:

.. figure:: fig-nlin/stdec_comb.png
   :width: 400

   *First four iterations of the gradient descent solution of linear regression*

.. --- end solution of exercise ---

.. --- end exercise ---

Other useful methods
====================

In this chapter we have covered the *basic*, but you should now be well equipped to dive into other methods. We highly recommend [Ref09]_ as a starting point, although the code examples are written in C++, the theory is presented in a very accurate, but informal way.
* Brents method:  uses root bracketing, bisection, and inverse quadratic interpolation. The 1D method of choice if the function and not its derivative is known

