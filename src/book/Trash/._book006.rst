.. !split

.. _ch:numint:

Numerical integration
%%%%%%%%%%%%%%%%%%%%%

Algorithmic thinking
====================

The only way to improve in coding and algorithmic thinking is practice. The concept of one dimensional numerical integration is easy to understand, i.e. to calculate the area under a curve. In this chapter we will implement several numerical methods, and it will serve as a very simple playground that illustrates the key aspects of numerical modeling

1. We start with a mathematical model (in this case an integral)

2. The mathematical model is formulated in discrete form 

3. Then we design an algorithm to solve the model 

4. The numerical solution for a test case is compared with the true solution (could be an analytical solution or data)

5. Error analysis: we investigate the accuracy of the algorithm by changing the number of iterations and/or make changes to the implementation or algorithm

The main point of this chapter is not to develop your own integration methods, the built in methods in Scipy will work in most cases. However, the way to break down the main task of calculating an integral into smaller tasks that is understandable by a computer, may work as a template for many different problems you would typically solve using a computer. A second motivation is that by analyzing the origin of numerical errors gives ideas for improving the algorithm, which is transferable to other problems.      

The midpoint rule
-----------------

.. index:: midpoint method

Numerical integration is encountered in numerous applications in physics and engineering sciences. 
Let us first consider the most simple case, a function :math:`f(x)`, which is a function of one variable, :math:`x`. The most straight forward way of calculating the area :math:`\int_a^bf(x)dx` is 
simply to divide the area under the function into :math:`N` equal rectangular slices with size :math:`h=(b-a)/N`, as illustrated in figure :ref:`fig:numint:mid`. The area of one box is:

.. _Eq:eq:numint:mid0:

.. math::

    \tag{194}
    M(x_k,x_k+h)=f(x_k+\frac{h}{2}) h,\
        

and the area of all the boxes is:

.. math::
        
        I(a,b)=\int_a^bf(x)dx\simeq\sum_{k=0}^{N-1}M(x_k,x_k+h)\nonumber
        

.. _Eq:eq:numint:mid1:

.. math::

    \tag{195}
    =h\sum_{k=0}^{N-1}f(x_k+\frac{h}{2})=h\sum_{k=0}^{N-1}f(a+(k+\frac{1}{2})h).
        \
        

Note that the sum goes from :math:`k=0,1,\ldots,N-1`, a total of :math:`N` elements. We could have chosen to let the sum go from :math:`k=1,2,\ldots,N`. 
In Python, C, C++ and many other programming languages the arrays start by indexing the elements from :math:`0,1,\ldots` to :math:`N-1`, 
therefore we choose the convention of having the first element to start at :math:`k=0`.

.. _fig:numint:mid:

.. figure:: fig-numint/func_sq.png
   :width: 800

   *Integrating a function with the midpoint rule*

Below is a Python code, where this algorithm is implemented for :math:`\int_0^\pi\sin (x)dx`

.. code-block:: python

    import numpy as np
    # Function to be integrated
    def f(x):
        return np.sin(x)
    
    def midpoint(f,a,b,N):
        """
        f : function to be integrated on the domain [a,b]
        N : number of integration points
        """
        h=(b-a)/N
        x=np.arange(a+0.5*h,b,h)
        return h*np.sum(f(x))
    N=10
    a=0
    b=np.pi
    Area = midpoint(f,a,b,N)
    print('Numerical value= ', Area)
    print('Error= ', (2-Area)) # Analytical result is 2


.. note::
   In the implementation above, we have taken advantage of Numpys ability to pass a vector to a function. This greatly enhances the speed and makes clean, readable code. If you were coding in a lower level programming language like Fortran, C or C++, you would probably implement the loop like (in Python syntax):
   
   .. code-block:: python
   
       for k in range(0,N): # loop over k=0,1,..,N-1
           val = lower_limit+(k+0.5)*h # midpoint value
           area += func(val)
       return area*h




The trapezoidal rule
--------------------

.. index:: trapezoidal method

The numerical error in the above example is quite low, only about 2$\%$ for :math:`N=5`. 
However, by just looking at the graph above it seems likely that we can develop a better algorithm by using trapezoids instead of rectangles, 
see figure :ref:`fig:numint:trap`.

.. _fig:numint:trap:

.. figure:: fig-numint/func_tr.png
   :width: 800

   *Integrating a function with the trapezoidal rule*

Earlier we approximated the area using the midpoint value: :math:`f(x_k+h/2)\cdot h`. Now we use :math:`A=A_1+A_2`, where :math:`A_1=f(x_k)\cdot h` 
and :math:`A_2=(f(x_k+h)-f(x_k))\cdot h/2`, hence the area of one trapezoid is:

.. _Eq:_auto48:

.. math::

    \tag{196}
    A\equiv T(x_k,x_k+h)=(f(x_k+h)+f(x_k))h/2.
        
        

This is the trapezoidal rule, and for the whole interval we get:

.. math::
        
        I(a,b)=\int_a^bf(x)dx\simeq\frac{1}{2}h\sum_{k=0}^{N-1}\left[f(x_k+h)+f(x_k)\right] \nonumber 
        

.. math::
          
        =h\left[\frac{1}{2}f(a)+f(a+h) + f(a+2h) +\nonumber\right. 
        

.. math::
          
        \left.\qquad\cdots + f(a+(N-2)h)+\frac{1}{2}f(b)\right]\nonumber 
        

.. _Eq:_auto49:

.. math::

    \tag{197}
    =h\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=1}^{N-2}f(a+k h)\right].
        
        

Note that this formula was bit more involved to derive, but it requires only one more function evaluations compared to the midpoint rule. 
Below is a python implementation:

.. code-block:: python

    def trapezoidal(f,a,b,N):
        """
        f : function to be integrated on the domain [a,b]
        N : number of integration points
        """
        h=(b-a)/N
        x=np.arange(a+h,b,h)
        return h*(0.5*f(a)+0.5*f(b)+np.sum(f(x)))

In the table below, we have calculated the numerical error for various values of :math:`N`.

=========  =========  ==============  =================  
:math:`N`  :math:`h`  Error Midpoint  Error Trapezoidal  
=========  =========  ==============  =================  
    1         3.14        -57\%             100\%        
    5        0.628       -1.66\%            3.31\%       
    10       0.314       -0.412\%          0.824\%       
   100       0.031      -4.11E-3\%        8.22E-3\%      
=========  =========  ==============  =================  

Note that we get the surprising result that this algorithm performs poorer, a factor of 2 than the midpoint rule.
How can this be explained? By just looking at figure :ref:`fig:numint:mid`, we see that the midpoint rule actually over predicts the area from :math:`[x_k,x_k+h/2]` 
 and under predicts in the interval :math:`[x_k+h/2,x_{k+1}]` or vice versa. The net effect is that for many cases the midpoint rule give a slightly better 
 performance than the trapezoidal rule. In the next section we will investigate this more formally.

Numerical errors on integrals
-----------------------------

.. index::
   single: numerical integrals, error

It is important to know the accuracy of the methods we are using, otherwise we do not know if the
computer produce correct results. In the previous examples we were able to estimate the error because we knew the analytical result. However, if we know the 
analytical result there is no reason to use the computer to calculate the result(!). Thus, we need a general method to estimate the error, and let the computer 
run until a desired accuracy is reached. 

In order to analyze the midpoint rule in more detail we approximate the function by a Taylor 
series at the midpoint between :math:`x_k` and :math:`x_k+h`: 

.. math::
        
        f(x)=f(x_k+h/2)+f^\prime(x_k+h/2)(x-(x_k+h/2))\nonumber
        

.. _Eq:_auto50:

.. math::

    \tag{198}
    +\frac{1}{2!}f^{\prime\prime}(x_k+h/2)(x-(x_k+h/2))^2+\mathcal{O}(h^3)
        
        

Since :math:`f(x_k+h/2)` and its derivatives are constants it is straight forward to integrate :math:`f(x)`:

.. math::
        
        I(x_k,x_k+h)=\int_{x_k}^{x_k+h}\left[f(x_k+h/2)+f^\prime(x_k+h/2)(x-(x_k+h/2))\right.\nonumber
        

.. _Eq:_auto51:

.. math::

    \tag{199}
    \left.+\frac{1}{2!}f^{\prime\prime}(x_k+h/2)(x-(x_k+h/2))^2+\mathcal{O}(h^3)\right]dx
        
        

The first term is simply the midpoint rule, to evaluate the two other terms we make the substitution: :math:`u=x-x_k`:

.. math::
        
        I(x_k,x_k+h)=f(x_k+h/2)\cdot h+f^\prime(x_k+h/2)\int_0^h(u-h/2)du\nonumber
        

.. math::
          
        +\frac{1}{2}f^{\prime\prime}(x_k+h/2)\int_0^h(u-h/2)^2du+\mathcal{O}(h^4)\nonumber
        

.. _Eq:_auto52:

.. math::

    \tag{200}
    =f(x_k+h/2)\cdot h-\frac{h^3}{24}f^{\prime\prime}(x_k+h/2)+\mathcal{O}(h^4).
        
        

Note that all the odd terms cancels out, i.e :math:`\int_0^h(u-h/2)^m=0` for :math:`m=1,3,5\ldots`. Thus the error for the midpoint rule, :math:`E_{M,k}`, on this particular interval is:

.. _Eq:_auto53:

.. math::

    \tag{201}
    E_{M,k}=I(x_k,x_k+h)-f(x_k+h/2)\cdot h=-\frac{h^3}{24}f^{\prime\prime}(x_k+h/2),
        
        

where we have ignored higher order terms. We can easily sum up the error on all the intervals, but clearly :math:`f^{\prime\prime}(x_k+h/2)` will 
not, in general, have the same value on all intervals. However, an upper bound for the error can be found by replacing :math:`f^{\prime\prime}(x_k+h/2)` 
with the maximal value on the interval :math:`[a,b]`, :math:`f^{\prime\prime}(\eta)`:

.. _Eq:eq:numint:em:

.. math::

    \tag{202}
    E_{M}=\sum_{k=0}^{N-1}E_{M,k}=-\frac{h^3}{24}\sum_{k=0}^{N-1}f^{\prime\prime}(x_k+h/2)\leq-\frac{Nh^3}{24}f^{\prime\prime}(\eta),\
        

.. _Eq:_auto54:

.. math::

    \tag{203}
    E_{M}\leq-\frac{(b-a)^3}{24N^2}f^{\prime\prime}(\eta),
        
        

where we have used :math:`h=(b-a)/N`. We can do the exact same analysis for the trapezoidal rule, but then we expand the function around :math:`x_k-h` instead of the midpoint. 
The error term is then:

.. _Eq:_auto55:

.. math::

    \tag{204}
    E_T=\frac{(b-a)^3}{12N^2}f^{\prime\prime}(\overline{\eta}).
        
        

At the first glance it might look like the midpoint rule always is better than the trapezoidal rule, but note that the second derivative is 
evaluated in different points (:math:`\eta` and :math:`\overline{\eta}`). Thus it is possible to construct examples where the midpoint rule performs poorer 
than the trapezoidal rule.

Before we end this section we will rewrite the error terms in a more useful form as it is not so easy to evaluate 
:math:`f^{\prime\prime}(\eta)` (since we do not know which value of :math:`\eta` to use). By taking a closer look at equation :ref:`(202) <Eq:eq:numint:em>`, 
we see that it is closely related to the midpoint rule for :math:`\int_a^bf^{\prime\prime}(x)dx`, hence:

.. _Eq:_auto56:

.. math::

    \tag{205}
    E_{M}=-\frac{h^2}{24}h
        \sum_{k=0}^{N-1}f^{\prime\prime}(x_k+h/2)\simeq-\frac{h^2}{24}\int_a^b
        f^{\prime\prime}(x)dx
        
        

.. _Eq:_auto57:

.. math::

    \tag{206}
    E_M\simeq\frac{h^2}{24}\left[f^\prime(b)-f^\prime(a)\right]=-\frac{(b-a)^2}{24N^2}\left[f^\prime(b)-f^\prime(a)\right]
        
        

The corresponding formula for the trapezoid formula is:

.. _Eq:_auto58:

.. math::

    \tag{207}
    E_T\simeq \frac{h^2}{12}\left[f^\prime(b)-f^\prime(a)\right]=\frac{(b-a)^2}{12N^2}\left[f^\prime(b)-f^\prime(a)\right]
        
        

Practical estimation of errors on integrals (Richardson extrapolation)
----------------------------------------------------------------------

.. index:: Richardson extrapolation

From the example above we were able to estimate the number of steps needed to reach (at least) a certain precision. 
In many practical cases we do not deal with functions, but with data and it can be difficult to evaluate the derivative. 
We also saw from the example above that the algorithm gives a higher precision than what we asked for. 
How can we avoid doing too many iterations? A very simple solution to this question is to double the number of intervals until 
a desired accuracy is reached. The following analysis holds for both the trapezoid and midpoint method, because in both cases 
the (global) error scale as :math:`h^2` [#trapez]_.

.. [#trapez] You can do the following analysis by assuming that the local error is :math:`h^3`, but then you need to take into account that you need to take twice as many steps, which will give the same result.

Assume that we have evaluated the integral with a step size :math:`h_1`, and the computed result is :math:`I_1`. 
Then we know that the true integral is :math:`I=I_1+c h_1^2`, where :math:`c` is a constant that is unknown. If we now half the step size: :math:`h_2=h_1/2`, 
then we get a new (better) estimate of the integral, :math:`I_2`, which is related to the true integral :math:`I` as: :math:`I=I_2+c h_2^2`. 
Taking the difference between :math:`I_2` and :math:`I_1` give us an estimation of the error:

.. _Eq:_auto59:

.. math::

    \tag{208}
    I_2-I_1=I-c h_2^2-(I-ch_1^2)=3c h_2^2,
        
        

where we have used the fact that :math:`h_1=2h_2`, Thus the error term is:

.. _Eq:_auto60:

.. math::

    \tag{209}
    E(a,b)=c h_2^2=\frac{1}{3}(I_2-I_1).
        
        

This might seem like we need to evaluate the integral twice as many times as needed. This is not the case, by choosing to exactly 
half the spacing we only need to evaluate for the values that lies halfway between the original points. We will demonstrate how 
to do this by using the trapezoidal rule, because it operates directly on the :math:`x_k` values and not the midpoint values. 
The trapezoidal rule can now be written as:

.. _Eq:_auto61:

.. math::

    \tag{210}
    I_2(a,b)=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=1}^{N_2-1}f(a+k h_2)\right],
        
        

.. math::
          
        =h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=\text{even values}}^{N_2-1}f(a+k h_2)\right.\nonumber
        

.. _Eq:_auto62:

.. math::

    \tag{211}
    \left.\qquad+\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2)\right],
        
        

in the last equation we have split the sum into odd an even values. The sum over the even values can be rewritten:

.. _Eq:_auto63:

.. math::

    \tag{212}
    \sum_{k=\text{even values}}^{N_2-1}f(a+k h_2)=\sum_{k=0}^{N_1-1}f(a+2k h_2)=\sum_{k=0}^{N_1-1}f(a+k h_1),
        
        

note that :math:`N_2` is replaced with :math:`N_1=N_2/2`, we can now rewrite :math:`I_2` as:

.. math::
        
        I_2(a,b)=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=0}^{N_1-1}f(a+k h_1)\right.\nonumber
        

.. _Eq:_auto64:

.. math::

    \tag{213}
    \left.+\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2)\right]
        
        

Note that the first terms are actually the trapezoidal rule for :math:`I_1`, hence:

.. _Eq:_auto65:

.. math::

    \tag{214}
    I_2(a,b)=\frac{1}{2}I_1(a,b)+h_2\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2).
        
        

The factor :math:`1/2` in front of :math:`I_1(a,b)`, appears because :math:`h_2=h_1/2`. 
A possible algorithm is then:
1. Choose a low number of steps to evaluate the integral, :math:`I_0`, the first time, e.g. :math:`N_0=1`

2. Double the number of steps, :math:`N_1=2N_0` 

3. Calculate the missing values by summing over the odd number of steps :math:`\sum_{k=\text{odd values}}^{N_1-1}f(a+k h_1)`

4. Check if :math:`E_1(a,b)=\frac{1}{3}(I_1-I_0)` is lower than a specific tolerance

5. If yes quit, if not, return to 2, and continue until :math:`E_i(a,b)=\frac{1}{3}(I_{i+1}-I_{i})` is lower than the tolerance  

Below is a Python implementation:

.. code-block:: python

    def int_adaptive_trapez2(lower_limit, upper_limit,func,tol):
        """
        adaptive quadrature, integrate a function from lower_limit
        to upper_limit within tol*(upper_limit-lower_limit)
    
        """
        S=[]
        S.append([lower_limit,upper_limit])
        I=0
        iterations=0
        while S:
            iterations +=1
            a,b=S.pop(-1) # last element
            m=(b+a)*0.5   # midpoint
            I1=0.5*(b-a)*(func(a)+func(b)) #trapezoidal for 1 interval 
            I2=0.25*(b-a)*(func(a)+func(b)+2*func(m)) #trapezoidal for 2 intervals
            if(np.abs(I1-I2)<3*np.abs((b-a)*tol)):
                I+=I2     # accuarcy met
            else:
                S.append([a,m]) # half the interval 
                S.append([m,b])
        print("Number of iterations: ", iterations)
        return I

If you compare the number of terms used in the adaptive trapezoidal rule, which was developed by halving the step size, and the adaptive midpoint rule that was derived on the basis of the theoretical error term, you will find the adaptive midpoint rule is more efficient. So why go through all this trouble? In the next section we will see that the development we did for the adaptive trapezoidal rule is closely related to Romberg integration, which is *much* more effective.

Romberg integration
===================

.. index:: Romberg integration

The adaptive algorithm for the trapezoidal rule in the previous section can be easily improved by remembering 
that the true integral was given by [#romerr]_ : :math:`I=I_i+ch_i^2+\mathcal{O}(h^4)`. The error term was in the previous example only used to 
check if the desired tolerance was achieved, but we could also have added it to our estimate of the integral to reach an accuracy to fourth order:

.. [#romerr] Note that all odd powers of :math:`h` is equal to zero, thus the corrections are always in even powers.  

.. _Eq:_auto66:

.. math::

    \tag{215}
    I=I_{i+1}+ch^2+\mathcal{O}(h^4)=I_{i+1}+\frac{1}{3}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^4).
        
        

As before the error term :math:`\mathcal{O}(h^4)`, can be written as: :math:`ch^4`. Now we can proceed as in the previous section: First we estimate the 
integral by one step size :math:`I_i=I+ch_i^4`, next we half the step size :math:`I_{i+1}=I+ch_{i+1}^4` and use these two estimates to calculate the error term:

.. math::
        
        I_{i+1}-I_{i}=I-c h_{i+1}^4-(I-ch_i^4)=-c h_{i+1}^4+c(2h_{i+1})^4=15c h_{i+1}^4,\nonumber
        

.. _Eq:_auto67:

.. math::

    \tag{216}
    ch_{i+1}^4=\frac{1}{15}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^6).
        
        

but now we are in the exact situation as before, we have not only the error term but the correction up to order :math:`h^4` for this integral:

.. _Eq:eq:numint:rom:

.. math::

    \tag{217}
    I=I_{i+1}+\frac{1}{15}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^6).\
        

Each time we half the step size we also gain a higher order accuracy in our numerical algorithm. Thus, there are two iterations going on at the same time; 
one is the iteration that half the step size (:math:`i`), and the other one is the increasing number of higher order terms added (which we will denote :math:`m`). 
We need to improve our notation, and replace the approximation of the integral (:math:`I_i`) with :math:`R_{i,m}`. Equation :ref:`(217) <Eq:eq:numint:rom>`, can now 
be written:

.. _Eq:_auto68:

.. math::

    \tag{218}
    I=R_{i+1,2}+\frac{1}{15}\left[R_{i+1,2}-R_{i,2}\right]+\mathcal{O}(h^6).
        
        

A general formula valid for any :math:`m` can be found by realizing:

.. _Eq:eq:numint:rom0:

.. math::

    \tag{219}
    I=R_{i+1,m+1}+c_mh_i^{2m+2}+\mathcal{O}(h_i^{2m+4})\
        

.. math::
          
        I=R_{i,m+1}+c_mh_{i-1}^{2m+2}+\mathcal{O}(h_{i-1}^{2m+4})\nonumber
        

.. _Eq:eq:numint:rom1:

.. math::

    \tag{220}
    =R_{i,m+1}+2^{2m+2}c_mh_{i}^{2m+2}+\mathcal{O}(h_{i-1}^{2m+4}),\
        

where, as before :math:`h_{i-1}=2h_i`. Subtracting equation :ref:`(219) <Eq:eq:numint:rom0>` and :ref:`(220) <Eq:eq:numint:rom1>`, we find an expression for the error term:

.. _Eq:eq:numint:rom2:

.. math::

    \tag{221}
    c_mh_{i}^{2m+2}=\frac{1}{4^{m+1}-1}(R_{i,m}-R_{i-1,m})\
        

Then the estimate for the integral in equation :ref:`(220) <Eq:eq:numint:rom1>` is:

.. _Eq:_auto69:

.. math::

    \tag{222}
    I=R_{i,m+1}+\mathcal{O}(h_i^{2m+2})
        
        

.. _Eq:_auto70:

.. math::

    \tag{223}
    R_{i,m+1}=R_{i,m}+\frac{1}{4^{m+1}-1}(R_{i+1,m}-R_{i,m}).
        
        

A possible algorithm is then:

1. Evaluate :math:`R_{0,0}=\frac{1}{2}\left[f(a)+f(b)\right](b-a)` as the first estimate

2. Double the number of steps, :math:`N_{i+1}=2N_i` or half the step size :math:`h_{i+1}=h_i/2` 

3. Calculate the missing values by summing over the odd number of steps :math:`\sum_{k=\text{odd values}}^{N_1-1}f(a+k h_{i+1})`

4. Correct the estimate by adding *all* the higher order error term :math:`R_{i,m+1}=R_{i,m}+\frac{1}{4^m-1}(R_{i+1,m+1}-R_{i,m+1})`

5. Check if the error term is lower than a specific tolerance :math:`E_{i,m}(a,b)=\frac{1}{4^{m+1}-1}(R_{i,m}-R_{i-1,m})`, if yes quit, if no goto 2, increase :math:`i` and :math:`m` by one

The algorithm is illustrated in figure :ref:`fig:numint:romberg`.

.. _fig:numint:romberg:

.. figure:: fig-numint/romberg.png
   :width: 400

   Illustration of the Romberg algorithm. Note that for each new evaluation of the integral :math:`R_{i,0}`, all the correction terms :math:`R_{i,m}` (for :math:`m>0`) must be evaluated again

Note that the tolerance term is not the correct one as it uses the error estimate for the current step, 
which we also use correct the integral in the current step to reach a higher accuracy. 
Thus the error on the integral will always be lower than the user specified tolerance.
Below is a Python implementation:

.. code-block:: python

    def int_romberg(func,a, b,tol,show=False):
        """ calculates the area of func on the domain [a,b]
            for the given tol, if show=True the triangular
            array of intermediate results are printed """
        Nmax = 100
        R = np.empty([Nmax,Nmax]) # storage buffer
        h = (b-a) # step size
        R[0,0]    =.5*(func(a)+func(b))*h
        N = 1
        for i in range(1,Nmax):
            h /= 2
            N *= 2
            odd_terms=0
            for k in range (1,N,2): # 1, 3, 5, ... , N-1
                val        = a + k*h
                odd_terms += func(val)
    		# add the odd terms to the previous estimate	
            R[i,0]   = 0.5*R[i-1,0] + h*odd_terms 
            for m in range(i): 
    			# add all higher order terms in h
                R[i,m+1]   = R[i,m] + (R[i,m]-R[i-1,m])/(4**(m+1)-1)                  
    		# check tolerance, best guess			
            calc_tol = abs(R[i,i]-R[i-1,i-1])       
            if(calc_tol<tol):
                break  # estimated precision reached 
        if(i == Nmax-1):
            print('Romberg routine did not converge after ',
                  Nmax, 'iterations!')
        else:      
            print('Number of intervals = ', N)
    
        if(show==True):
            elem = [2**idx for idx in range(i+1)]
            print("Steps StepSize Results")
            for idx in range(i+1):
                print(elem[idx],' ',
                      "{:.6f}".format((b-a)/2**idx),end = ' ')
                for l in range(idx+1):
                    print("{:.6f}".format(R[idx,l]),end = ' ')
                print('')  
        return R[i,i] #return the best estimate

Note that the Romberg integration only uses 32 function evaluations to reach a precision of :math:`10^{-8}`, whereas the adaptive midpoint and trapezoidal rule in the previous
section uses 20480 and 9069 function evaluations, respectively. 

Alternative implementation of adaptive integration
--------------------------------------------------
Before we proceed, we will consider an alternative implementation of the adaptive method presented in the previous sections, with the following modification
1. We will use Simpsons rule (see the exercise at the end), which takes the following form :math:`\int_a^bf(x)dx\simeq\frac{h}{6}\left[f(a)+4f(a+\frac{h}{2})+2f(a+h)+ 4f(a+3\frac{h}{2})+2f(a+2h)+\cdots+f(b)\right]`

2. We only divide the intervals needed to reach the desired accuracy.

Simpsons rule is accurate up to :math:`\mathcal{O}(h^4)`, and by following the same arguments as above we can estimate the error as :math:`E_i(a,b)=\frac{1}{15}(I_{i+1}-I_{i})`. The factor 1/15 (as opposed to 1/3) originates from the higher order accuracy. The integration proceeds as follows
* ``S`` is an empty list

* ``S.append([a,b])``

* :math:`I=0`

* ``while S not empty do:``

  * ``[a,b]=S.pop(-1)``

  * :math:`m=(b+a)/2`

  * :math:`I_1=` ``simpson_step(a,b)``

  * :math:`I_2=` ``simpson_step(a,m)+simpson_step(m,b)``

  * if :math:`|I_1-I_2|<15|b-a|\cdot tol`

    * :math:`I+=I_2`

  * else:

    * ``S.append([a,m])``

    * ``S.append([m,b])``

  * return :math:`I`

Note the use of the list ``S``, we remove the interval :math:`[a,b]` from the list and calculates the integral. If the integral is not accurate enough we add to new intervals to the list, and continue until we reach the desired accuracy, then we proceed with the next interval. Since we remove (``pop``) the element from the list, we know that we will finish the evaluation once the list is empty. This algorithm allows for different sub interval to have different degrees of subdivisions, contrary to Rombergs algorithm. The full python implementation is shown below

.. code-block:: python

    def simpson_step(a, b,func):
        m=0.5*(a+b)
        return (b-a)/6*(func(a)+func(b)+4*func(m))
    
    def int_adaptive_simpson(func,a, b,tol):
        """
        adaptive quadrature, integrate a function from a
        to b within tol*(b-a) uses simpsons rule
        """
        S=[]
        S.append([a,b])
        I=0
        iterations=0
        while S:
            iterations +=1
            a,b=S.pop(-1) # last element
            m=(b+a)*0.5   # midpoint
            I1=simpson_step(a,b,func) #simpsons for 1 interval 
            I2=simpson_step(a,m,func)+simpson_step(m,b,func) # ...2 intervals
            if(np.abs(I1-I2)<15*np.abs((b-a)*tol)):
                I+=I2     # accuarcy met
            else:
                S.append([a,m]) # half the interval 
                S.append([m,b])
        print("Number of iterations: ", iterations)
        return I

Gaussian quadrature
===================

.. index:: Gaussian quadrature

Many of the methods we have looked into are of the type:

.. _Eq:eq:numint:qq1:

.. math::

    \tag{224}
    \int_a^b f(x) dx = \sum_{k=0}^{N-1} \omega_k f(x_k),\
        

where the function is evaluated at fixed interval. For the midpoint rule :math:`\omega_k=h` for all values of :math:`k`, for the trapezoid rule 
:math:`\omega_k=h/2` for the endpoints and :math:`h` for all the interior points. 
For the Simpsons rule (see exercise) :math:`\omega_k=h/3, 4h/3,2h/3,4h/3,\ldots,4h/3,h/3`. 
Note that all the methods we have looked at so far samples the function in equal spaced points, :math:`f(a+k h)`, 
for :math:`k=0, 1, 2\ldots, N-1`. If we now allow for the function to be evaluated at unevenly spaced points, we can do a lot better. 
This realization is the basis for Gaussian Quadrature. We will explore this in the following, 
but to make the development easier and less cumbersome, we transform the integral from the domain :math:`[a,b]` to :math:`[-1,1]`:

.. _Eq:_auto71:

.. math::

    \tag{225}
    \int_a^bf(t)dt=\frac{b-a}{2}\int_{-1}^{1}f(x)dx\text{ , where:}
        
        

.. _Eq:_auto72:

.. math::

    \tag{226}
    x=\frac{2}{b-a}t-\frac{b+a}{b-a}.
        
        

The factor in front comes from the fact that :math:`dt=(b-a)dx/2`, thus we can develop our algorithms on the domain :math:`[-1,1]`, 
and then do the transformation back using: :math:`t=(b-a)x/2+(b+a)/2`.


.. note::
   The idea we will explore is as follows:
   If we can approximate the function to be integrated on the domain :math:`[-1,1]` (or on :math:`[a,b]`) as a 
   polynomial of as *large a degree as possible*, then the numerical integral of this polynomial will be very close to the integral of the 
   function we are seeking.



This idea is best understood by a couple of examples. Assume that we want to use :math:`N=1` in equation :ref:`(224) <Eq:eq:numint:qq1>`:

.. _Eq:_auto73:

.. math::

    \tag{227}
    \int_{-1}^{1}f(x)\,dx\simeq\omega_0f(x_0).
        
        

We now choose :math:`f(x)` to be a polynomial of as large a degree as possible, but with the requirement that the integral is exact. If :math:`f(x)=1`, we get:

.. _Eq:_auto74:

.. math::

    \tag{228}
    \int_{-1}^{1}f(x)\,dx=\int_{-1}^{1}1\,dx=2=\omega_0,
        
        

hence :math:`\omega_0=2`. If we choose :math:`f(x)=x`, we get:

.. _Eq:_auto75:

.. math::

    \tag{229}
    \int_{-1}^{1}f(x)\,dx=\int_{-1}^{1}x\,dx=0=\omega_0f(x_0)=2x_0,
        
        

hence :math:`x_0=0`. 

.. admonition:: The Gaussian integration rule for :math:`N=1` is

   
   .. math::
           
           \int_{-1}^{1}f(x)\,dx\simeq 2f(0)\text{, or: }\nonumber
           
   
   .. _Eq:_auto76:

.. math::

    \tag{230}
    \int_{a}^{b}f(t)\,dt\simeq\frac{b-a}{2}\,2f(\frac{b+a}{2})=(b-a)f(\frac{b+a}{2}).




This equation is equal to the midpoint rule, by choosing :math:`b=a+h` we reproduce equation :ref:`(194) <Eq:eq:numint:mid0>`. If we choose :math:`N=2`:

.. _Eq:_auto77:

.. math::

    \tag{231}
    \int_{-1}^{1}f(x)\,dx\simeq\omega_0f(x_0)+\omega_1f(x_1),
        
        

we can show that now $ f(x)=1,\,x,\,x^2\,x^3$ can be integrated exact:

.. _Eq:_auto78:

.. math::

    \tag{232}
    \int_{-1}^{1}1\,dx=2=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0+\omega_1\,,
        
        

.. _Eq:_auto79:

.. math::

    \tag{233}
    \int_{-1}^{1}x\,dx=0=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0+\omega_1x_1\,,
        
        

.. _Eq:_auto80:

.. math::

    \tag{234}
    \int_{-1}^{1}x^2\,dx=\frac{2}{3}=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0^2+\omega_1x_1^2\,,
        
        

.. _Eq:_auto81:

.. math::

    \tag{235}
    \int_{-1}^{1}x^3\,dx=0=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0^3+\omega_1x_1^3\,,
        
        

hence there are four unknowns and four equations. The solution is: :math:`\omega_0=\omega_1=1` and :math:`x_0=-x_1=1/\sqrt{3}`.


.. admonition:: The Gaussian integration rule for :math:`N=2` is

   
   .. _Eq:_auto82:

.. math::

    \tag{236}
    \int_{-1}^{1}f(x)\,dx\simeq f(-\frac{1}{\sqrt{3}})+f(\frac{1}{\sqrt{3}})\, \text{, or:}
           
           
   
   .. _Eq:_auto83:

.. math::

    \tag{237}
    \int_{a}^{b}f(x)\,dx\simeq \frac{b-a}{2}\left[f(-\frac{b-a}{2}\frac{1}{\sqrt{3}}+\frac{b+a}{2})
           +f(\frac{b-a}{2}\frac{1}{\sqrt{3}}+\frac{b+a}{2})\right].




.. code-block:: python

    def int_gaussquad2(func, lower_limit, upper_limit):
        x   = np.array([-1/np.sqrt(3.),1/np.sqrt(3)])
        w   = np.array([1, 1])
        xp  = 0.5*(upper_limit-lower_limit)*x
        xp += 0.5*(upper_limit+lower_limit)
        area = np.sum(w*func(xp))
        return area*0.5*(upper_limit-lower_limit)

The case N=3
~~~~~~~~~~~~

For the case :math:`N=3`, we find that :math:`f(x)=1,x,x^2,x^3,x^4,x^5` can be integrated exactly:

.. _Eq:_auto84:

.. math::

    \tag{238}
    \int_{-1}^{1}1\,dx=2=\omega_0+\omega_1+\omega_2\,,
        
        

.. _Eq:_auto85:

.. math::

    \tag{239}
    \int_{-1}^{1}x\,dx=0=\omega_0x_0+\omega_1x_1+\omega_2x_2\,,
        
        

.. _Eq:_auto86:

.. math::

    \tag{240}
    \int_{-1}^{1}x^2\,dx=\frac{2}{3}=\omega_0x_0^2+\omega_1x_1^2+\omega_2x_2^2\,,
        
        

.. _Eq:_auto87:

.. math::

    \tag{241}
    \int_{-1}^{1}x^3\,dx=0=\omega_0x_0^3+\omega_1x_1^3+\omega_2x_2^3\,,
        
        

.. _Eq:_auto88:

.. math::

    \tag{242}
    \int_{-1}^{1}x^4\,dx=\frac{2}{5}=\omega_0x_0^4+\omega_1x_1^4+\omega_2x_2^4\,,
        
        

.. _Eq:_auto89:

.. math::

    \tag{243}
    \int_{-1}^{1}x^5\,dx=0=\omega_0x_0^5+\omega_1x_1^5+\omega_2x_2^5\,,
        
        

the solution to these equations are :math:`\omega_{0,1,2}=5/9, 8/9, 5/9` and :math:`x_{1,2,3}=-\sqrt{3/5},0,\sqrt{3/5}`. Below is a Python implementation:

.. code-block:: python

    def int_gaussquad3(lower_limit, upper_limit,func):
        x  = np.array([-np.sqrt(3./5.),0.,np.sqrt(3./5.)])
        w  = np.array([5./9., 8./9., 5./9.])
        xp = 0.5*(upper_limit-lower_limit)*x
        xp += 0.5*(upper_limit+lower_limit)
        area = np.sum(w*func(xp))
        return area*0.5*(upper_limit-lower_limit)

Note that the Gaussian quadrature converges very fast. From :math:`N=2` to :math:`N=3` function evaluation we reduce the error (in this specific case) 
from 6.5% to 0.1%. Our standard trapezoidal formula needs more than 20 function evaluations to achieve this, the Romberg method uses 4-5 function
evaluations. How can this be? If we use the standard Taylor formula for the function to be integrated, we know that for :math:`N=2` the Taylor 
formula must be integrated up to :math:`x^3`, so the error term is proportional to :math:`h^4f^{(4)}(\xi)` (where :math:`\xi` is some x-value in :math:`[a,b]`). 
:math:`h` is the step size, and we can replace it with :math:`h\sim (b-a)/N`, thus the error scale as :math:`c_N/N^4` (where :math:`c_N` is a constant). 
Following the same argument, we find for :math:`N=3` that the error term is :math:`h^6f^{(6)}(\xi)` or that the error term scale as :math:`c_N/N^6`. 
Each time we increase :math:`N` by a factor of one, the error term reduces by :math:`N^2`. Thus if we evaluate the integral for :math:`N=10`, 
increasing to :math:`N=11` will reduce the error by a factor of :math:`11^2=121`.

Error term on Gaussian integration
----------------------------------

.. index::
   single: Gaussian quadrature, error term

The Gaussian integration rule of order :math:`N` integrates exactly a polynomial of order :math:`2N-1`. 
From Taylors error formula, see equation :ref:`(4) <Eq:eq:taylor:error>` in the chapter :ref:`ch:taylor`,
we can easily see that the error term must be of order :math:`2N`, and be proportional to :math:`f^{(2N)}(\eta)`, see [Ref07]_ for more details on the derivation of error terms. The drawback with an analytical error term derived from series expansion is that it involves the derivative of the function. As we have already explained, this is very unpractical and it is much more practical to use the methods described in the section :ref:`sec:numint:parct`. Let us consider this in more detail, assume that we evaluate the integral using first a Gaussian integration rule with :math:`N` points, and then :math:`N+1` points. Our estimates of the "exact" integral, :math:`I`,  would then be:

.. _Eq:eq:numint:gerr1:

.. math::

    \tag{244}
    I=I_N+ch_{N}^{2N},
        

.. _Eq:eq:numint:gerr2:

.. math::

    \tag{245}
    I=I_{N+1}+ch_{N+1}^{2N+1}.
        
        

In principle :math:`h_{N+1}\neq h_{N}`, but in the following we will assume that :math:`h_N\simeq h_{N+1}`, and :math:`h\ll 1`. Subtracting equation :ref:`(244) <Eq:eq:numint:gerr1>` and :ref:`(245) <Eq:eq:numint:gerr2>` we can show that a reasonable estimate for the error term :math:`ch^{2N}` would be:

.. _Eq:_auto90:

.. math::

    \tag{246}
    ch^N= I_{N+1}-I_N.
        
        

If this estimate is lower than a given tolerance we can be quite confident that the higher order estimate :math:`I_{N+1}` approximate the true integral within our error estimate. This is the method implemented in SciPy, `integrate.quadrature <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.quadrature.html>`__

Common weight functions for classical Gaussian quadratures
----------------------------------------------------------
Integrating functions over an infinite range
============================================

.. index::
   single: numerical integral, infinite

Integrating a function over an infinite range can be done by the following trick. Assume that we would like to evaluate

.. _Eq:eq:numint:inf:

.. math::

    \tag{247}
    \int_a^\infty f(x) dx.
        
        

If we introduce the following substitution

.. math::
        
        z=\frac{x-a}{1+x-a},
        
        
        or equivalently
        
        x=a+\frac{z}{1-z},
        
        

then if :math:`x=a`, :math:`z=0`, and if :math:`x\to\infty` then :math:`z\to1`, hence:

.. _Eq:eq:numint:infs3:

.. math::

    \tag{248}
    \int_a^\infty f(x) dx = \int_0^1 f(a+\frac{z}{1-z}) \frac{dz}{(1-z)^2}.
        
        

.. --- begin exercise ---

Exercise 6.1: Numerical Integration
-----------------------------------

**a)**
Show that for a linear function, :math:`y=a\cdot x+b` both the trapezoidal rule and the rectangular rule are exact

**b)**
Consider :math:`I(a,b)=\int_a^bf(x)dx` for :math:`f(x)=x^2`. The analytical result is :math:`I(a,b)=\frac{b^3-a^3}{3}`. Use the Trapezoidal and 
  Midpoint rule to evaluate these integrals and show that the error for the Trapezoidal rule is exactly twice as big as the Midpoint rule.

**c)**
Use the fact that the error term on the trapezoidal rule is twice as big as the midpoint rule to derive Simpsons formula: :math:`I(a,b)=\sum_{k=0}^{N-1}I(x_k,x_k+h)=\frac{h}{6}\left[f(a)+ 4f(a+\frac{h}{2})+2f(a+h)+4f(a+3\frac{h}{2})+2f(a+2h)+\cdots+f(b)\right]` Hint: :math:`I(x_k,x_k+h)=M(x_k,x_k+h)+E_M` (midpoint rule) and :math:`I(x_k,x_k+h)=T(x_k,x_k+h)+E_T=T(x_k,x_k+h)-2E_M` (trapezoidal rule).

.. --- begin solution of exercise ---

**Solution.**
Simpsons rule is an improvement over the midpoint and trapezoidal rule. It can be derived in different ways, we will make use of 
the results in the previous section. If we assume that the second derivative is reasonably well behaved on the interval :math:`x_k` 
and :math:`x_k+h` and fairly constant we can assume that :math:`f^{\prime\prime}(\eta)\simeq f^{\prime\prime}(\overline{\eta})`, hence :math:`E_T=-2E_M`.

.. _Eq:_auto91:

.. math::

    \tag{249}
    I(x_k,x_k+h)=M(x_k,x_k+h)+E_M\text{ (midpoint rule)}
        
        

.. math::
          
        I(x_k,x_k+h)=T(x_k,x_k+h)+E_T\nonumber
        

.. _Eq:_auto92:

.. math::

    \tag{250}
    =T(x_k,x_k+h)-2E_M\text{ (trapezoidal rule)},
        
        

we can now cancel out the error term by multiplying the first equation with 2 and adding the equations:

.. _Eq:_auto93:

.. math::

    \tag{251}
    3I(x_k,x_k+h)=2M(x_k,x_k+h)+T(x_k,x_k+h)
        
        

.. _Eq:_auto94:

.. math::

    \tag{252}
    =2f(x_k+\frac{h}{2}) h+\left[f(x_k+h)+f(x_k)\right] \frac{h}{2}
        
        

.. _Eq:_auto95:

.. math::

    \tag{253}
    I(x_k,x_k+h)=\frac{h}{6}\left[f(x_k)+4f(x_k+\frac{h}{2})+f(x_k+h)\right].
        
        

Now we can do as we did in the case of the trapezoidal rule, sum over all the elements:

.. math::
        
        I(a,b)=\sum_{k=0}^{N-1}I(x_k,x_k+h)\nonumber
        

.. math::
          
        =\frac{h}{6}\left[f(a)+ 4f(a+\frac{h}{2})+2f(a+h)+4f(a+3\frac{h}{2})\right.\nonumber
        

.. _Eq:_auto96:

.. math::

    \tag{254}
    \left.\qquad+2f(a+2h)+\cdots+f(b)\right]
        
        

.. _Eq:_auto97:

.. math::

    \tag{255}
    =\frac{h^\prime}{3}\left[f(a)+ f(b) + 4\sum_{k= \text{odd}}^{N-2}f(a+k h^\prime)+2\sum_{k= \text{even}}^{N-2}f(a+k h^\prime)\right],
        
        

note that in the last equation we have changed the step size :math:`h=2h^\prime`.

.. --- end solution of exercise ---

**d)**
Show that for :math:`N=2` (:math:`f(x)=1,x,x^3`), the points and Gaussian quadrature rule for :math:`\int_{0}^{1}x^{1/2}f(x)\,dx`
is :math:`\omega_{0,1}=-\sqrt{70}{150} + 1/3, \sqrt{70}{150} + 1/3`
and :math:`x_{0,1}=-2\sqrt{70}{63} + 5/9, 2\sqrt{70}{63} + 5/9`
1. Integrate :math:`\int_0^1x^{1/2}\cos x\,dx` using the rule derived in the exercise above and compare with the standard Gaussian quadrature rule for (:math:`N=2`, and :math:`N=3`).

**e)**
Make a Python program that uses the Midpoint rule to integrate experimental data that are unevenly spaced and given in the form of two arrays.

.. --- end exercise ---

