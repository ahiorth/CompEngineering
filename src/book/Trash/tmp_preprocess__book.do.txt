# Note on the Springer T4 style: here we use the modifications
# introduced in t4do.sty and svmonodo.sty (both are bundled with DocOnce).

TITLE:  Modeling and Computational Engineering 
AUTHOR: Aksel Hiorth, University of Stavanger
DATE: today

<% book = True%>
<% all  = False%>
__Summary.__
(Work in Progress) The purpose of this document is to explain how computers solve mathematical models.
Many of the most common numerical methods are presented, we show how to implement them in Python, and discuss their limitations.
The mathematical formalism is kept to a minimum. All the material is available at
"github":"https://github.com/ahiorth/CompEngineering". For each of the chapters there is a Jupyter "notebook":"https://github.com/ahiorth/CompEngineering/tree/master/pub/chapters". This makes it possible to run all the codes in this document.
We strongly recommend to install "the Anaconda Python distribution":"https://www.anaconda.com/". All documents have been prepared using "doconce":"https://github.com/ahiorth/CompEngineering/tree/master/pub/chapters".


##% if FORMAT == 'html':
##FIGURE: [wip, width=200]
##Work in progress
##% endif

## Handy mako variables and functions for the preprocessing step
## Mako variables and functions common to a lot of files
## (this file is included in, e.g., flow/main_flow.do.txt).

<%
# Note that goo.gl and bitly URLs cannot have slashes and continuation,
# therefore we use tinyurl.

src_path = 'https://github.com/hplgit/setup4book-doconce/tree/master/doc/src/chapters'
src_path = 'http://tinyurl.com/kukz8pt'

doc_path = 'http://hplgit.github.io/setup4book-doconce/doc/pub'
doc_path = 'http://tinyurl.com/oul3xhn'

#doc_index = doc_path + '/index.html'

# All chapters: nickname -> title dict
chapters = {
 'rules': 'Directory and file structure',
 'fake': 'Some document',
 'mako': 'Use of Mako to aid book writing',
}

%>

## Externaldocuments: ../chapters/fake/main_fake

!split
========= Preface  =========
label{ch:preface}
This book is  work in progress. It started out as a traditional book on numerical methods and numerical analysis, but I am currently working on bringing the modeling aspects more into focus.

The book is divided into chapters where I cover numerical methods. A numerical algorithm is a set of instructions executed in a specific order to find the solution to a mathematical or a physical problem. In each chapter, I try to explain and derive many of the standard numerical methods from first principles in a way that require little mathematical knowledge. It is important to understand the origin of numerical algorithms, because a numerical algorithm will only in a special case solve the underlying mathematical problem exact. We usually compare a computer generated solution of a model to data, you need to be able to tell if the mismatch is due to numerical errors introduced by the specific algorithm, or missing physical effects in the original mathematical model. Sometimes the numerical algorithm is unstable, i.e. it converges to the wrong solution. Thus, you need to understand the limitations of the algorithm. Another motivation to learn about numerical algorithms, is that they are *not that difficult* to implement, they usually follow a very similar pattern, but there are some ''tricks''. It is extremely useful to learn these tricks, it will put you into a position where you can combine different methods and optimize them for your specific problem.

In the course I teach, the theory is combined with modeling projects. I am currently working on making the modeling part more explicit in this book. Modeling is an art, and there are many models that can be used to simulate a phenomenon. In my experience, sometimes people use models that are too complicated compared to what they want to model. By complicated, I mean models that require a lot (sometimes too much) information about a system compared to the actual measuring data available. I believe that simplified models should be used in an exploratory manner to understand a certain phenomenon, before applying more complicated models. What many people realize is that a simple model that does not match data, can in many cases be more valuable than a complicated model that match data. If a simple model does not match data, it means that we have ignored important effect(s). This in turn mean that the effects or the physics we thought where important, might not be the most important after all. We have learned that our understanding is incomplete and we should search for the missing effects (or take another look at the quality of the data). The key point is that we should always ask ourselves ''What have we learned?''.

However, it is not easy to learn modeling and it is even harder if one use models developed by other people. The only way to learn is to do it yourself, test different mathematical models of a phenomenon, and compare to data. In this process you will also be able to understand the strength and limitations of models, or to use the famous quote by G. E. Box ''All models are wrong, but some are useful''.

Modeling can be broken down into several parts: 
o Identify important physical phenomena. This involves abstraction, formulate the key physical mechanisms as mathematical equations. In this process, it is important to interact with people who have domain knowledge.
o Implement the model, solve and validate it against known analytical solutions. Analytical solutions exists when one simplify the system.
o Calibrate model parameters against data. A simple model would have much less parameters than a complicated model, hence it is much harder to match data. After this step, one usually needs to identify effects missing or remove effects that might not be important.


*Aksel Hiorth, September 2022

TOC: on

!split
========= Introduction to Python  =========
label{ch:pyt}

There are plenty of online resources if you want get an overview of Python syntax, such as cite{vanderplas2016whirlwind}, for which the full book is available on "github":"https://github.com/jakevdp/WhirlwindTourOfPython".

In this chapter I will try to focus on the key part of Python that you will need in this book, and maybe more importantly, give some advice on how to write good, reusable, Python code. As you might have discovered tasks can be solved in many different ways in Python. This is clearly a strength because you would most likely be able to solve any task thrown at you. On the other hand it is a weakness, because code can get messy and hard to follow, especially if you solve the same task in different parts of your code using different libraries or syntax.

In this chapter, I will explain how I tend to solve some common tasks, in the process we will also cover some stuff that you should know. If you need more information on each topic below, there are plenty of online sources. 

The code examples are meant as an inspiration, and maybe you do not agree and have solutions that you think are much better. If that is the case, I would love to know, so that I can update this chapter.

!bnotice Speed and readability
Many people are concerned about speed or execution time. My advice is to focus on readable code, and get the job done. When the code is working, it is very easy to go back and change parts that are slow. Remember that you should test the code on big problems, sometimes the advantage of Numpy or Scipy is not seen before the system is large. You can use the magic commands `%timeit` to check performance of different functions. You can also use "Numba":"https://numba.pydata.org/", which translate python code into optimized machine code. 
!enotice

======= Personal guidelines  =======
It is important to have some guidelines when coding, and for Python there are clear style guides called "PEP 8":"https://www.python.org/dev/peps/pep-0008/". Take a look at the official guidelines, and  make some specific rules for yourself, and stick to them. The reason for this is that if you make a large program or library, people will recognize your style and it is easier to understand the code. If you are working in a team, it is even more important - try to agree on some basic rules, for example:

__Code Guidelines:__
* Variable names should be meaningful.
* Naming of variables and functions. Should you write `def my_function(...):` or `def MyFunction(..)`, i.e are words separated by an underscore or a capital letter? Personally I use capital letters for class definition, and underscore for function definitions.
* (Almost) always use doc strings, you would be amazed how easy it is to forget what a function does. Shorter (private) functions usually do not need comments or doc strings, if you use good variable names - it should be easy to understand what is happening by just looking at the code.
* Inline comments should be used sparingly, only where strictly necessary.
* Strive to make code general, in particular not type specific, e.g. in Python it is easy to make functions that will work if a list (array) or a single value is passed.
* Use exception handling, in particular for larger projects.
* DRY - Do not Repeat Yourself cite{thomas2019pragmatic}. If you need to change the code in more than one place to extend it, you may forget to change everywhere and introduce bugs.
* The DRY principle also applies to *knowledge sharing*, it is not only about copy and paste code, but knowledge should only be represented in one place. 
* Import libraries using the syntax `import library as ..`, Numpy would typically be `import numpy as np`. The syntax `from numpy import *` could lead to conflicts between modules as functions could have the same name in two different modules.

__Work Guidelines:__
* Do not copy and paste code without understanding it. It is OK to be inspired by others, but be aware that in some cases code examples are unnecessary complicated. Too much copy and paste will result in a code with a mix of different styles. 
* Stick to a limited number of libraries. I try to do as much as possible with "Numpy":"https://numpy.org/", "Pandas":"https://pandas.pydata.org/", and  "Matplotlib":"https://matplotlib.org/".
* Unexpected behavior of functions. Functions should be able to discover if something is wrong with arguments, and give warnings.

===== Code editor =====
You would like to use an editor that gives you some help. This is particularly useful when you do not remember a variable or function name, you can guess  the name and a drop down list will appear which will let you pick the name or function you want. If you enter  a function name, the editor will write some useful information about the function, some screenshots are shown in figure ref{fig:py:vscode}.

FIGURE: [fig-python/vscode.png, width=400 frac=1.0] A screenshot of vscode (left) the editor helps to identify which variable name you want, (right) the editor shows relevant information about the function you would like to call. label{fig:py:vscode}

Currently my favorite editor is "vscode":"https://code.visualstudio.com/", it can be used for any language, and there are a lot of add ins that can be installed to make the coding experience more pleasant. Spyder is also a very good alternative, but this editor is mainly for Python. It takes some time to learn how an editor works, so it is good if the editor can be used for multiple purposes.  However, always be open to new ideas and products, it will only make you more efficient. As an example, in some cases you will have a particular difficult error in the code, and then it could help to open and run that code in a different editor, you might get a slightly different error messages, which could help you locate the error.

======= Types in Python =======
idx{types}
In Python you do not need to define types as in a compiled language. In many ways one can say that Python only has *one* type.  To not define types is generally an advantage as it lets you write code with fewer lines, and it is easier to write functions that will work with any kind of type. As an example, in the programming language C, if you want to write a function that lets you add two numbers, you have to write one version if the arguments are integers and one version if the arguments are floats.

The way that Python store and organize data is called a data model, and it is well described in the "official documentation":"https://docs.python.org/3/reference/datamodel.html". The important point is that all data in Python is an object or a relation between objects. The `is` operator can be used to check if two objects have the same identity, that means they are the same object. The `id` operator gives an unique integer value for the object, and if two objects have the same id number they are the same object, e.g.
!bc pypro
y=10
x=y
x is y # gives true
print(id(x))
print(id(y)) # prints the same integer as id(x)
!ec
For those familiar with C or C++, one would first have to define `x` and `y` as the type `int` and then they would already have a different place in memory and they can *never* be the same (even if they contain the same number). We will return to this point in more detail discussing lists and arrays in Python, as it can lead to unexpected behavior.

Another thing you might have experienced during Python coding is that you get error messages that refer to pieces of code that you have no knowledge of. This can happen when you pass the wrong type (e.g. a string instead of a number). Since Python only has one type, the wrong type will not be discovered before it is actually used. This error could be deep into some other library that you have no knowledge of. 

===== Basic types =====
idx{basic types}
I will assume that you are familiar with the common types like floats (for real numbers), strings (text, lines, word, a character), integer (whole numbers), Boolean (True, False). What is sometimes useful is to be able to test what kind of type a variable is, this can be done with `type()`
!bc pypro
my_float = 2.0 
my_int   = 3
my_bool  = True
print(type(my_float))
print(type(my_int))
print(type(my_bool))
!ec
The output of the code above will be `float, int, bool`. If you want to test the type  of a variable you can do
!bc pypro
if isinstance(my_int,int):
    print('My variable is integer')
else:
    print('My variable is not integer')
!ec
Python also has build in support for complex numbers. An example are `1+2j`, `j` is the imaginary part of the complex number. Note there is no multiplication sign between 2 and `j`.

===== Lists =====

idx{lists} idx{list comprehension}
Lists are extremely useful, and they have very nice syntax that in my opinion is more elegant than Numpy arrays. Whenever you want to do more than one thing with only a slight change between the elements, you should think of lists. Lists are defined using the square bracket `[]` symbol
!bc pypro
my_list = []      # an empty list
my_list = []*10   # still an empty list ...
my_list = [0]*10  # a list with 10 zeros
my_list = ['one', 'two','three'] # a list of strings
my_list = ['one']*10 # a list with 10 equal string elements 
!ec
!bnotice
To get the first element in a list, we do e.g. `my_list[0]`. In a list with 10 elements the last element would be `my_list[9]`, the length of a list can be found by using the `len()` function, i.e. `len(my_list)=10`. Thus, the last element can also be found by doing `my_list[len(my_list)-1]`. However, in Python you can always get the last element by doing `my_list[-1]`, the second last element would be `my_list[-2]` and so on.
!enotice
Sometimes you do not want to initialize the list with everything equal, and it can be tiresome to write everything out yourself. If that is the case you can use *list comprehension*
!bc pypro
my_list = [i for i in range(10)] # a list from 0,1,..,9
my_list = [i**3 for i in range(10)] # a list with elements 0,1,8, ..,729
!ec
We will cover the for loop below, but basically what is done is that the statement `i in range(10)`, gives `i` the value 0, 1, $\ldots$, 9 and the first `i` inside the list tells python to use that value as the element in the list. Using this syntax, there are plenty of opportunities to initialize. Maybe you want to pick from a list words that contain a particular subset of characters
!bc pypro
my_list  = ['hammer', 'nail','saw','lipstick','shirt']
new_list = [i for i in my_list if 'a' in i]
!ec
Now `new_list=['hammer', 'nail', 'saw']`.

=== List arithmetic ===
I showed you some examples above, where we used multiplication to create a list with equal copies of a single element, you can also join two lists by using addition
!bc pypro
my_list  = ['hammer','saw']
my_list2 = ['screw','nail','glue']
new_list = my_list + my_list2
!ec
Now `new_list=['hammer', 'saw', 'screw', 'nail', 'glue']`, we can also multiply the list with an integer and get a larger list with several copies of the original list.

=== List slicing ===
Clearly we can access elements in a list by using the index to the element, i.e. first element is `my_list[0]`, and the last element is `my_list[-1]`. Python also has very nice syntax to pick out a subset of a list. The syntax is `my_list[start:stop:step]`, the step makes it possible to skip elements
!bc pypro
my_list=['hammer', 'saw', 'screw', 'nail', 'glue']
my_list[:]      # ['hammer', 'saw', 'screw', 'nail', 'glue']
my_list[1:]     # ['saw', 'screw', 'nail', 'glue']
my_list[:-1]    # ['hammer', 'saw', 'screw', 'nail']
my_list[1:-1]   # ['saw', 'screw', 'nail']
my_list[1:-1:2] # ['saw','nail']
my_list[::1]    # ['hammer', 'saw', 'screw', 'nail', 'glue']
my_list[::2]    # ['hammer', 'screw', 'glue']
!ec

Sometimes you have lists of lists, if you want to get e.g. the first element of each list you cannot access those elements using list slicing, you have to use a for loop or list comprehension
!bc pypro
my_list  = ['hammer','saw']
my_list2 = ['screw','nail','glue']
new_list=[my_list,my_list2]
# extract the first element of each list
new_list2 = [ list[0] for list in new_list]
!ec
`new_list2=['hammer','screw']`

!bnotice When to use lists
Use lists if you have mixed types, and as storage containers. Be careful when you do numerical computation not to mix lists and Numpy arrays. Adding two lists e.g. `[1,2]+[1,1]`, will give you `[1,2,1,1]`, whereas adding two Numpy arrays will give you `[2,3]`.
!enotice

===== Numpy arrays  =====
Numpy arrays are awesome, and they should be your preferred choice when doing numerical operations. We import Numpy as `import numpy as np`, some examples of initialization
!bc pypro
my_array=np.array([0,1,2,3]) # initialized from list
my_array=np.zeros(10) # array with 10 elements equal to zero
my_array=np.ones(10)  # array with 10 elements equal to one
!ec
A typical use of Numpy arrays is when you want to create equally spaced numbers to evaluate a function, this can be done in (at least) two ways
!bc pypro
my_array=np.arange(0,1,0.2) # [0, 0.2, 0.4, 0.6, 0.8]
my_array=np.linspace(0,1,5) # [0., 0.25, 0.5, 0.75, 1.]
!ec
Note that in the second case, the edges of the domain (0,1) are included while in the first case the upper edge is not. 

!bnotice Do not mix Numpy arrays and lists in functions
If a function is written to use  Numpy arrays as *arguments*, make sure that it *returns* Numpy arrays. If you have to use a list inside the function to e.g. store the results of a calculation, convert the list to a Numpy array before returning it by `np.array(my_list)`.  
!enotice

=== Array slicing ===
You can access elements in Numpy arrays in the same way as lists, the syntax is `my_array[start,stop,step]`
!bc pypro
my_array=np.arange(0,6,1)
my_array[:]      # [0,1,2,3,4,5]
my_array[1:]     # [1,2,3,4,5]
my_array[:-1]    # [0,1,2,3,4]
my_array[1:-1]   # [1,2,3,4]
my_array[1:-1:2] # [1,3]
my_array[::2]    # [0,2,4]
!ec
However, as opposed to lists all the basic mathematical operations addition, subtraction, multiplication are meaningful (*if the arrays have equal length, or shape*)
!bc
my_array  = np.array([0,1,2])
my_array2 = np.array([3,4,5])
my_array+my_array2 # [3,5,7]
my_array*my_array2 # [0,4,10]
my_array/my_array2 # [0,.25,.4]
!ec
Note that these operations do what you would expect them to do. If you have arrays of arrays, you can easily access elements in the arrays
!bc
my_array  = np.array([[0,1,2],[3,4,5]]) # shape 2x3 matrix
my_array[0,:] # [0,1,2] First row
my_array[1,:] # [3,4,5] Second row
my_array[:,0] # [0,3] First column
my_array[:,1] # [1,4] Second column
!ec
Not the extra `[]` in the definition of `my_array`.  Numpy arrays have a shape property, which makes it very easy to create different matrices. The array `[0,1,2,3,4,5]` has shape (6,), but we can change the shape to create e.g. a $2\times3$ matrix
!bc pypro
my_array  = np.array([0,1,2,3,4,5])
my_array.shape = (2,3) # [[0,1,2],[3,4,5]] 2 rows and 3 columns
my_array.shape = (3,2) # [[0,1],[2,3],[4,5]] 3 rows and 2 columns
!ec

===== Dictionaries =====
If you have not used dictionaries before they might feel unnecessary, but if you get used to them and their syntax, they can make your code much more flexible and easier to expand. You should use dictionaries, when you have data sets that you want to access fast. A very good mental image to have is an excel sheet where data are organized in columns. Each column has a header name, or a *key*. Assume we have the following table

|-----------------------------------|
| A        | B           | C        |
|----c-----------c------------c-----|
|  1.0     | 2.0         | 3.0      |
|  4.0     | 5.0         |          |
|  6.0     | 7.0         |          |
|-----------------------------------|

This could be represented as a dictionary as
!bc pypro
my_dict={'A':[1.0,4.0,6.0],'B':[2.0,5.0,7.0],'C':[3.0]}
!ec
The syntax is `{key1:values, key2:values2, ...}`. We access the values in the dictionary by the key i.e. `print(my_dict['A'])` would print `[1.0,4.0,6.0]`. If you want to print out all the elements in a dictionary, you can use a for loop (see next section for more details about for loops)
!bc pypro
for key in my_dict:
    print(key, my_dict[key])
!ec

======= Looping =======
There are basically two ways of iterating through lists or to do a series of computations, using a for-loop or a while-loop. In most cases a for loop can also be written as a while loops and vice versa. You would typically use a for-loop when you are iterating over a fixed number of elements, very typical example is when we are iterating in a numerical computation from time zero to the end time. A while-loop is typically used when we do not know before the run time when to stop, this could be that we are waiting for user input or to reach a certain numerical accuracy in our calculation before proceeding.


===== For loops =====
A typical example of a for loop is to loop over a list and do something, and maybe during the execution store the results in a list
!bc pypro
numbers=['one','two','three','one','two']
result=[] # has to be declared as empty
for number in numbers:
    if number == 'one':
       result.append(1) 
!ec
The result of this code is `result=[1, 1]`. The `number` variable changes during the iteration, and takes the value of each element in the list. Note that I use `numbers` for the list and `number` as the iterator, this makes it quite easy to read and understand the code. In many cases you want to have the index, not only the element in the list
!bc pypro
numbers  = ['one','two','three','one','two']
numerics = [  1  ,  2  ,   3   , 1   , 2   ]
result=[] # has to be declared as empty
for idx,number in enumerate(numbers):
    if number == 'one':
       result.append(numerics[idx]) 
!ec
The result of this code is `result=[1, 1]`. In this case the function `enumerate(numbers)` returns two values: the index, which is stored in `idx`, and the value of the list element, which is stored in `number`.

A more elegant way to achieve the same results without using the `enumerate()` function is to use `zip`
!bc pypro
numbers  = ['one','two','three','one','two']
numerics = [  1  ,  2  ,   3   , 1   , 2   ]
result=[] # has to be declared as empty
for numeric,number in zip(numerics,numbers):
    if number == 'one':
       result.append(numeric) 
!ec
The `zip` function can be used with several lists of same length.

In many cases you might be in a situation that you want to plot more than one function in a plot. It is then very tempting to copy and paste the previous code, but it is more elegant to use a for loop and lists
!bc pypro
import numpy as np
import matplotlib.pyplot as plt
x_val   = np.linspace(0,1,100) # 100 equal spaced points from 0 to 1
y_vals  = [x_val,x_val*x_val]
labels  = [r'x', r'$x^2$']
cols    = ['r','g']
points  = ['-*','-^']
for y_val,point,col,label in zip(y_vals,points,cols,labels):
    plt.plot(x_val,y_val,point,c=col,label=label)
plt.grid()
plt.legend()
plt.show()
!ec
Output of code is shown in figure ref{fig:python:loop}.

FIGURE: [fig-python/plt_loop, width=400 frac=1.0] Output of code. label{fig:python:loop}
===== While loops =====
A while loop is used whenever you do not know before run time when to stop iterating. The syntax of the while loop is to do something while a condition is true
!bc pypro
import numpy as np
finished = False
sum =0
while not finished:
      sum += np.random.random() #returns a random number between 0,1
      if sum >= 10.:
      	 finished = True
!ec
In some cases we are iterating from $t_0$, $t_1$, etc. to a final time $t_f$, if we use a fixed time step, $\Delta t$, we can calculate the number of steps i.e $N= \text{int} ((t_f-t_0)/\Delta t)$, and use a for loop. On the other hand, in a more fancy algorithms we can change the time step as the simulation proceeds and then we need to choose a while loop, e.g. `while t0 <= tf:`. 
===== Functions in Python =====
When to use functions? There is no particular rule, *but whenever you
start to copy and paste code from one place to another, you should
consider to use a function*. Functions makes the code easier to read.
It is not easy to identify which part of a program is a good candidate
for a function, it requires skill and experience. Most likely you will
end up changing the function definitions as your program develops.

!bnotice Use short functions
Short functions makes the code easier to read. Each function has a particular task, and it does only one thing. If functions do too many tasks there is a chance that you will have several functions doing some of the same operations. Whenever you want to extend the program, you may have to make changes several places in the code. The chance then is that you will forget to do the change in some of the functions and introduce a bug.
!enotice

===== Defining a mathematical function =====
Throughout this course you will write many functions that do mathematical operations. In many cases, you would also pass a function to another function to make your code more modular. Lets say we want to calculate the derivative of $\sin x$, using the most basic definition of a derivative $f^\prime(x) = f(x+\Delta x)-f(x)/\Delta x$, we could do it as
!bc pypro
def derivative_of_sine(x,delta_x):
    ''' returns the derivative of sin x '''
    return (np.sin(x+delta_x)-np.sin(x))/delta_x

print('The derivative of sinx at x=0 is :', derivative_of_sine(0,1e-3))
!ec
We will discuss in a later chapter why $\Delta x=10^{-3}$ is a reasonable choice. 
If we would like to calculate the derivative at multiple points, that is straightforward since we have used the Numpy version of $\sin x$.
!bc pypro
x=np.array([0,.5,1])
print('Derivative of sinx at x=0,0.5,1 is :', derivative_of_sine(x,1e-3))
!ec
The challenge with our implementation is that if we want to calculate the derivative of another function we have to implement the derivative rule again for that function. It is better to have a separate function that calculates the derivative
!bc pypro
def f(x):
    return np.sin(x)

def df(x,f,delta_x=1e-3):
    ''' returns the derivative of f '''
    return (f(x+delta_x)-f(x))/delta_x
print('Derivative of sinx at x=0 is :', df(0,f))
!ec
Note also that we have put `delta_x=1e-3` as a *default argument*. Default arguments have to come at the end of the argument lists, `df(x,delta_x=1e-3,f)` is not allowed. All of this looks well, but what you would experience is that your functions would not be as simple as $\sin x$. In many cases your functions need additional arguments to be evaluated e.g.:
!bc pypro
def s(t,s0,v0,a):
    '''
    t  : time
    s0 : initial starting point
    v0 : initial velocity
    a  : acceleration
    returns the distance traveled
    '''
    return s0+v0*t+a*t*t*0.5 #multiplication (0.5)is general faster
    	   		     #than division (2)
!ec
How can we calculate the derivative of this function? If we try to do `df(1,s)` we will get the following message
!bc pypro
TypeError: s() missing 3 required positional
	   arguments: 's0', 'v0', and 'a'
!ec
This happens because the `df` function expect that the function we send into the argument list has a call signature `f(x)`. What many people do to avoid this error is to use global variable, that is to define `s0, v0`, and `a` at the top of the code. This is not always the best solution. Python has a special variable `*args` which can be used to pass multiple arguments to your function, thus if we rewrite `df` like this
!bc pypro
def df(x,f,*args,delta_x=1e-3):
    ''' returns the derivative of f '''
    return (f(x+delta_x,*args)-f(x,*args))/delta_x
!ec
we can do (assuming `s0=0`, `v0=1`, and `a=9.8`)
!bc pypro
print('The derivative of sinx at x=0 is :', df(0,f))
print('The derivative of s(t) at t=1 is :', df(0,s,0,1,9.8))
!ec

===== Scope of variables  =====
In small programs you would not care about scope, but once you have several functions, you will easily get into trouble if you do not consider the scope of a variable. By scope of a variable we mean where the variable is available, first some simple examples


__A variable created inside a function is only available within the function:__
``

!bc pypro
def f(x):
    a=10
    b=20
    return a*x+b
!ec
Doing `print(a)` outside the function will throw an error: `name 'a' is not defined`. What happens if we define variable `a` outside the function?
!bc pypro
a=2
def f(x):
    a=10
    b=20
    return a*x+b
!ec
If we first call the function `f(0)`, and then do `print(a)` Python would give the answer `2`, *not* `10`. A *local* variable `a` is created inside `f(x)`, that does not interfere with the variable `a` defined outside the function.

__The `global` keyword can be used to pass and access variables in functions:__

``

!bc pypro
global a
a=2
def f(x):
    global a
    a=10
    b=20
    return a*x+b
!ec
In this case `print(a)` *before* calling `f(x)` will give the answer `2` and *after* calling `f(x)` will give `10`.

!bnotice Use of global variables
Sometimes global variables can be very useful, and help you to make the code simpler. But make sure to use a *naming convention*  for them, e.g. end all the global variables with an underscore. In the example above we would write `global a_`. A person reading the code would then know that all variables ending with an underscore are global, and can potentially be modified by several functions.  
!enotice

===== Passing arrays and lists to functions =====
In the previous section, we looked at some simple examples regarding the scope of variables, and what happened with that variable inside and outside a function. The examples used integer or floats. However in most applications you will pass an array or a list to a function, and then you need to be aware that the behavior is not always would you might expect.
!bnotice Unexpected behavior
Sometimes functions do not do what you expect, this might be because the function does not treat  the arguments as you might think. The best advice is to make a very simple version of your function and test it for yourself. Is the behavior what you expect? Try to understand why or why not.
!enotice
Let us look at some examples, and try to understand what is going on and why.
!bc pypro
x=3
def f(x):
    x = x*2
    return x
print('x =',x)
print('f(x) returns ', f(x))
print('x is now ', x)
!ec
In the example above we can use `x=3`, `x=[3]`, `x=np.array([3])`, and after execution `x` is unchanged (i.e. same value as before `f(x)`) was called. Based on what we have discussed before, this is maybe what you would expect, but if we now do
!bc pypro
x=[3]
def append_to_list(x):
    return x.append(1)
print('x = ',x)
print('append_to_list(x) returns ', append_to_list(x))
print('x is now ', x)
!ec
(Clearly this function will only work for lists, due to the append command.) After execution, we get the result
!bc pypro
x = [3]
append_to_list(x) #returns  [3 1], x is now  [3, 1]
!ec
Even if this might be exactly what you wanted your function to do, why does `x` change here when it is a list and not in the previous case when it is a float? Before we explain this behavior let us rewrite the function to work with Numpy arrays
!bc pypro
x=np.array([3])
def append_to_np(x):
    return np.append(x,1)
print('x = ',x)
print('append_to_np(x) returns ', append_to_np(x))
print('x is now ', x)
!ec
The output of this code is
!bc pypro
x =  np.array([3])
append_to_np(x) #returns  [3 1], x is now  [3]
!ec
This time `x` was not changed, what is happening here? It is important to understand what is going on because it deals with how Python handles variables in the memory. If `x` contains million of values, it can slow down your program, if we do
!bc pypro
N=1000000
x=[3]*N
%%timeit append_to_list(x)
x=np.array([3]*N)
%%timeit append_to_np(x)
!ec
On my computer I found that `append_to_list` used 76 nano seconds, and `append_to_np`
used 512 micro seconds, the Numpy function was about 6000 times slower! To add to the confusion consider the following functions
!bc pypro
x=np.array([3])
def add_to_np(x):
    x=x+3
    return x

def add_to_np2(x):
    x+=3
    return x
print('x = ',x)
print('add_to_np(x) returns ', add_to_np(x))
print('x is now ', x)

print('x = ',x)
print('add_to_np2(x) returns ', add_to_np2(x))
print('x is now ', x)
!ec
The output is
!bc pypro
x =  np.array([3])
add_to_np(x) #returns [6], x is now [3] 
x =  np.array([3])
add_to_np2(x) #returns  [6], x is now  [6]
!ec
In both cases the function returns what you expect, but it has an unexpected (or at least a different) behavior regarding the variable `x`. What about speed?
!bc pypro
N=10000000
x=np.array([3]*N)
%%timeit add_to_np(x)
x=np.array([3]*N)
%%timeit add_to_np2(x)
!ec
`add_to_np` is about twice as slow as `add_to_np2`. In the next section we will try to explain the difference in behavior.

!bnotice Avoiding unwanted behavior of functions
The examples in this section are meant to show you that if you pass an array to a function, the array can be altered outside the scope of the function. If this is not what you want, it could lead to bugs that are hard to detect. Thus, if you experience unwanted behavior pick out the part of function involving list or array operations and test one by one in the editor.
!enotice

===== Call by value or call by reference  =====
For anyone that has programmed in C or C++ call by reference or value is something one need to think about constantly. When we pass a variable to a function there are two choices, should we pass a copy of the variable or should we pass information about where the variable is stored in memory?
!bnotice Value and reference
In C and C++ pass by value means that we are making a copy in the memory of the variable we are sending to the function, and pass by reference means that we are sending the actual parameter or more specific the address to the memory location of the parameter. In Python all variables are passed by object reference.
!enotice
In C and C++ you always tell in the function definition if the variables are passed by value or reference. Thus if you would like a change in a variable outside the function definition, you pass the variable by reference, otherwise by value. In Python we always pass by (object) reference.

=== Floats and integers ===
To gain a deeper understanding, we can use the `id` function, the `id` function gives the unique id to a variable. In C this would be the actual memory address, lets look at a couple of examples
!bc pypro
a=10.0
print(id(a)) #gives on my computer 140587667748656
a += 1
print(id(a)) #gives on my computer 140587667748400
!ec
Thus, after adding 1 to `a`, `a` is assigned *a new place in memory*. This is very different from C or C++, in C or C++ the variable, once it is created, *always has the same memory address*. In Python this is not the case, it works in the opposite way. The statement `a=10.0`, is executed so that *first* 10.0 is created in memory, secondly `x` is assigned the reference to 10.0. The assignment operator `=` indicates that `a` should point to whatever is on the right hand side. Another example is     
!bc pypro
a=10.0
b=10.0
print(a is b) # prints False
b=a
print(a is b ) # prints True
!ec
In this case 10.0 is created in two different places in the memory and a different reference is assigned to `a` and `b`. However if we put `b=a`, `b` points to the same object as `a` is pointing on. More examples
!bc pypro
a=10
b=a
print(a is b) # True
a+=2
print(a is b) # False
!ec
When we add 2 to `a`, we actually add 2 to the value of 10, the number 12 is assigned a new place in memory and `a` will be assigned that object, whereas `b` would still points the old object 10.

=== Lists and arrays ===
Yous should think of lists and arrays as containers (or a box). If we do
!bc pypro
x=[0,1,2,3,4]
print(id(x))
x[0]=10
print(id(x)) # same id value as before and x=[10,1,2,3,4]
!ec
First, we create a list, which is basically a box with the numbers 0, 1, 2, 3, 4. The variable `x` points to *the box*, and `x[0]` points to 0, and `x[1]` to 1 etc. Thus if we do `x[0]=10`, that would be the same as picking 0 out of the box and replacing it with 10, but *the box stays the same*. Thus when we do `print(x)`, we print the content of the box. If we do
!bc pypro
x=[0,1,2,3,4]
y=x
print(x is y) # True
x.append(10)  # x is now [0,1,2,3,4,10]
print(y)      # y=[0,1,2,3,4,10]
print(x is y) # True
!ec
What happens here is that we create a box with the numbers 0, 1, 2, 3, 4, `x` is referenced that box. Next, we do `y=x` so that `y` is referenced the *same box* as `x`. Then, we add the number 10 to that box, and `x` and `y` still points to the same box.

Numpy arrays behave differently, and that is basically because if we want to add a number to a Numpy array we have to do `x=np.array(x,10)`. Because of the assignment operator `=` , we take the content of the original box add 10 and put it into a *new* box
!bc pypro
x=np.array([0,1,2,3,4])
y=x
print(x is y)     # True
x=np.append(x,10) # x is now [0,1,2,3,4,10]
print(y)          # y=[0,1,2,3,4]
print(x is y)     # False
!ec
The reason for this behavior is that the elements in Numpy arrays (contrary to lists) have to be continuous in the memory, and the only way to achieve this is to create a new box that is large enough to also contain the new number. This also explains that if you use the `np.append(x,some_value)` inside a function where `x` is large it could slow down your code, because the program has to delete `x` and create a new very large box each time it would want to add a new element. A better way to do it is to create `x` *large enough* in the beginning, and then just assign values `x[i]=a`. 

===== Mutable and immutable objects =====
What we have explained in the previous section is related to what is known as mutable and immutable objects. These terms are used to describe objects that have an internal state that can be changed (mutable), and objects that have an internal state that cannot be changed after they have been created. Example of mutable objects are lists, dictionaries, and arrays. Examples of immutable objects are floats, ints, tuples, and strings. Thus if we create the number 10 its value cannot be changed (and why would we do that?). Note that this is *not the same as saying that*  `x=10` and that the internal state of `x` cannot change, this is *not* true. We are allowed to make `x` reference another object. If we do `x=10`, then `x is 10` will give true and they will have the same value if we use the `id` operator on `x` and `10`. If we later say that `a=11` then `a is 10` will give false and `id(a)` and `id(10)` give different values, but * `id(10)` will have the same value as before*.

Lists are mutable objects, and once a list is created, we can change the content without changing the reference to that object. That is why the operations `x=[]` and `x.append(1)`, does not change the id of x, and also explain that if we put `y=x`, `y` would change if `x` is changed. Contrary to immutable objects if `x=[]`, and `y=[]` then `x is y` will give false. Thus, whenever you create a list it will be an unique object.    

!bnotice A final tip
You are bound to get into strange, unwanted behavior when working with lists, arrays and dictionaries (mutable) objects in Python. Whenever, you are unsure, just make a simple version of your lists and perform some of the operations on them to investigate if the behavior is what you want.
!enotice
Finally, we show some ``unexpected'' behavior, just to demonstrate that it is easy to do mistakes and one should always test code on simple examples. 
!bc pypro
x_old=[]
x  = [1, 2, 3]
x_old[:] = x[:] # x_old = [1, 2, 3]
x[0] = 10
print(x_old) # "expected" x_old = [10, 2, 3], actual [1, 2, 3] 
!ec
Comment: We put the *content* of the `x` container into `x_old`, but `x` and `x_old` reference different containers.  
!bc pypro
def add_to_list(x,add_to=[])
    add_to.append(x)
    return add_to

print(add_to_list(1)) # "expected" [1] actual [1]
print(add_to_list(2)) # "expected" [2] actual [1, 2]
print(add_to_list(3)) # "expected" [3] actual [1, 2, 3]
!ec
Comment: `add_to=[]` is a default argument and it is created once when the program starts and not each time the function is called.
!bc pypro
x = [10]
y = x
y = y + [1]
print(x, y) # prints [10] [10, 1]

x = [10]
y = x
y += [1] 
print(x, y) # prints [10, 1] [10, 1]
!ec
Comment: In the first case `y + [1]` creates a new object and the assignment operator `=` assign `y` to that object, thus `x` stays the same. In the second case the `+=` adds `[1]` to the `y` container without changing the container, and thus `x` also changes.

========= Introduction to Pandas  =========
label{ch:pan}

======= What is Pandas? =======
Pandas is a Python package that among many things is used to handle data, and perform operations on groups of data. It is built on top of Numpy, which makes it easy to perform vectorized operations. Pandas is written by Wes McKinney, and one of it objectives is according to the official website " '' providing fast, flexible, and expressive data structures designed to make working with ''relational'' or ''labeled'' data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python''":"https://pandas.pydata.org/". Pandas also has excellent functions for reading and writing excel and csv files.  An excel file is read directly into memory in what is called a `DataFrame`. A DataFrame is a two dimensional object where data are typically stored in column or row format. Pandas has a lot of functions that can be used to calculate statistical properties of the data frame as a whole. In this chapter, we will focus on basic data manipulation, stuff you might do in excel, but can be done much faster in Python and Pandas.


======= Creating a data frame =======
In the following we will assume that you have imported pandas, like this:
!bc pypro
import pandas as pd
!ec

===== From an empty DataFrame =====
This is perhaps the most basic way of creating a DataFrame, first we create an empty DataFrame:
!bc pypro
df = pd.DataFrame()
!ec
!bnotice Variable name
Note that we often use `df` as a variable name for a DataFrame, this is a choice, but it is a good choice as someone else reading the code could infer from a name that `df` is a DataFrame. If you need more than one DataFrame variable you could use `df1`, `df2`, etc. or even better, use a descriptive name, `df_sales_data`.
!enotice
Next, we can add columns to the DataFrame:
!bc pypro
df=pd.DataFrame()
df['ints']=[0,1,2,3]
df['floats']=[4.,5.,6.,7.]
df['tools']=['hammer','saw','rock','nail']
print(df) # to view data frame
!ec
Note that all columns need to have the same size.

!bnotice `pd.Series()`
Even if we initialize the DataFrame column with a list, the command `type(df['a'])` will tell you that the column in the DataFrame are of type `pd.Series()`. Thus the fundamental objects in Pandas are of type `Series`. Series are more flexible, and it is possible to calculate `df['a']/df['b']`, whereas `[0,1,2,3]/[4,5,6,7]` is not possible.  
!enotice

===== Create DataFrame from dictionary =====
A DataFrame can be easily generated from a dictionary. A dictionary is a special data structure, where an unique key is associated with a data type (key:value pair). In this case, the key would be the title of the column, and the value would be the data in the columns.
##To generate the excel file in figure ref{fig:file}, we can do (see figure ref{fig:pandas:cc} for the final result)

!bc pypro
my_dict={'ints':[0,1,2,3], 'floats':[4.,5.,6.,7.],
'tools':['hammer','saw','rock','nail']
}
df=pd.DataFrame(my_dict)
print(df) # to view
!ec

##FIGURE: [fig-pandas/df.png, width=400 frac=1.0] The resulting DataFrame of ##Covid-19 data. label{fig:pandas:cc}
##Note that all columns must have the same length to create the DataFrame.

===== From a file =====
Assume you have some data organized in excel or in a csv file. The csv file could just be a file with column data, they could be separated by a comma or tab.

FIGURE: [fig-pandas/covid_comb.png, width=400 frac=1.0] Official Covid-19 data, and example of files (left) tab separated (right) excel file. label{fig:file}

!bc pypro
df=pd.read_excel('../data/corona_data.xlsx') # excel file
df2=pd.read_csv('../data/corona_data.dat',sep='\t') # csv tab separated file
!ec
If the excel file has several sheets, you can give the sheet name directly, e.g. `df=pd.read_excel('file.xlsx',sheet_name='Sheet1')`, for more information see the "documentation":"https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html". 

!bnotice Accessing files
Accessing files from python can be painful. If excel files are open in excel, Windows will not allow a different program to access it - always remember to close the file before opening it. Sometimes we are not in the right directory, to check which directory you are in, you can always do the following
!bc pypro
import os
print(os.getcwd()) # prints current working directory
!ec
!enotice

We can easily save the data frame to excel format and open it in Excel
!bc pypro
df.to_excel('covid19.xlsx', index=False) # what happens if index=True?
!ec

!bnotice Index column
Whenever you create a DataFrame, Pandas by default create an index column, it contains an integer for each row starting at zero. It can be accessed by `df.index`, and it is also possible to define another column as index column. 
!enotice

===== Accessing data in  DataFrames =====

===  Selecting columns  ===

If we want to pick out a specific column we can access it in the following way
!bc pypro
df=pd.read_excel('../data/corona_data.xlsx') 
# following two are equivalent
time=df['TIME'] # by the name, alternatively
time=df[df.columns[1]]
# following two are equivalent
time=df.loc[:,['TIME']] # by loc[] if we use name
time=df.iloc[:,1] # by iloc, pick column number 1
!ec
The `loc[]` and `iloc[]` functions also allow list slicing, one can then pick e.g. every second element in the column by `time=df.iloc[::2,1]` etc. The difference is that `loc[]` uses the name, and `iloc[]` the index (usually an integer). 

Why do we have several ways of doing the same operation? It turns out that although we are able to extract what we want with these operations, they are of different type:
!bc pypro
print(type(df['TIME']))
print(type(df.loc[:,['TIME']]))
!ec
===  Selecting rows  ===
When selecting rows in a DataFrame, we can use the `loc[]` and `iloc[]` functions
!bc pypro
# pick rows number 0 and 1
time=df.loc[0:1,:] # by loc[] 
time=df.iloc[0:2,:] # by iloc
!ec

!bnotice `pandas.DataFrame.loc` vs `pandas.DataFrame.iloc`
When selecting rows `loc` and `iloc` behave differently, `loc` includes the endpoints (in the example above both row 0 and 1), whereas `iloc` includes the starting point and up to the endpoint. 
!enotice

=== Challenges when accessing columns or rows ===

!bnotice Special characters 
Sometimes when reading files from excel, headers may contains invisible characters like newline `\n` or tab `\t` or maybe Norwegian special letters that have not been read in properly. If you have problems accessing a column by name do `print(df.columns)` and check if the name matches what you would expect.  
!enotice

If the header names have unwanted white space, one can do:
!bc pypro
df.columns = df.columns.str.replace(' ', '') # all white spaces
df.columns = df.columns.str.lstrip() # the beginning of string
df.columns = df.columns.str.rstrip() # end of string
df.columns = df.columns.str.strip()  # both ends
!ec
Similarly for unwanted tabs:
!bc pypro
df.columns = df.columns.str.replace('\t', '') # remove tab
!ec
If you want to make sure that the columns do not contain any white spaces, you can use "`pandas.Series.str.strip()`":"https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.str.strip.html"
!bc pypro
df['LOCATION']=df['LOCATION'].str.strip()
!ec

=== Time columns not parsed properly ===
If you have dates in the file (as in our case for the `TIME` column), you should check if they are in the `datetime` format and not read as `str`.

!bnotice `datetime`
The `datetime` library is very useful for working with dates. Data types of the type `datetime` (or equivalently `timestamp` used by Pandas) contain both date and time in the format `YYYY-MM-DD hh:mm:ss`. We can initialize a variable, `a`, by `a=datetime.datetime(2022,8,30,10,14,1)`, to access the hour we do `a.hour`, the year by `a.year` etc. It is also easy to increase e.g. the day by one by doing `a+datetime.timedelta(days=1)`. 
!enotice

!bc pypro
import datetime as dt
time=df['TIME']
# what happens if you set
# time=df2['TIME'] #i.e df2 is from pd.read_csv ?
print(time[0])
print(time[0]+dt.timedelta(days=1))
!ec

The code above might work fine or in some cases a date is parsed as a string by Pandas, then we need to convert that column to the correct format. If not, we get into problems if you want to plot data vs the time column.

Below are two ways of converting the `TIME` column:
!bc pypro
df2['TIME']=pd.to_datetime(df2['TIME'])
# just for testing that everything went ok
time=df2['TIME']
print(time[0])
print(time[0]+dt.timedelta(days=1))
!ec

Another possibility is to do the conversion when reading the data:
!bc pypro
df2=pd.read_csv('../data/corona_data.dat',sep='\t',parse_dates=['TIME']) 
!ec
If you have a need to specify all data types, to avoid potential problems down the line this can also be done. First create a dictionary, with column names and data types:
!bc pypro
types_dict={"LOCATION":str,"TIME":str,"ELAPSED_TIME_SINCE_OUTBREAK":int,
	"CONFIRMED":int,"DEATHS":int,"RECOVERED":int}
df2=pd.read_csv('../data/corona_data.dat',sep='\t',dtype=types_dict,
	parse_dates=['TIME']) # set data types explicit
!ec
Note that the time data type is `str`, but we explicitly tell Pandas to convert those to `datetime`.

===== Filtering and visualizing data =====
=== Boolean masking ===
Typically you would select rows based on a criterion, the syntax in Pandas is that you enter a series containing `True` and `False` for the rows you want to pick out, e.g. to pick out all entries with Afghanistan we can do:
!bc pypro
df[df['LOCATION'] == 'Afghanistan']
!ec
The innermost statement `df['LOCATION'] == 'Afghanistan'` gives a logical vector with the value `True` for the five last elements and `False` for the rest. Then we pass this to the DataFrame, and in one go the unwanted elements are removed. It is also possible to use several criteria, e.g. only extracting data after a specific time
!bc pypro
df[(df['LOCATION'] == 'Afghanistan') &
		   (df['ELAPSED_TIME_SINCE_OUTBREAK'] > 2)]
!ec
Note that the parenthesis are necessary, otherwise the logical operation would fail.

=== Plotting a DataFrame ===
Pandas has built in plotting, by calling "`pandas.DataFrame.plot`":"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html".
!bc pypro
df2=df[(df['LOCATION'] == 'Afghanistan')]
df2.plot()
#try 
#df2=df2.set_index('TIME')
#df2.plot() # what is the difference?
#df2.plot(y=['CONFIRMED','DEATHS'])
!ec
===== Performing mathematical operations on DataFrames =====
When performing mathematical operations on DataFrames there are at least two strategies
* Extract columns from the DataFrame and perform mathematical operations on the columns using Numpy, leaving the original DataFrame intact
* To operate directly on the data in the DataFrame using the Pandas library

!bnotice Speed and performance
Using Pandas or Numpy should in principle be equally fast. Do not worry about performance before it is necessary. Use the methods you are confident with, and try to be consistent. By consistent, we mean that if you have found one way of doing a certain operation stick to that one and try not to implement many different ways of doing the same thing. 
!enotice

We can always access the individual columns in a DataFrame by the syntax `df['column_name']`. 
=== Example: mathematical operations on DataFrames ===
o Create a DataFrame with one column (`a`) containing ten thousand random uniformly distributed numbers between 0 and 1 (checkout "`np.random.uniform`":"https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html")
o Add two new columns: one which all elements of `a` is squared and one where the sine function is applied to column `a`
o Calculate the inverse of all the numbers in the DataFrame
o Make a plot of the results (i.e. `a` vs `a*a`, and `a` vs `sin(a)`)

=== Solution ===
o First we make the DataFrame:
!bc pypro
import numpy as np
import pandas as pd
N=10000
a=np.random.uniform(0,1,size=N)
df=pd.DataFrame() # empty DataFrame
df['a']=a
!ec
If you like you could also try to use a dictionary. Next, we add the new columns:
!bc pypro
df['b']=df['a']*df['a'] # alternatively np.square(df['a'])
df['c']=np.sin(df['a'])
!ec

o The inverse of all the numbers in the DataFrame can be calculated by simply doing:
!bc pypro
1/df
!ec
Note: you can also do `df+df` and many other operations on the whole DataFrame.

o To make plots there are several possibilities. Personally, I tend most of the time to use the  "`matplotlib`":"https://matplotlib.org/" library, simply because I know it quite well, but Pandas has a great deal of very simple methods you can use to generate nice plots with very few commands.

__Matplotlib:__
``
!bc pypro
import matplotlib.pyplot as plt
plt.plot(df['a'],df['b'], '*', label='$a^2$')
plt.plot(df['a'],df['c'], '^', label='$\sin(a)$')
plt.legend() 
plt.grid() # make small grid lines
plt.show()
!ec

__Pandas plotting:__
``
First, let us try the built in plot command in Pandas:
!bc pypro
df.plot()
!ec
If you compare this plot with the previous plot, you will see that Pandas plots all columns versus the index columns, which is not what we want. But, we can set `a` to be the index column:
!bc pypro
df=df.set_index('a')
df.plot()
!ec
We can also make separate plots:
!bc pypro
df.plot(subplots=True)
!ec
or scatter plots
!bc pypro
df=df.reset_index()
df.plot.scatter(x='a',y='b')
df.plot.scatter(x='a',y='c')
!ec
Note that we have to reset the index, otherwise there is no column named `a`. 

===== Grouping, filtering and aggregating data =====
Whenever you have a data set, you would like to do some exploratory analysis. That typically means that you would like to group, filter or aggregate data. Perhaps, we would like to plot the covid data not per country, but the data as a function of dates. Then you first must sort the data according to date, and then sum all the occurrences on that particular date. For all of these purposes we can use the "`pd.DataFrame.groupby()`":"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html " function. To sort our DataFrame on dates and sum the occurrences we can do:
!bc pypro
df=pd.read_excel('../data/corona_data.xlsx')
df.groupby('TIME').sum()
!ec

Another case could be that we wanted to find the total number of confirmed, deaths and recovered cases in the full database. As always in Python this can be done in different ways, by e.g. splitting the database into individual countries and do  `df[['CONFIRMED','DEATHS','RECOVERED']].sum()` or accessing each column individually and sum each of them e.g. `np.sum(df['CONFIRMED'])`.  However, with the `groupby()` function
!bc pypro
df.groupby('LOCATION').sum()
!ec
Here Pandas sum all columns with the same location, and drop columns that cannot be summed. By doing `df.groupby('LOCATION').mean()` or `df.groupby('LOCATION').std()` we can find the mean or standard deviation (per day).

FIGURE: [fig-pandas/group.png, width=400 frac=1.0] The results of `df.groupby('LOCATION').sum()`. label{fig:pandas:group}

===== Simple statistics in Pandas =====
Finally, it is worth mentioning the built in methods `pd.DataFrame.mean`, `pd.DataFrame.median`, `pd.DataFrame.std` which calculate the mean, median and standard deviation on the columns in the DataFrame where it make sense (i.e. avoid strings and dates). To get all these values in one go (and a few more) on can also use `pd.DataFrame.describe()`
!bc pypro
df.describe()
!ec
The output is shown in figure ref{fig:pandas:desc}
FIGURE: [fig-pandas/describe.png, width=400 frac=1.0] Output from the describe command. label{fig:pandas:desc}


===== Joining two DataFrames =====
=== Appending DataFrames  ===
The DataFrame with the Covid-19 data in the previous section could have been created from two separate DataFrames, using "`concat()`":"https://pandas.pydata.org/docs/reference/api/pandas.concat.html". First, create two DataFrames: 
!bc pypro
import datetime as dt
a=dt.datetime(2020,2,24,23,59)
b=dt.datetime(2020,2,7,23,59)
my_dict1={'LOCATION':7*['Afghanistan'], 
'TIME':[a+dt.timedelta(days=i) for i in range(7)],
'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1, 2, 3, 4, 5, 6],
'CONFIRMED':7*[1],
'DEATHS':7*[0],
'RECOVERED': 7*[0]}
my_dict2={'LOCATION':6*['Diamond Princess'], 
'TIME':[b+dt.timedelta(days=i) for i in range(6)],
'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1, 2, 3, 4, 5],
'CONFIRMED':[61, 61, 64, 135, 135, 175],
'DEATHS':6*[0],
'RECOVERED': 6*[0]}
df1=pd.DataFrame(my_dict1)
df2=pd.DataFrame(my_dict2)
!ec
Next, add them row wise (see figure ref{fig:pandas:concat}):

!bc pypro
df=pd.concat([df1,df2])
print(df) # to view
!ec

FIGURE: [fig-pandas/concat.png, width=400 frac=1.0] The result of `concat()`. label{fig:pandas:concat}

If you compare this DataFrame with the previous one, you will see that the index column is different. This is because when joining two DataFrames Pandas does not reset the index by default, doing `df=pd.concat([df1,df2],ignore_index=True)` resets the index. It is also possible to join DataFrames column wise:
!bc pypro
pd.concat([df1,df2],axis=1)
!ec

=== Merging DataFrames ===

In the previous example we had two non overlapping DataFrames (separate countries and times). It could also be the case that some of the data was overlapping e.g. continuing with the Covid-19 data, one could assume that there was one data set from one region and one from another region in the same country:
!bc pypro
my_dict1={'LOCATION':7*['Diamond Princess'], 
'TIME':[b+dt.timedelta(days=i) for i in range(7)],
'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1, 2, 3, 4, 5, 6],
'CONFIRMED':7*[1],
'DEATHS':7*[0],
'RECOVERED': 7*[0]}
my_dict2={'LOCATION':2*['Diamond Princess'], 
'TIME':[b+dt.timedelta(days=i) for i in range(2)],
'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1],
'CONFIRMED':[60, 60],
'DEATHS':2*[0],
'RECOVERED': 2*[0]}
df1=pd.DataFrame(my_dict1)
df2=pd.DataFrame(my_dict2)
!ec
If we do `pd.concat([df1,df2])` we will simply add all values after each other. What we want to do is to sum the number of confirmed, recovered and deaths for the same date. This can be done in several ways, but one way is to use "`pd.DataFrame.merge()`":"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html".You can specify the columns to merge on, and choose `outer` which is union (all data from both frames) or `inner` which means the intersect (only data which you merge on that exists in both frames), see figure ref{fig:pandas:join} for a visual image.

FIGURE: [fig-pandas/fig_join.png, width=400 frac=1.0] The result of using `how=outer, inner, left`, or `right` in `pd.DataFrame.merge()`. label{fig:pandas:join}

To be even more specific, after performing the commands
!bc pypro
df1.merge(df2,on=['LOCATION','TIME'],how='outer')
df1.merge(df2,on=['LOCATION','TIME'],how='inner')
!ec

we get the results in figure ref{fig:pd:merge} 

FIGURE: [fig-pandas/merge.png, width=400 frac=1.0] Merging to dataframes using `outer` (top) and `inner` (bottom). label{fig:pd:merge}

Clearly in this case we need to choose `outer`. In the merge process pandas adds an extra subscript `_x` and `_y` on columns that contain the same header name. We also need to sum those, which can be done as follows
(see figure ref{fig:pandas:merge3} for the final result):

!bc pypro
df=df1.merge(df2,on=['LOCATION','TIME'],how='outer')
cols=['CONFIRMED','DEATHS', 'RECOVERED']
for col in cols:
    df[col]=df[[col+'_x',col+'_y']].sum(axis=1) # sum row elements
    df=df.drop(columns=[col+'_x',col+'_y']) # remove obsolete columns
# final clean up
df['ELAPSED_TIME_SINCE_OUTBREAK']=df['ELAPSED_TIME_SINCE_OUTBREAK_x']		
df=df.drop(columns=['ELAPSED_TIME_SINCE_OUTBREAK_y','ELAPSED_TIME_SINCE_OUTBREAK_x'])
!ec

FIGURE: [fig-pandas/merge3.png, width=400 frac=1.0] Result of outer merging and summing. label{fig:pandas:merge3}

===== Working with folders and files =====
When working with big data sets you might want to split data into smaller sets, and also write them to different folders (or files) to view each individually in excel. Working with files and folders in a way that will work on any kind of platform has always been a challenge, but it is greatly simplified by the "Pathlib library":"https://docs.python.org/3/library/pathlib.html".

=== Basic use of Pathlib ===

__List all sub directories and files:__
``
!bc pypro
from pathlib import Path
p=Path('.') # the directory where your python file is located
for x in p.iterdir():
    if x.is_dir():
        print('Found dir: ', x)
    elif x.is_file():
        print('Found file: ', x)
!ec

__List all files of a type:__
``
!bc pypro
p=Path('.')
for p in p.rglob("*.png"):# rglob means recursively, searches sub directories
    print(p.name)
!ec
If you want to print the full path do `print(p.absolute())`.

__Create a directory:__
``
!bc pypro
Path('tmp_dir').mkdir()
!ec
If you run the code twice it will produce an error, because the directory exists, then we can simply do `Path('tmp_dir').mkdir(exist_ok=True)`.

__Print current directory:__
``
!bc pypro
Path.cwd()
!ec

__Joining paths:__
``
!bc pypro
p=Path('.')
new_path = p / 'tmp_dir' / 'my_file.txt'
print(new_path.absolute())
new_path.touch()
!ec

=== Basic use of `os` ===
We have already encountered the use of `os` when printing the working directory, i.e. `print(os.getcwd())`. If you want to create a directory named `tmp`, one can do

__Creating a directory:__
``
!bc pypro
import os
os.mkdir('tmp')
!ec
__Moving into a directory:__

To move into that directory do`:
!bc pypro
os.chdir('tmp')
os.chdir('..') # move back up
!ec

=== Splitting data into different folders and files ===

__Using the Pathlib library:__
``
!bc pypro
df=pd.read_excel('../data/corona_data.xlsx')
countries = df['LOCATION'].unique() #skip duplicates
data_folder=Path('../covid-data')
data_folder.mkdir()
for country in countries:
    new_path=data_folder / country
    new_path.mkdir()
    excel_file=country+'.xlsx'
    df2=df[df['LOCATION']==country]
    df2.to_excel(new_path/excel_file,index=False)
!ec
If you run the code twice, it will fail, but that can be resolved by e.g. `data_folder.mkdir(exist_ok=True)`.  

__Using the `os` library:__
``
!bc pypro
# first get all the countries:
df=pd.read_excel('../data/corona_data.xlsx')
countries = df['LOCATION'].unique() #skip duplicates
os.mkdir('../covid-data')
os.chdir('../covid-data')
for country in countries:
    os.mkdir(country)
    os.chdir(country)
    df2=df[df['LOCATION']==country]
    df2.to_excel(country+'.xlsx',index=False)
    os.chdir('..') # move up
!ec

More robust way of creating a directory:
!bc pypro
def my_mkdir(name):
    if os.path.isdir(name):
        print('Directory ', name,' already exists')
    else:
        os.mkdir(name)
        print('creating directory ',name)
!ec

If you want to collect all data again, you can do as follows:
!bc pypro
df_new=pd.DataFrame()
data_folder=Path('../covid-data')
for dir in data_folder.iterdir():
    if dir.is_dir():      
        file=dir.name+'.xlsx'
        df=pd.read_excel(dir/file)
        print('Reading file ', file)
        df_new=pd.concat([df_new,df],ignore_index=True)
!ec

===== Writing more robust code =====
Most likely in the last sections you have encountered long error messages from Python. Errors could be
* syntax errors, grammatically incorrect code e.g. calling functions that do not exist, using variables that are not defined or writing lines with missing instructions, indentation errors. 
* exceptions e.g. open a file that does not exist, accessing a Pandas header with the wrong name, performing wrong mathematical operations (1/0).
* logical errors (bugs), code that runs but produces wrong results. These errors are of course some of the most difficult errors to find and can only be discovered by comparing the output of the code to known answers. In many cases errors are introduced when extending the code, and unit tests can be extremely helpful.

In the rest of this section, we will discuss how to avoid or to handle exceptions. The goal is to write code that catch all the exceptions before they happen, tries to do something with them or prints out a reasonable error message of what went wrong.

Let us look at the code that we have written so far, starting from the top of the notebook.

__Accessing columns in Pandas:__
So far we have just accessed the columns directly, but it is very quick to write a wrong name, thus instead of doing
!bc pypro
time=df['TIME']
!ec
we should try to check if the column exist before accessing it from the DataFrame. There are many ways of achieving this:
!bc pypro
def get_column_from_dataframe(name,df):
    '''
    name: name of column
    df: Pandas DataFrame
    returns: column if found, and empty otherwise 
    '''
    if name in df.columns:
        return df[name]
    else:
        print('Column not found')
        print('Possible column names are : ', df.columns)
        return pd.Series(dtype=object)
# run the following code with df containing covid data
get_column_from_dataframe('TIME2',df)
get_column_from_dataframe('TIME',df)
!ec
Note the use of doc string in the beginning, the doc string will be printed in advanced editors once you write the name of the function. It also helps you to remember what the function does. It is a good practice to return something of the same type, because then the rest of the code can execute. If it is critical that you find the name of the column, you can always test from the outside:
!bc pypro
s=get_column_from_dataframe('TIME2',df)
if s.empty:
    print('Exiting ...')
    exit() # note this shuts down the kernel
!ec
In the function `get_column_from_dataframe` many more things could go wrong, the user could pass a variable that is not a DataFrame, to catch all exceptions one can do:
!bc pypro
def get_column_from_dataframe_v2(name,df):
    '''
    name: name of column
    df: Pandas DataFrame
    returns: column if found, and empty otherwise 
    '''
    try:
        return df[name]
    except:
        print('Something went wrong ...')
        print('Maybe wrong column name?')
        return pd.Series(dtype=object)
# run the following code with df containing covid data
get_column_from_dataframe_v2('TIME2',df)
!ec
The `try` and `except` handling is very elegant in Python, and a very easy way of making the code more robust. Python first tries `df[name]` if that is not successful (e.g. wrong column name, wrong DataFrame, maybe Pandas is not even imported) it jumps to the exception.

Another thing to consider is case insensitive search, we should be able to access a country or a header using e.g. `Afghanistan` or `afghanistan`. A possible solution could be to make sure that when you read in the column, both the DataFrame column and the passed column name are uppercase:
!bc pypro
def get_column_from_dataframe_v3(name,df):
    '''
    name: name of column (case insensitive)
    df: Pandas DataFrame
    returns: column if found, and empty otherwise 
    '''
    COL=df.columns.str.upper()
    NAME=name.upper()
    try:
        idx=COL.get_loc(NAME)
        return df.iloc[:,idx]
    except:
        print('Column not found')
        print('Possible column names are : ', df.columns)
        return pd.Series(dtype=object)
        
get_column_from_dataframe_v3('time',df)
!ec
Now, we might want to make our code more robust by collecting data from specific rows, e.g. a specific country `df[df['LOCATION'] == 'Afghanistan']`. This operation assumes 1) that the column `LOCATION` exists and 2) that the country is spelled correctly. However, we have already written code to get a column and check that it exists, but it is written inside a function with a different purpose. Thus, it is better to split the code above in two parts:
!bc pypro
def get_col_index(name,df):
    '''
    name: name of column (case insensitive)
    df: Pandas DataFrame
    returns: pos if exists, and -1 otherwise 
    '''
    COL=df.columns.str.upper()
    NAME=name.upper()
    try:
        return COL.get_loc(NAME)
    except:
        print('Column not found')
        print('Possible column names are : ', df.columns)
        return -1

def get_column_from_dataframe_v4(name,df):
    '''
    name: name of column (case insensitive)
    df: Pandas DataFrame
    returns: column if found, and empty otherwise 
    '''
    idx=get_col_index(name,df)
    if idx>-1:
        return df.iloc[:,idx]
    else:
        return pd.Series(dtype=object)
        
def get_rows_from_dataframe(name,df,col='LOCATION'):
    '''
    name: name of rows (case insensitive)
    df: Pandas DataFrame
    col: name of column to use as logical test
    returns: DataFrame, and empty otherwise 
    '''
    idx=get_col_index(col,df)
    if idx>-1:
        NAME=name.upper()
        return df[df.iloc[:,idx].str.upper() == NAME]
    else:
        return pd.DataFrame()
get_rows_from_dataframe('afGhaniStan',df)
!ec

__To summarize:__

o We want to catch errors before they occur, this is most efficiently done by wrapping simple operations in functions.
o Functions should be as small as possible, that would increase their reusability.
o Almost all exceptions can be caught by using the `try` and `except` functionality in Python
o Write doc strings in functions to increase user friendliness.
o Write meaningful error messages, if possible also print out some additional information to help the user.



!split
========= Finite differences =========
label{ch:taylor}

###### Content provided under a Creative Commons Attribution license, CC-BY 4.0; code under MIT License. (c)2018 Aksel Hiorth

======= Why are gradients important? =======
If you are going to walk up a mountain, it is not enough to know the height of the mountain, you also want to know how steep the mountain is. Even if the mountain is very low, it can still be extremely difficult to reach the top if it is very steep. The steepness is how much the height is changing as a function of time (if we walk with the same pace) or how much the height is changing compared to our horizontal movement. To be more precise lets say we move from $x=a$ to $x=b$, and the height increases from $h_a$ to $h_b$, the steepness is
!bt
\begin{equation}
\frac{h_b-h_a}{x_b-x_a}.
label{eq:tay:gradb}
\end{equation}
!et
If we climb a ladder, the horizontal movement is small ($x_b-x_a$ is small) and the increase in height is large, hence the steepness is large. If we walk a long a flat path we have no vertical movement and the steepness is zero ($h_a=h_b$). Mathematically, if we let $x_b$ and $x_a$ come infinitely close, the steepness is called a *gradient*, and we denote it by $\nabla h(x)$. Note also that the sign of the gradient tells something about the direction. If we climb up a ladder, the height is increasing (positive gradient), on the other hand if we are climbing down the height is decreasing ($h_a>h_b$) and the gradient is negative. 

If we consider the height of a mountain in two dimensions, $h=h(x,y)$, this is basically the contour lines on a map. The spacing between the contour lines is the gradient, if the spacing between contour lines are small the mountain side is steeper than if they are larger.  

!bnotice Gradients vs derivatives
If we are only considering a single variable, height as a function of time or position, $x$, we often denote the gradient ($\nabla h$), $h^\prime(x)$ and call it the derivative of $h(x)$. In higher dimension, e.g. $h(x,y)$, we use the term partial derivatives, because there are now different variables we can vary e.g. latitude and altitude. The gradient is now a *vector*, $\nabla=[\partial h/\partial x, \partial h/\partial y]$. $\partial h/\partial x$ is the partial derivative of $h(x,y)$ with respect to $x$, i.e. we keep $y$ constant and only differentiate with respect to $x$.   
!enotice

Another example where gradients are important is the flow of heat. Heat flows from hot to cold places, the amount of heat is proportional to the temperature difference, i.e. a gradient in temperature. The flow of air is also from points of high pressure to low pressure, i.e. a gradient in pressure. 

A primary task of a modeler is to predict something. If there are no gradients in a system, nothing would happen and there is no reason to model anything. Hence, an extremely important task when we model something is to treat gradients carefully. If gradients are not represented correctly in a computer, the output of a simulation will introduce errors that sometimes can be so large that one cannot trust the simulation results.



======= Continuous functions and finite representation: numerical errors =======
A computer can only deal with numbers. To simulate a physical system in a computer we have to divide space and time into finite pieces, and assign numbers to different parts of time and/or space. 

## In order to simulate e.g. a rocket flying into space we typically find the position of #the rocket at a specific time $t$, and the computer model calculates the new position at ##a3 later time $t+h$. $h$ is a step size, and if we assume it is one minute, then we ##have discretized one hour into 60 discrete chunks of time. The challenge for any ##modeler is to know if 1 minute is too short or too long? If $h$ was one second instead ##of one minute, one hour would be split into 3600 pieces. The simulation time would go ##up, but would the *accuracy* of our calculation be any better? The goal of any ##numerical simulation is to keep the numerical error to an acceptable level. We will ##never get rid of it as you will see in this chapter. 

##Most physical systems are described in terms of *differential equations*. A ##differential equation describe how a physical phenomenon evolves in space and time. The ##solution to a differential equation is a function of space and/or time. The function ##could describe the temperature evolution of the earth, it could be growth of cancer ##cells, the water pressure in an oil reservoir, the list is endless. If we can solve the ##model analytically, the answer is given in terms of a known function. Most of the ##models cannot be solved analytically, then we have to rely on computers to help us. The ##computer does not have any concept of continuous functions, a function is always ##evaluated at some specific points in space and/or time. 
!bnotice Numerical errors
idx{numerical error}
Whenever we divide space and/or time into finite pieces, we introduce numerical errors. These errors tend to become smaller, but not always, when we use more pieces. The difference between the "true" answer and the answer obtained from a practical (numerical) calculation is called the *numerical error*.
!enotice

When we divide space and time into finite pieces to represent them in a computer, a natural question to ask is how many pieces do we need? Consider an almost trivial example, let say you want visualize the function $f(x)=\sin x$. To do this we need to choose where, which values of $x$, we want to evaluate our function. To make an efficient program we want to use as few points as possible but still capture shape of the true function.  
% if FORMAT == 'ipynb':
Run the script below, and you will 
see a plot of $\sin x$ on the interval $[-\pi,\pi]$, and $[-2.2,-1]$.
@@@CODE src-taylor/func_plot.py
% endif
In figure ref{fig:taylor:sinx}, we have plotted $\sin x$ for various discretization (spacing between the points) in the interval $[-\pi,\pi]$.
FIGURE: [fig-taylor/func_plot, width=600] A plot of $\sin x$ for different spacing of the $x$-values. label{fig:taylor:sinx}

From the figure we see that in some areas only a couple of points are needed in order to
represent the function well, and in some areas more points are needed. To state it more clearly; between $[-1,1]$ a linear function (few points) approximate $\sin x$ well, 
whereas in the area where the gradient of the function changes more rapidly e.g. in $[-2,-1]$, we need the points to be more closely spaced to capture the behavior of the true function.

What is a *good representation* representation of the true function? We cannot rely on visual inspection every time, and most of the time we do not know the true answer so we would not know what to compare it with. In the next section we will show how Taylor polynomial representation of a function is a natural starting point to answer this question.
                                                                                  
======= Taylor polynomial approximation =======
How can we evaluate numerical errors if we do not know the true answer? There are at least two answers to this

o The pragmatic engineering approach is to do a simulation with a coarse grid, then refine the grid until the solution does not change very much. This is perfectly fine *if you know that your numerical code is bug free*, because even if the simulation converges to a solution we do not know if it is the *true solution*. In too many cases this is not so. Therefore even in well tested industrial codes, it is always good to test them on a simple test case where you know the exact solution.
o Taylors formula can be used to represent any continuous function with continuous gradients or most solutions to a mathematical model. Taylors formula gives us an estimate of the numerical error introduced when we divide space and time into finite pieces.

There are many ways of representing a function, $f(x)$, like Fourier series, Legendre polynomials, but perhaps one of the most widely used is Taylor polynomials.   
Taylor series are perfect for computers, simply because it makes it possible to evaluate any function with a set of limited operations: *addition, subtraction, and multiplication*. Let us start off with the formal definition: 
!bnotice Taylor polynomial:
idx{Taylor polynomial}
The Taylor polynomial, $P_n(x)$ of degree $n$ of a function $f(x)$ at the point $c$ is defined as:
!bt
\begin{align}
 P_n(x) &= f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots+\frac{f^{(n)}(c)}{n!}(x-c)^n\nonumber\\
&=\sum_{k=0}^n\frac{f^{(k)}(c)}{k!}(x-c)^k.\label{eq:taylor:taylori}
\end{align}
!et
!enotice
Note that $x$ can be anything, space, time, temperature etc. If the series is around the point $c=0$, the Taylor polynomial $P_n(x)$ is often called a Maclaurin polynomial. If the series converge (i.e. that the higher order terms approach zero), then we can represent the function $f(x)$ with its corresponding Taylor series around the point $x=c$:
!bt
\begin{align}
 f(x) &= f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots
=\sum_{k=0}^\infty\frac{f^{(k)}}{k!}(x-c)^k.\label{eq:taylor:taylor}
\end{align}
!et
!bnotice The magic of Taylors formula
Taylors formula, equation (ref{eq:taylor:taylor}), states that if we know the function value and its gradients *in a single point $c$*, we can estimate the function everywhere *using only  information from the single point $c$*. How can this be, how can information in a single point be used to predict the behavior of the function everywhere? One way of thinking about it could be to imagine an object moving in a constant gravitational field without air resistance. Newtons laws then tells us that  if we know the starting point e.g. ($x(0)$), the velocity ($v=dx/dt$), and the acceleration ($a=dv/dt=d^2x/dt^2$) in that point we can predict the trajectory of the object. This trajectory is exactly the first terms in Taylors formula, $x(t)=x(0) + vt+at^2/2$. 
!enotice
% if FORMAT != 'ipynb':
An example of how Taylors formula works for a known function, can be seen in figure ref{fig:mac_sin}, where we show the first nine terms in the Maclaurin series for $\sin x$ (all even terms are zero). 
FIGURE: [fig-taylor/mac_sin, width=600] Nine first terms of the Maclaurin series of $\sin x$. label{fig:mac_sin}
% endif
% if FORMAT == 'ipynb':
An example of how Taylors formula works for a known function, can be seen by running the script  below where we calculate different orders of the Maclaurin series of $\sin x$.    
@@@CODE src-taylor/mac_sin.py 
% endif
Notice that close to $x=0$ we only need one term, as we move further away from this point more and more term needs to be added. Thus, Taylors formula is only exact if we include an infinite number of terms. In practice we only include a limited number of terms and truncate the series up to a given order. Luckily, Taylors formula include an estimate of the error we do when we truncate the series. 
idx{truncation error}
!bnotice Truncation error in Taylors formula:
idx{Taylor polynomial, error term}
!bt
\begin{align}
R_n(x)&=f(x)-P_n(x)=\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-c)^{n+1}\nonumber\\
      &=\frac{1}{n!}\int_c^x(x-\tau)^{n}f^{(n+1)}(\tau)d\tau,\label{eq:taylor:error}
\end{align}
!et 
Notice that the mathematical formula is basically the next order term ($n+1$) in the Taylor series, but with $f^{(n+1)}(c)\to f^{(n+1)}(\eta)$. $\eta$ is an (unknown) value in the domain $[x,c]$.
!enotice
Notice that if $c$ is very far from $x$ the truncation error increases. The fact that we do not know the value of $\eta$ is usually not a problem, in many cases we just replace $f(\eta)$ with the maximum value it can take on the domain. Equation (ref{eq:taylor:error}) gives us an direct estimate of discretization error. 
!bnotice Example: evaluate $\sin x$
Whenever you do e.g. `np.sin(1)` in Python or an equivalent statement in another language, Python has to tell the computer how to evaluate $\sin x$ at $x=1$. Write a Python code that calculates $\sin x$ up to a user specified accuracy.

__Solution__
The Maclaurin series of $\sin x$ is:
idx{Maclaurin series}
!bt
\begin{equation}
\sin x = x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\cdots=\sum_{k=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}.
label{sin}
\end{equation}
!et
If we want to calculate $\sin x$ to a precision lower than a specified value we can do it as follows:

@@@CODE src-taylor/mac_sin_eps.py 
 
This implementation needs some explanation:

* The error term is given in equation (ref{eq:taylor:error}), and it is an even power in $x$. We do not which $\eta$ to use in equation (ref{eq:taylor:error}), instead we simply say that the error in our estimate is smaller than the highest order term. Thus, we stop the evaluation if the highest order term in the series is lower than the uncertainty. Note that the final error has to be smaller as the higher order terms in any convergent series is smaller than the previous.  Our estimate should then always be better than the specified accuracy.
* We evaluate the polynomials in the Taylor series by using the previous values too avoid too many multiplications within the loop, we do this by using the following identity:
!bt
  \begin{align}
  \sin x&=\sum_{k=0}^{\infty} (-1)^nt_n, \text{ where: } t_n\equiv\frac{x^{2n+1}}{(2n+1)!}, \text{ hence :}\nonumber\\
  t_{n+1}&=\frac{x^{2(n+1)+1}}{(2(n+1)+1)!}=\frac{x^{2n+1}x^2}{(2n+1)! (2n+2)(2n+3)}\nonumber\\
  &=t_n\frac{x^2}{(2n+2)(2n+3)}
  \end{align}
!et 
!enotice

======= Calculating Numerical Derivatives of Functions =======

As stated earlier many models are described by differential equations. Differential equations contains derivatives, and we need to tell the computer how to calculate those. By using a simple transformation, $x\to x+h$ and $c\to x$ (hence $x-c\to h$), Taylors formula in equation (ref{eq:taylor:taylor}) can be written
!bt
\begin{equation}
f(x+h)=f(x)+f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(x)h^2+\cdots.
label{eq:taylor:t}
\end{equation}
!et
This is useful because this equation contains the derivative of $f(x)$ on the right hand side. To be even more explicit let us truncate the series to a certain power. Remember that you can always do this but we need to replace $x$ with $\eta$ in the last term we choose to keep
!bt
\begin{equation}
f(x+h)=f(x)+f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(\eta)h^2
label{eq:taylor:t3}
\end{equation}
!et
where $\eta\in[x,x+h]$. Solving this equation with respect to $f^\prime(x)$ gives us
!bt
\begin{equation}
f^\prime(x)=\frac{f(x+h)-f(x)}{h}-\frac{1}{2}f^{\prime\prime}(\eta)h.
label{eq:taylor:fd}
\end{equation}
!et
Note that if $h\to0$, this expression is equal to the definition of the derivative. The beauty of equation (ref{eq:taylor:fd}) is that it contains an expression for the error we make *when $h$ is not zero*. Equation (ref{eq:taylor:fd}) is usually called the *forward difference* idx{forward difference}. As you might guess, we can also choose to use the *backward difference* idx{backward difference} by simply replacing $h\to-h$. Is equation (ref{eq:taylor:fd}) the only formula for the derivative? The answer is no, and we are going to derive the formula for the *central difference* idx{central difference}, by writing Taylors formula for $x+h$ and $x-h$ up to the third order

!bt
\begin{align}
f(x+h)&=f(x)+f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(x)h^2+\frac{1}{3!}f^{(3)}(\eta_1)h^3,   
label{eq:taylor:c1}\\
f(x-h)&=f(x)-f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(x)h^2-\frac{1}{3!}f^{(3)}(\eta_2)h^3.
label{eq:taylor:c2}
\end{align}
!et
where $\eta_1\in[x,x+h]$, and $\eta_2\in[x-h,x]$. Subtracting  equation (ref{eq:taylor:c1}) and (ref{eq:taylor:c2}), we get the following expression for the central difference idx{central difference}
##!bt WHY IS THIS WRONG????????
##\begin{equation}
##f^\prime(x)=\frac{f(x+h)-f(x-h)}{2h} -\frac{h^2}{6}f^{(3)}(\eta),label{eq:taylor:cd}
##\end{equation}
##!et

!bt
\begin{equation}
f^\prime(x)=\frac{f(x+h)-f(x-h)}{2h} -\frac{h^2}{6}f^{(3)}(\eta),label{eq:taylor:cd}
\end{equation}
!et

where $\eta\in[x-h,x+h]$. Note that the error term in this equation is *one order higher* than in equation (ref{eq:taylor:fd}), meaning that it is expected to be more accurate. In figure ref{fig:taylor:fd} there is a graphical interpretation of the finite difference approximations to the derivative. 

FIGURE: [fig-taylor/fd.png, width=400 frac=1.0] A graphical interpretation of the forward and central difference formula. label{fig:taylor:fd}


=== Higher order derivative ===
label{sec:taylor:hhd}
We are also now in the position to derive a formula for the second order derivative. Instead of subtracting equation (ref{eq:taylor:c1}) and (ref{eq:taylor:c2}), we can add them. Then the first order derivative disappear and we are left with an expression for the second derivative
!bt
\begin{equation}
f^{\prime\prime}(x) = \frac{f(x+h)+f(x-h)-2f(x)}{h^2}- \frac{h^2}{12}f^{(4)}(\eta)
label{eq:taylor:2der},
\end{equation}
!et
We can also calculate higher order derivatives by expanding about $x\pm h$ and $x\pm 2h$, adding one more term it follows from equation (ref{eq:taylor:cd})
!bt
\begin{align}
f(x+h)-f(x-h)&=2hf^\prime(x)+\frac{2}{3!}h^3f^{(3)}(x)+\frac{2}{5!}h^5f^{(5)}(\eta),\no\\
f(x+2h)-f(x-2h)&=2(2h)f^\prime(x)+\frac{2}{3!}(2h)^3f^{(3)}(x)+\frac{2}{5!}h^5f^{(5)}(\eta).
label{}
\end{align}
!et
It is now possible to find an expression for the third derivative
!bt
\begin{equation}
f^{(3)}(x) = \frac{f(x-h)-f(x+h)-\frac{1}{2}f(x-2h)+\frac{1}{2}f(x+2h)}{h^3}+ \frac{h^2}{4}f^{(5)}(\eta)
label{eq:taylor:3der},
\end{equation}
!et
or a higher order first derivative
!bt
\begin{equation}
f^{\prime}(x) = \frac{2f(x+h)-2f(x-h)-\frac{1}{4}f(x+2h)+\frac{1}{4}f(x-2h)}{3h}+ \frac{h^4}{30}f^{(5)}(\eta)
label{eq:taylor:5der}.
\end{equation}
!et

!bnotice Example: calculate the numerical derivative and second derivative of $\sin x$
Choose a specific point, e.g. $x=1$, and calculate the numerical error for various values of the step size $h$.
__Solution:__
The derivative of $\sin x$ is $\cos x$, we can calculate the numerical derivatives using Python

@@@CODE src-taylor/df2_mod.py fromto: def@plt.save

In figure ref{fig:taylor:df2} you can see the figure produced by the code above.
!enotice

FIGURE: [fig-taylor/df2_mod.png, width=400 frac=1.0] Numerical error of derivatives of $\sin x$ for various step sizes. label{fig:taylor:df2}

There are several important lessons from figure ref{fig:taylor:df2}
o When the step size is high and decreasing (from right to left in the figure), we clearly see that the numerical error *decreases*.
o The numerical error scales as expected from right to left. The forward difference formula scales as $h$, i.e. decreasing the step size by 10 reduces the numerical error by 10. The central difference and second order derivative formula scales as $h^2$, reducing the step size by 10 reduces the numerical error by 100
o At a certain point the numerical error start to *increase*. For the forward difference formula this happens at $~10^{-8}$.

The numerical error has a minimum, *it does not continue to decrease when $h$ decreases*. The explanation for this behavior is two competing effects: *truncation errors* and *roundoff errors*. The truncation errors have already been discussed in great detail, in the next section we will explain roundoff errors.

===== Roundoff Errors =====
idx{roundoff erros}
In a computer a floating point number,$x$, is represented as:
!bt
\begin{align}
x=\pm q2^m.
label{eq:taylor:sci2}
\end{align}
!et
This is very similar to our usual scientific notation where we represents large (or small numbers) as $\pm q E m=\pm q 10^{m}$. The processor in a computer handles a chunk of bits at one time, this chunk of bit is usually termed *word*. The number of bits (or byte which almost always means a group of eight bits) in a word is handled as a unit by a processor.   
Most modern computers uses 64-bits (8 bytes) processors. We are not going too much into all the details, the most important message is that the units handled by the processor are *finite*. Thus we cannot, in general, store numbers in a computer with infinite accuracy.
!bnotice Machine Precision
idx{machine precision}
Machine precision, $\epsilon_M$ is the smallest number we can add to one and get something different than one, i.e. $1+\epsilon_M>1$. For a 64-bits computer this value is $\epsilon_M=2^{-52}\simeq2.2210^{-16}$.
!enotice
In the next section we explain exactly why the machine precision has this value, but if you just accept this for a moment we can demonstrate why the machine precision is important and why you need to care about it. First just to convince you that the machine precision has the value of $2^{-52}$ in your computer you can do the following in Python
!bc pypro
print(1+2**-52) # prints a value larger than 1
print(1+2**-53) # prints 1.0
!ec
Next, consider the simple calculation
!bc pypro
a=0.1+0.2
b=0.3
print(a==b) # gives False
!ec
Why is `a==b` false, the calculation involves only numbers with one decimal? The reason is that the computer uses the binary system, and in the binary system there is no way of representing 0.2 and 0.3 with a finite number of bits, as an example 0.2 in the binary system is
!bt
\begin{equation}
0.2_{10}=0.0011001100\ldots_2 (=2^{-3}+2^{-4}+2^{-7}+2^{-8}+2^{-11}+\cdots)
label{eq:taylor:02}
\end{equation}
!et
Note that we use the subscript $_{10}$ and $_2$ to represent the decimal and binary system respectively.
Thus in the computer 0.2 will be represented as $0.1999\ldots$ and when we add 0.1 we will get a number really close to 0.3 but not equal to 0.3. Some floats have an exact binary representation e.g. $0.125_{10}=2^{-8}_{10}=0.00000001_2$. Thus the following code will produce the expected result
!bc pypro
a=0.125+0.25
b=0.375
print(a==b) # gives True
!ec
!bnotice Comparing two floats
Whenever you want to compare if two floats, $a$ and $b$, are equal in a computer program, you should never do $a==b$ because of roundoff errors. Rather you should choose a variant of $|a-b|<\epsilon$, where you check if the numbers are *close enough*. In practice you also might want to normalize the values and do $|1-b/a|<\epsilon$. 
!enotice
The roundoff errors can also play a very big role in calculations, it is particularly apparent when subtracting two numbers of similar magnitude as illustrated in the following code
!bc pypro
h=2**-53
a=1+h
b=1-h
print((a-b)/h) # analytical result is 2
!ec
The calculation above is very similar to the calculation done when evaluating derivatives, and if you run the code you will see that Python does not give the expected value of 2.
!bnotice Choosing the right step size
A step size that is too low will give higher numerical error because roundoff errors dominate the numerical error. 
!enotice
At the end we will mention a simple trick that you can use sometimes to avoid roundoff errors cite{flannery1992numerical}. In practice we can never get rid of roundoff errors in the calculation $f(x+h)$, but since we can choose the step size $h$ we can choose to choose values such that $x$ and $x+h$ differ by an exact binary number
!bc pypro
x=1
h=0.0002 
temp = x+h
h=temp-x
print(h) # improved value of h with exact binary representation
!ec
In the next sections we will show why $\epsilon_M=2^{-52}$, and why a finite word size leads necessary has to imply a maximum and minimum number.  
=== Binary numbers ===
Binary numbers are used in computers because processors are made of billions of transistors, the end states of a transistor is off or on, representing a 0 or 1 in the binary system. Assume, for simplicity, that we have a processor that uses a word size of 4 bits (instead of 64 bits). How many *unsigned* (positive) integers can we represent in this processor? Lets write down all the possible combinations, of ones and zeros and also do the translation from base 2 numerical system to base 10 numerical system:

!bt
\begin{equation}
\begin{matrix}
0&0&0&0=0\cdot 2^3+0\cdot 2^2+0\cdot 2^1+0\cdot 2^0=0\\
0&0&0&1=0\cdot 2^3+0\cdot 2^2+0\cdot 2^1+1\cdot 2^0=1\\
0&0&1&0=0\cdot 2^3+0\cdot 2^2+1\cdot 2^1+0\cdot 2^0=2\\
0&0&1&1=0\cdot 2^3+0\cdot 2^2+1\cdot 2^1+1\cdot 2^0=3\\
0&1&0&0=0\cdot 2^3+1\cdot 2^2+0\cdot 2^1+0\cdot 2^0=4\\
0&1&0&1=0\cdot 2^3+1\cdot 2^2+0\cdot 2^1+1\cdot 2^0=5\\
0&1&1&0=0\cdot 2^3+1\cdot 2^2+1\cdot 2^1+0\cdot 2^0=6\\
0&1&1&1=0\cdot 2^3+1\cdot 2^2+1\cdot 2^1+1\cdot 2^0=7\\
1&0&0&0=1\cdot 2^3+0\cdot 2^2+0\cdot 2^1+0\cdot 2^0=8\\
1&0&0&1=1\cdot 2^3+0\cdot 2^2+0\cdot 2^1+1\cdot 2^0=9\\
1&0&1&0=1\cdot 2^3+0\cdot 2^2+1\cdot 2^1+0\cdot 2^0=10\\
1&0&1&1=1\cdot 2^3+0\cdot 2^2+1\cdot 2^1+1\cdot 2^0=11\\
1&1&0&0=1\cdot 2^3+1\cdot 2^2+0\cdot 2^1+0\cdot 2^0=12\\
1&1&0&1=1\cdot 2^3+1\cdot 2^2+0\cdot 2^1+1\cdot 2^0=13\\
1&1&1&0=1\cdot 2^3+1\cdot 2^2+1\cdot 2^1+0\cdot 2^0=14\\
1&1&1&1=1\cdot 2^3+1\cdot 2^2+1\cdot 2^1+1\cdot 2^0=15
\end{matrix}
.
label{eq:taylor:bin4}
\end{equation}
!et
Hence, with a 4 bits word size, we can represent $2^4=16$ integers. The largest number is $2^4-1=15$, and the smallest is zero. What about negative numbers? If we still keep to a 4 bits word size, there are still $2^4=16$ numbers, but we distribute them differently. The common way to do it is to reserve the first bit to be a *sign* bit, a "0" is positive and "1" is negative, i.e. $(-1)^0 = 1$, and $(-1)^1=-1$. Replacing the first bit with a sign bit in equation (ref{eq:taylor:bin4}), we get the following sequence of numbers 0,1,2,3,4,5,6,7,-0,-1,-2,-3,-4,-5,-6,-7. The "-0", might seem strange but is used in the computer to extend the real number line $1/0=\infty$, whereas $1/-0=-\infty$. In general when there are $m$ bits, we have a total of $2^m$ numbers. If we include negative numbers, we can choose to have $2^{m-1}-1$, negative, and $2^{m-1}-1$ positive numbers, negative zero and positive zero, i.e. $2^{m-1}-1+2^{m-1}-1+1+1=2^m$.

What about real numbers? As stated earlier we use the scientific notation as in equation (ref{eq:taylor:sci2}), but still the scientific notation might have a real number in front, e.g. $1.25\cdot 10^{-3}$. To represent the number $1.25$ in binary format we use a decimal separator, just as with base 10. In this case 1.25 is 1.01 in binary format
!bt
\begin{equation}
1.01=1\cdot 2^0 + 0\cdot 2^{-1}+1\cdot 2^{-2}=1 + 0 + 0.25=1.25.
label{eq:taylor:b2fr}
\end{equation}
!et
The scientific notation is commonly referred to as *floating point representation*. The term "floating point" is used because the decimal point is not in the same place, in contrast to fixed point where the decimal point is always in the same place. To store the number 1e-8=0.00000001 in floating point format, we only need to store 1 and -8 (and possibly the sign), whereas in fixed point format we need to store all 9 numbers.  In equation (ref{eq:taylor:bin4}) we need to spend one bit to store the sign, leaving (in the case of 4 bits word size) three bits to be distributed among the *mantissa*, $q$, and the exponent, $m$. It is not given how many bits should be used for the mantissa and the exponent. Thus there are choices to be made, and all modern processors uses the same standard, the "IEEE Standard 754-1985":"https://standards.ieee.org/standard/754-1985.html". 

=== Floating point numbers and the IEEE 754-1985 standard ===
idx{IEEE 754-1985 standard}
A 64 bits word size is commonly referred to as *double precision*, whereas a 32 bits word size is termed *single precision*. In the following we will consider a 64 bits word size. We would like to know: what is the roundoff error, what is the largest number that can be represented in the computer, and what is the smallest number? Almost all floating point numbers are represented in *normalized* form. In normalized form the mantissa is written as $M=1.F$, and it is only $F$ that is stored,   $F$ is termed the *fraction*. We will return to the special case of some of the unnormalized numbers later. In the IEEE standard one bit is reserved for the sign, 52 for the fraction ($F$) and 11 for the exponent ($m$), see figure ref{fig:taylor:64bit} for an illustration.

FIGURE: [fig-taylor/64bit.png, width=400 frac=1.0] Representation of a 64 bits floating point number according to the IEEE 754-1985 standard. For a 32 bits floating point number, 8, is reserved for the exponent and 23 for the fraction. label{fig:taylor:64bit}

The exponent must be positive to represent numbers with absolute value larger than one, and negative to represent numbers with absolute value less than one.  To make this more explicit the simple formula in equation (ref{eq:taylor:sci2}) is rewritten:
!bt
\begin{equation}
\pm q 2^{E-e}.
label{eq:taylor:ieee}
\end{equation}
!et
The number $e$ is called the *bias* and has a fixed value, for 64 bits it is $2^{11-1}-1=1023$ (32-bits: $e=2^{8-1}-1=127$). The number $E$ is represented by 11 bits and can thus take on values from 0 to $2^11-1=2047$. If we have an exponent of e.g. -3, the computer adds 1023 to that number and store the number 1020. Two numbers are special numbers and reserved to represent infinity and zero, $E=0$ and $E=2047$. Thus *the largest and smallest possible numerical value of the exponent is: 2046-1023=1023, and 1-1023=-1022, respectively*. The fraction of a normalized floating point number takes on values from $1.000\ldots 00$ to $1.111\ldots 11$. Thus the lowest normalized number is
!bt
\begin{align}
1.000 + \text{ (49 more zeros)}\cdot 2^{-1022}&=2^0\cdot2^{-1022}\no\\
&=2.2250738585072014\cdot 10^{-308}.
label{}
\end{align}
!et
It is possible to represent smaller numbers than $2.22\cdot10^{-308}$, by allowing *unnormalized* values. If the exponent is -1022, then the mantissa can take on values from $1.000\ldots 00$ to $0.000\ldots 01$, but then accuracy is lost. So the smallest possible number is $2^{-52}\cdot{2^-1022}\simeq4.94\cdot10^{-324}$. 
The highest normalized number is
!bt
\begin{align}
1.111 + &\text{ (49 more ones)}\cdot2^{1023}=(2^0+2^{-1}+2^{-2}+\cdots+2^{-52})\cdot2^{1023}\no
\\=(2-2^{-52})\cdot2^{1023}
&=1.7976931348623157\cdot 10^{308}.
label{}
\end{align}
!et

If you enter `print(1.8*10**(308))` in Python, the answer will be `Inf`. If you enter `print(2*10**(308))`, Python will (normally) give an answer. This is because 
the number $1.8\cdot10^{308}$ is floating point number, whereas $2\cdot 10^{308}$ is an *integer*, and Python does something clever when it comes to representing integers. 
Python has a third numeric type called long int, which can use the available memory to represent an integer.

What about the machine precision? The machine precision, $\epsilon_M$, is the *smallest possible number that can be added to one, and get a number larger than one*, i.e. $1+\epsilon_M>1$.  The smallest possible value of the mantissa is $0.000\ldots 01=2^{-52}$, thus the lowest number must be of the form $2^{-52}\cdot 2^{m}$. If the exponent , $m$, is lower than 0 then when we add this number to 1, we will only get 1. Thus the machine precision is $\epsilon_M=2^{-52}=2.22\cdot10^{-16}$ (for 32 bits $2^{-23}=1.19\cdot10^{-7}$). In practical terms this means that e.g. the value of $\pi$ is $3.14159265358979323846264338\ldots$, but in Python it can only be represented by 16 digits: $3.141592653589793$.


=== Roundoff error and truncation error in numerical derivatives ===
!bnotice Roundoff Errors
idx{roundoff errors}
All numerical floating point operations introduces roundoff errors at each step in the calculation due to finite word size, these errors accumulate in long simulations and introduce random errors in the final results. After $N$ operations the error is at least $\sqrt{N}\epsilon_M$ (the square root is a random walk estimate, and we assume that the errors are randomly distributed). The roundoff errors can be much, much higher when numbers of equal magnitude are subtracted. You might be so unlucky that after one operation the answer is completely dominated by roundoff errors.   
!enotice

The roundoff error when we represent a floating point number $x$ in the 
machine will be of the order $x/10^{16}$ (*not* $10^{-16}$). In general, when we evaluate a function the error will be of the order 
$\epsilon|f(x)|$, where $\epsilon\sim10^{-16}$. Thus equation (ref{eq:taylor:fd}) is modified in the following way when we take into account the roundoff errors:
!bt
\begin{align}
f^\prime(x)=\frac{f(x+h)-f(x)}{h}\pm\frac{2\epsilon|f(x)|}{h}-\frac{h}{2}f^{\prime\prime}(\eta),\label{eq:taylor:derr2}
\end{align}
!et
we do not know the sign of the roundoff error, so the total error $R_2$ is:
!bt
\begin{align}
R_2=\frac{2\epsilon|f(x)|}{h}+\frac{h}{2}|f^{\prime\prime}(\eta)|.\label{eq:taylor:derr3}
\end{align}
!et
We have put absolute values around the function and its derivative to get the maximal error, it might be the case that the roundoff error cancel part of the 
truncation error. However, the roundoff error is random in nature and will change from machine to machine, and each time we run the program. 
Note that the roundoff error increases when $h$ decreases, and the approximation error decreases when $h$ decreases. This is exactly what we saw in figure ref{fig:taylor:df2}. We can find the 
best step size, by differentiating $R_2$ and put it equal to zero:
!bt
\begin{align}
\frac{dR_2}{dh}&=-\frac{2\epsilon|f(x)|}{h^2}+\frac{1}{2}f^{\prime\prime}(\eta)=0\nonumber\\
h&=2\sqrt{\epsilon\left|\frac{f(x)}{f^{\prime\prime}(\eta)}\right|}\simeq 2\cdot10^{-8},\label{eq:taylor:derr4}
\end{align}
!et
In the last equation we have assumed that $f(x)$ and its derivative is $~1$. This step size corresponds to an error of order $R_2\sim10^{-8}$. 
Inspecting figure ref{fig:taylor:df2} we see that the minimum is located at $h\sim10^{-8}$.      

We can perform a similar error analysis as we did before, and then we find for equation (ref{eq:taylor:cd}) and (ref{eq:taylor:2der}) that the total
numerical error is:
!bt
\begin{align}
R_3&=\frac{\epsilon|f(x)|}{h}+\frac{h^2}{6}f^{(3)}(\eta),\label{eq:taylor:derr3b}\\
R_4&=\frac{4\epsilon|f(x)|}{h^2}+\frac{h^2}{12}f^{(4)}(\eta),\label{eq:taylor:derr4b}
\end{align}
!et
respectively. Differentiating these two equations with respect to $h$, and set the equations equal to zero, we find an optimal step size of
$h\sim10^{-5}$ for equation (ref{eq:taylor:derr3b}), which gives an error of $R_2\sim 10^{-16}/10^{-5}+(10^{-5})^2/6\simeq10^{-10}$, and $h\sim10^{-4}$ for equation
(ref{eq:taylor:derr4b}), which gives an error of $R_4\sim 4\cdot10^{-16}/(10^{-4})^2+(10^{-4})^2/12\simeq10^{-8}$. Note that we get the surprising result for the first order 
derivative in equation (ref{eq:taylor:cd}), that a higher step size gives a more accurate result. 



!split

========= Partial differential equations and linear systems =========
label{ch:lin}
% if FORMAT == 'ipynb':
!bc pycod
import matplotlib.pyplot as plt
import numpy as np
!ec
% endif
Most problems in nature are nonlinear. That means that the system response is not proportional to the system variables, e.g. doubling the CO$_2$ concentration in the atmosphere does not lead to a doubling of the earth surface temperature. Still, linear solvers lies at the heart of all grid based models describing e.g. the earths climate. The reason is that although the *global* model is nonlinear, the model can be formulated *locally* as a linear model. Typically the simulation code solves the nonlinear problem through a series of steps where each step is a solution of a linear problem. The topic of solving linear systems of equations have been extensively studied, and sophisticated linear equation solving packages have been developed. Python uses functions from the "LAPACK":"https://en.wikipedia.org/wiki/LAPACK" library.

In the next sections we will show in detail how differential equations can be solved as a linear problem. We will first start off by deriving one of the most useful differential equations describing conservation of a quantity, e.g. mass, energy, momentum, charge.

======= The continuity equation =======
idx{continuity equation}
The continuity equation is fundamental to all mathematical models describing a physical phenomenon. To gain more understanding of its origin we will take the time to derive it from first principles. We will do so in one dimension, consider a volume in space between $A(x)$ and $A(x+dx)$ in figure ref{fig:lin:flux}. To be concrete we will assume that the green arrows represents the flow of heat. Thus there are heat flowing into and out of the system, and also heat that can be generated within the system by e.g. chemical reactions. The conservation equation can be formulated with words

FIGURE: [fig-lin/flux.png, width=400 frac=0.5] A closed volume, $V(x)=A(x)dx$, where a quantity flows in and out (illustrated by the green lines), there is also a possibility for generation or loss of the same quantity inside the volume. label{fig:lin:flux}

!bt
\begin{align}
\frac{\text{heat into V(x)}}{\text{time}}-\frac{\text{heat out of V(x)}}{\text{time}}
&+\frac{\text{heat generated in V(x)}}{\text{time}} \no\\
&= \frac{\text{change of heat in V(x)}}{\text{time}}.
label{eq:lin:flux}
\end{align}
!et
We formulate the conservation equation per time, because we would like to investigate the time dependency of heat flow. The next step is to replace the terms ''heat into/out of'' with a useful mathematical quantity. It turns out that the term *flux* is particularly useful, because it is an *intensive* quantity. An intensive quantity is a quantity that is *independent of the system size*, like density. The flux is denoted by the symbol $J$
!bt
\begin{equation}
J(x)=\frac{\text{quantity (heat)}}{\text{area}\cdot\text{time}},
label{}
\end{equation}
!et
and was first introduced by Isaac Newton. Thus to find the amount of heat transported through a surface per time we simply multiply the flux with the surface area. Next, we define the heat per volume as $q(x)$, and the heat produced per volume as $\sigma$. Then equation (ref{eq:lin:flux}) can be written
!bt
\begin{align}
\frac{J(x)A(x)}{dt}-\frac{J(x+dx)A(x+dx)}{dt}&+
\frac{\sigma(t+dt)V(x)-\sigma(t)V(x)}{dt}\no\\=
&\frac{q(t+dt)V(x)-q(t)V(x)}{dt}.
label{eq:lin:cont1}
\end{align}
!et
Using Taylor expansion we can write
!bt
\begin{align}
J(x+dx)A(x+dx) &=J(x)A(x)+\frac{d(J(x)A(x)}{dx}dx+{\cal O}(dx^2),
label{eq:lin:cont2} \\
\sigma(t+dt) &=\sigma(t)+\frac{d\sigma}{dt}dt+{\cal O}(dt^2),\no\\
q(t+dt) &=q(t)+\frac{dq}{dt}dt+{\cal O}(dt^2),
label{eq:lin:cont3}
\end{align}
!et
Inserting these equations into equation(ref{eq:lin:cont1}), using $V(x)=A(x)dx$, and taking the limit $dx,dt\to0$ we arrive at
!bnotice The continuity equation in 1 dimension
!bt
\begin{equation}
-\frac{d(J(x)A(x))}{dx}+\frac{d\sigma(t)}{dt}A(x)=\frac{dq(t)}{dt}A(x).
label{eq:lin:cont4}
\end{equation}
!et
!enotice
We have kept the area in equation (ref{eq:lin:cont4}), because we are only considering flow of heat in one dimension and then we can allow for the area to change in the $y$ and $z$ dimension. When the continuity equation is derived in three dimensions, one consider a volume $V(x,y,z)=dxdydz$, then the area in equation (ref{eq:lin:cont4}) will drop out and $d/dx\to\nabla=[\partial/\partial x, \partial/\partial y, \partial/\partial z]$ 
!bnotice The continuity equation in 3 dimensions
!bt
\begin{equation}
-\nabla\cdot\mathbf{J}+\frac{d\sigma(t)}{dt}=\frac{dq(t)}{dt}.
label{eq:lin:cont5}
\end{equation}
!et
!enotice

======= Continuity equation as a linear problem  =======

How can a differential equation be formulated as a matrix problem? To see this we need to discretize equation (ref{eq:lin:cont4}). We will discretize the equation in one dimension, and we will use a regular grid, where we keep the same distance, $h$, between the points. Assume our system has dimension $L$, in figure ref{fig:lin:grid}, there are two examples of discretization.  

FIGURE: [fig-lin/grid.png, width=400 frac=1.0] Examples of discretization of a system with length $L$ (left) the  boundaries lies exactly at the boundary nodes, (right) boundary nodes lies half-way between the grid nodes. label{fig:lin:grid}

There are many things to consider when discretizing equations, but perhaps the most important are
o Treat the boundary nodes correctly. In most cases the dominating numerical errors are introduced through the boundaries. Always draw a picture of the system, if the boundaries lies exactly at the grid nodes it is usually easier to find a good numerical representation. If the boundaries lies a distance from the nodes, e.g. to the right in figure ref{fig:lin:grid}, then one usually need to do some interpolation.
o Should finite volume or finite difference approach be used? A finite volume approach is especially attractive for conservation equations.

!bnotice Finite difference and finite volume
idx{finite volume}
We have already encountered finite difference discretization in the last chapter where we used various approximations to calculate derivatives, i.e. we calculate derivatives by calculating the *difference* between $f(x+h)$ and $f(x)$ (or $f(x+h)$ and $f(x-h)$). The finite volume formulation is also a finite difference scheme, but it is formulated such that we always ensure that the quantity we are simulating is conserved (regardless of numerical errors). Formally, one transforms the divergence term (the term that contains the flux $\nabla \mathbf{J}$) into a surface integral using the Gauss (divergence) theorem
!bt
\begin{equation}
\int_V\nabla\cdot \mathbf{J}=\int_S\mathbf{J}\cdot \hat{\mathbf{n}}
label{eq:lin:dgau}
\end{equation}
!et
!enotice
There are excellent books written on the finite volume method, see e.g. cite{leveque2002finite}. Here we will mainly focus on the key idea, which is to formulate a scheme that conserves the flux.
The process of formulating a finite volume scheme is very close to the derivation of the continuum equation we did in the beginning of the chapter. We consider our numerical discretization as several boxes (exactly like the dotted lines in figure ref{fig:lin:grid}), the continuum equation is written down for each box an therefore we are ensured that the quantities are conserved *regardless of the size of the boxes*.

!bnotice Example: Finite difference and volume discretization of the heat equation
Let us consider the heat equation, where the heat flux is given as
!bt
\begin{equation}
J=-k\frac{dT}{dx},
label{eq:lin:e1}
\end{equation}
!et
where $k$ describes the thermal conductivity of the solid. We will further assume that there is a constant source term $d\sigma/dt=\kappa=const$, and steady state $dq/dt=0$. Then equation (ref{eq:lin:cont4}) can be written
!bt
\begin{equation}
k\frac{d^2T}{dx^2}+\kappa=0,
label{eq:lin:e2}
\end{equation}
!et
The finite difference discretization is now straight forward, just replace the term $d^2T/dx^2$ with a suitable finite difference formula for the second derivative, e.g.
!bt
\begin{align}
k\frac{T(x+h)+T(x-h)-2T(x)}{h^2}+\kappa&=0,\no\\
k\frac{T_{i+1}+T_{i-1}-2T_i}{h^2}+\kappa&=0.
label{eq:lin:e3}
\end{align}
!et
Note that in the last equations we have introduced the short hand notation $T(x)\equiv T_i$, and $T(x\pm h)=T_{i\pm 1}$. 

The finite volume discretization approach is slightly different, we then operate with *cell averaged values*. The heat in the box is the volume averaged heat. Since the divergence term is replaced with a surface integral, equation (ref{eq:lin:dgau}), we calculate the flow of heat into the boundary $x-h/2$ and out of the boundary $x+h/2$ as
!bt
\begin{equation}
\frac{J_{x+h/2}
-J_{x-h/2}}{h}+\kappa=0.
label{eq:lin:e4}
\end{equation}
!et
Note that this equation is exactly the same as equation (ref{eq:lin:cont1}), with the only exception that the point $x$ is placed in the center of the box.
The diffusive flux is $-kdT/dx$, and in order to be consistent with this law we have to write the flux between two cells as proportional to the difference between the cell average values
!bt
\begin{align}
-\frac{k}{h}\left(\frac{T_{i+1}-T_i}{h}-\frac{T_{i}-T_{i-1}}{h}\right)+\kappa&=0\nonumber,\\
k\frac{T_{i+1}+T_{i-1}-2T_i}{h^2}+\kappa&=0.
label{eq:lin:e5}
\end{align}
!et
In this case we actually recover the same equation as we did for the finite difference approach equation (ref{eq:lin:e3}). 
!enotice

=== Boundary conditions ===
Basically there are two types of boundary conditions i) the flux is known at the edges of the computational domain and/or ii) the physical quantity we are solving for is known. To be more specific, and to see how all connects, we will continue with the example above on the heat equation. Consider the outline of nodes as in figure ref{fig:lin:grid}, we will consider two possibilities i) where the physical boundary lies exactly between nodes, and ii) where the physical boundary is exactly at the grid nodes. In the finite volume scheme, we need to make sure that the flux over the surface is calculated correctly, and then we have to use the formulas in figure ref{fig:lin:bbc}
!bt
\begin{equation}
\left.\frac{dT}{dx}\right|_{x=0}=\frac{T_{-1}-T_0}{h}+{\cal O}(h^2).
label{eq:lin:nn}
\end{equation}
!et
Note that if the boundary node lies exactly at $x=0$, we have to replace $T_0$ with $T_{1}$. A flux boundary condition is usually called Neumann boundary condition after Carl Neumann (1832–1925) a German mathematician, and the constant value boundary condition is called Dirichlet boundary condition after another German mathematician, Peter Gustav Lejeune Dirichlet (1805–1859). If the boundary nodes lies exactly at the physical boundary, it is trivial to implement, just replace $T_N=T_b$ i.e. with the boundary value. On the other hand if the physical boundary lies a distance from the node, we have to interpolate the value from the physical coordinate to the simulation node.
!bt
\begin{align}
T_N&=T(x+h)=T(x+h/2+h/2)\no\\
&=T_{N+1/2}+\left.\frac{dT}{dx}\right|_{x+h/2}+{\cal O}(h^2)=T_b+\frac{T_N-T_{N-1}}{h}\frac{h}{2}+{\cal O}(h^2),\text{ hence:}\nonumber\\
T_N&=2T_b-T_{N-1}+{\cal O}(h^2).label{eq:lin:dd}
\end{align}
!et
Notice that the result make sense, $T_b=(T_N+T_{N-1})/2$, i.e. the value midway is the average of the values at the neighboring nodes.

FIGURE: [fig-lin/bbc, width=400 frac=1.0] Flux boundary condition (Neumann), and value boundary condition (Dirichlet). For the upper right boundary condition we use Taylors formula to interpolate, see equation (ref{eq:lin:dd}). label{fig:lin:bbc}

!bnotice Example: Steady state heat equation as a linear problem
Consider the case where we have 4 grid nodes and the outline of the simulation nodes are as in figure ref{fig:lin:grid} to the left, i.e. nodes at the physical  boundaries. Assume a zero flux boundary condition to the left, and a constant temperature,$T_b$, to the right. Write the heat equation
!bt
\begin{equation}
k\frac{d^2T}{dx^2}+\kappa=0,
label{eq:lin:exx1}
\end{equation}
!et
as a matrix equation.

__Solution:__
First, we use the discrete version of equation (ref{eq:lin:exx1}) in equation (ref{eq:lin:e3}) for $i=0, 1, 2, 3$
!bt
\begin{align}
T_{-1}+T_1-2T_0 &=-h^2\kappa/k,\nonumber \\
T_{0}+T_2-2T_1 &=-h^2\kappa/k\nonumber \\
T_{1}+T_3-2T_2 &=-h^2\kappa/k\nonumber \\
T_{2}+T_4-2T_3 &=-h^2\kappa/k.
label{eq:lin:exx2}
\end{align}
!et
Now, we have four equations, but six unknowns ($T_{-1}, T_0, T_1, T_2, T_3, T_4$). $T_{-1}$, and $T_4$ can be found from the boundary conditions. Using the formulas in figure ref{fig:lin:bbc} at the lower left and lower right, we get $dT/dx=0$, and $T_{-1}=T_1$, and $T_4=T_b$. Thus the first and last equation in equation (ref{eq:lin:exx2}), can be written
!bt
\begin{align}
2T_1-2T_0 &=-h^2\kappa/k,\nonumber \\
T_{2}-2T_3 &=-h^2\kappa/k-T_b.
label{eq:lin:2b}
\end{align}
!et
Now, we can formulate equation (ref{eq:lin:exx2}) as a matrix problem, with the unknowns on the left side and the unknown on the right hand side.
!bt
\begin{align}
\left(
\begin{array}{cccc}
-2&2&0&0\\
1&-2&1&0\\
0&1&-2&1\\
0&0&1&-2\\
\end{array}
\right)
\left(
\begin{array}{c}
T_0\\
T_1\\
T_2\\
T_3\\
\end{array}
\right)
=
\left(
\begin{array}{c}
-h^2\kappa/k\\
-h^2\kappa/k\\
-h^2\kappa/k\\
-h^2\kappa/k-T_b
\end{array}
\right).
\end{align}
label{eq:lin:exx4}
!et


!enotice
In principle, to discretize an equation is straight forward, but there are some 

First, we are going to consider a *steady state* solution. Steady state means that the solution does no longer change as a function of time, i.e. $dq/dt=0$ in equation (ref{eq:lin:cont4}). We are also going to assume that the area is constant $A(x)=A$, thus the equation we want to solve is


======= Solving linear equations =======
There are a number of excellent books covering this topic, see e.g. cite{press2007,trefethen1997,stoer2013,strang2019}.
In most of the examples covered in this course we will encounter problems where we have a set of *linearly independent* equations and one equation for each unknown. For these type of problems there are a number of methods that can be used, and they will find a solution in a finite number of steps. If a solution cannot be found it is usually because the equations are not linearly independent, and our formulation of the physical problem is wrong.

Assume that we would like to solve the following set of equations:
!bt
\begin{align}
2x_0+x_1+x_2+3x_3&=1, label{eq:lin:la} \\
x_0+x_1+3x_2+x_3&=-3, label{eq:lin:lb} \\
x_0+4x_1+x_2+x_3&=2, label{eq:lin:lc} \\
x_0+x_1+2x_2+2x_3&=1. label{eq:lin:ld} 
\end{align}
!et
These equations can be written in matrix form as:
!bt
\begin{equation}
\mathbf{A\cdot x}=\mathbf{b},
label{eq:lin:mat}
\end{equation}
!et
where:
!bt
\begin{equation}
\mathbf{A}\equiv\begin{pmatrix}
2&1&1&3\\
1&1&3&1\\
1&4&1&1\\
1&1&2&2
\end{pmatrix}
\qquad
\mathbf{b}\equiv
\begin{pmatrix}
1\\-3\\2\\1
\end{pmatrix}
\qquad
\mathbf{x}\equiv
\begin{pmatrix}
x_0\\x_1\\x_2\\x_3
\end{pmatrix}.
label{eq:lin:matA}
\end{equation}
!et
You can easily verify that $x_0=-4, x_1=1, x_2=-1, x_3= 3$ is the
solution to the above equations by direct substitution. If we were to
replace one of the above equations with a linear combination of any of
the other equations, e.g. replace equation (ref{eq:lin:ld}) with
$3x_0+2x_1+4x_2+4x_3=-2$, there would be no unique solution (infinite
number of solutions). This can be checked by calculating the determinant of the matrix $\mathbf{A}$, if $\det \mathbf{A}=0 $,  
What is the difficulty in solving these equations? Clearly if none of the equations are linearly dependent, and we have $N$ independent linear equations, it should be straight forward to solve them? Two major numerical problems are i) even if the equations are not exact linear combinations of each other, they could be very close, and as the numerical algorithm progresses they could at some stage become linearly dependent due to roundoff errors. ii) roundoff errors may accumulate if the number of equations are large cite{press2007}.

===== Gauss-Jordan elimination =====
idx{Gauss-Jordan elimination}
Let us continue the discussion by consider Gauss-Jordan elimination, which is a *direct* method. A direct method uses a final set of operations to obtain a solution. According to cite{press2007} Gauss-Jordan elimination is the method of choice if we want to find the inverse of $\mathbf{A}$. However, it is slow when it comes to calculate the solution of equation
(ref{eq:lin:mat}). Even if speed and memory use is not an issue, it is also not advised to first find the inverse, $\mathbf{A}^{-1}$, of $\mathbf{A}$, then multiply it with $\mathbf{b}$ to obtain the solution, due to roundoff errors (Roundoff errors occur whenever we subtract to numbers that are very close to each other). To simplify our notation, we write equation (ref{eq:lin:matA}) as:
!bt
\begin{equation}
\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
1&1&3&1&-3\\
1&4&1&1&2\\
1&1&2&2&1
\end{array}
\right).
\end{equation}
!et
The numbers to the left of the vertical dash is the matrix $\mathbf{A}$, and to the right is the vector $\mathbf{b}$. The Gauss-Jordan elimination procedure proceeds by doing the same operation on the right and left side of the dash, and the goal is to get only zeros on the lower triangular part of the matrix. This is achieved by multiplying rows with the same (nonzero) number, swapping rows, adding a multiple of a row to another:
!bt
\begin{align}
&\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
1&1&3&1&-3\\
1&4&1&1&2\\
1&1&2&2&1
\end{array}
\right)\to
\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
0&1/2&5/2&-1/2&-7/2\\
0&7/2&1/2&-1/2&3/2\\
0&1/2&3/2&1/2&1/2
\end{array}
\right)\to\label{eq:lin:gj1}\\
&\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
0&1/2&5/2&-1/2&-7/2\\
0&0&-17&3&26\\
0&0&1&-1&4
\end{array}
\right)
\to
\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
0&1/2&5/2&-1/2&-7/2\\
0&0&-17&3&26\\
0&0&0&14/17&42/17
\end{array}
\right)\no
\end{align}
!et
The operations done are: ($1\to2$) multiply first row with $-1/2$ and add to second, third and the fourth row, ($2\to 3$) multiply second row with $-7$, and add to third row, multiply second row with $-1$ and add to fourth row, ($3\to4$) multiply third row with $-1/17$ and add to fourth row. These operations can easily be coded into Python:
@@@CODE src-lin/nlin_sym.py fromto: A = np.array@# Back
The python code is a bit compact, below there is an implementation using for loops
!bc pypro
# Gauss-Jordan Forward Elimination - for loops
for i in range(N):
    for j in range(i+1,N):
        fact    = A[j,i]/A[i,i]
        for k in range(i+1,N):
            A[j,k] = A[j,k]- fact*A[i,k]
        b[j]  = b[j]- b[j-1]*fact
	A[j,i]= 0. # alternatively k=i,...,N
!ec

!bnotice Number of (long) operations
The code above reveals that that there are quite a few multiplications or divisions being performed in the forward elimination. Multiplications and divisions are more time consuming than addition and subtraction, and are usually termed *long* operations. Not all loops runs from zero to $N$, the innermost from $k=i+1\ldots N-1$, i.e. a total of $N-i-2$, the second contains $N-i-2$ and one multiplication for the `b` vector. Hence we have number of long operations
!bt
\begin{equation}
\sum_{i=0}^{N-1}(N-i-2)^2+(N-i-2)=\frac{N}{3}(N^2-3N+2).
label{}
\end{equation}
!et
The important result is that when the system of equations becomes large $N^3\gg N^2$ and the algorithm scales as $N^3$.
!enotice
Notice that the final matrix has only zeros beyond the diagonal, such a matrix is called *upper triangular*. We still have not found the final solution, but from an upper triangular (or lower triangular) matrix it is trivial to determine the solution. The last row immediately gives us $14/17z=42/17$ or $z=3$, now we have the solution for z and the next row gives: $-17y+3z=26$ or $y=(26-3\cdot3)/(-17)=-1$, and so on. In a more general form, we can write our solution of the matrix $\mathbf{A}$ after making it upper triangular as:
!bt
\begin{equation}
\begin{pmatrix}
a^\prime_{0,0}&a^\prime_{0,1}&a^\prime_{0,2}&a^\prime_{0,3}\\
0&a^\prime_{1,1}&a^\prime_{1,2}&a^\prime_{1,3}\\
0&0&a^\prime_{2,2}&a^\prime_{2,3}\\
0&0&0&a^\prime_{3,3}
\end{pmatrix}
\cdot
\begin{pmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
b^\prime_{0}\\
b^\prime_{1}\\
b^\prime_{2}\\
b^\prime_{3}
\end{pmatrix}
label{eq:lin:back}
\end{equation}
!et
The back substitution can then be written formally as:
!bt
\begin{equation}
x_i=\frac{1}{a^\prime_{ii}}\left[b_i^\prime-\sum_{j=i+1}^{N-1}a^\prime_{ij}x_j\right],\quad i=N-1,N-2,\ldots,0
label{eq:lin:back2}
\end{equation}
!et
The back substitution can now easily be implemented in Python as:
@@@CODE src-lin/nlin_sym.py fromto: # Back@# Back substitution - for loop
Notice that in the Python implementation, we have used vector operations instead of for loops. This makes the code more efficient, but it could also be implemented with for loops: 
@@@CODE src-lin/nlin_sym.py fromto: # Back substitution - for loop@print
!bnotice Number of (long) operations
As for the forward elimination, we can find how the backward substitution scales. Notice that here there are only two loops, hence we have number of long operations
!bt
\begin{equation}
\sum_{i=0}^{N-1}(N-i-1)=\frac{N}{2}(N-1).
label{}
\end{equation}
!et
Thus, the backward substitution scales as $N^2$. 
!enotice
There are at least two things to notice with our implementation:
* Matrix and vector notation makes the code more compact and efficient. In order to understand the implementation it is advised to put $i=1, 2, 3, 4$, and then execute the statements in the Gauss-Jordan elimination and compare with equation (ref{eq:lin:gj1}).
* The implementation of the Gauss-Jordan elimination is not robust, in particular one could easily imagine cases where one of the leading coefficients turned out as zero, and the routine would fail when we divide by `A[i-1,i-1]`. By simply changing equation (ref{eq:lin:lb}) to $2x_0+x_1+3x_2+x_3=-3$, when doing the first Gauss-Jordan elimination, both $x_0$ and $x_1$ would be canceled. In the next iteration we try to divide next equation by the leading coefficient of $x_1$, which is zero, and the whole procedure fails.
===== Pivoting =====
idx{pivoting}
The solution to the last problem is solved by what is called *pivoting*. The element that we divide on is called the *pivot element*. It actually turns out that even if we do Gauss-Jordan elimination *without* encountering a zero pivot element, the Gauss-Jordan procedure is numerically unstable in the presence of roundoff errors cite{press2007}. There are two versions of pivoting, *full pivoting* and *partial pivoting*. In partial pivoting we only interchange rows, while in full pivoting we also interchange rows and columns. Partial pivoting is much easier to implement, and the algorithm is as follows:
o Find the row in $\mathbf{A}$ with largest absolute value in front of $x_0$ and change with the first equation, switch corresponding elements in $\mathbf{b}$
o Do one Gauss-Jordan elimination, find the row in $\mathbf{A}$ with the largest absolute value in front of $x_1$ and switch with the second (same for $\mathbf{b}$), and so on.
For a linear equation we can multiply with a number on each side and the equation would be unchanged, so if we where to multiply one of the equations with a large value, we are almost sure that this equation would be placed first by our algorithm. This seems a bit strange as our mathematical problem is the same. Sometimes the linear algebra routines tries to normalize the equations to find the pivot element that would have been the largest element if all equations were normalized according to some rule, this is called *implicit pivoting*.  
===== LU decomposition =====
idx{LU decomposition}
As we have already seen, if the matrix $\mathbf{A}$ is reduced to a triangular form it is trivial to calculate the solution by using back substitution. Thus if it was possible to decompose the matrix $\mathbf{A}$ as follows:

!bt
\begin{equation}
\mathbf{A}=\mathbf{L}\cdot\mathbf{U}label{eq:lin:lu}
\end{equation}
!et

!bt
\begin{equation}
\begin{pmatrix}
a_{0,0}&a_{0,1}&a_{0,2}&a_{0,3}\\
a_{1,0}&a_{1,1}&a_{1,2}&a_{1,3}\\
a_{2,0}&a_{2,1}&a_{2,2}&a_{2,3}\\
a_{3,0}&a_{3,1}&a_{3,2}&a_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
l_{0,0}&0&0&0\\
l_{1,0}&l_{1,1}&0&0\\
l_{2,0}&l_{2,1}&l_{2,2}&0\\
l_{3,0}&l_{3,1}&l_{3,2}&l_{3,3}
\end{pmatrix}
\cdot
\begin{pmatrix}
u_{0,0}&u_{0,1}&u_{0,2}&u_{0,3}\\
0&u_{1,1}&u_{1,2}&u_{1,3}\\
0&0&u_{2,2}&u_{2,3}\\
0&0&0&u_{3,3}
\end{pmatrix}.
\end{equation}
!et
The solution procedure would then be to rewrite equation (ref{eq:lin:mat}) as:
!bt
\begin{align}
\mathbf{A\cdot x}=\mathbf{L}\cdot\mathbf{U}\cdot\mathbf{x}=\mathbf{b},label{eq:lin:matb}
\end{align}
!et
If we define a new vector $\mathbf{y}$:
!bt
\begin{align}
\mathbf{y}\equiv\mathbf{U}\cdot\mathbf{x},
\end{align}
!et
we can first solve for the $\mathbf{y}$ vector:
!bt
\begin{align}
\mathbf{L}\cdot\mathbf{y}=\mathbf{b},label{eq:lin:for}
\end{align}
!et
and then for $\mathbf{x}$:
!bt
\begin{align}
\mathbf{U}\cdot\mathbf{x}=\mathbf{y}.
\end{align}
!et
Note that the solution to equation (ref{eq:lin:for}) would be done by *forward substitution*:
!bt
\begin{equation}
y_i=\frac{1}{l_{ii}}\left[b_i-\sum_{j=0}^{i-1}l_{ij}x_j\right],\quad i=1,2,\ldots N-1.
label{eq:lin:back3}
\end{equation}
!et
Why go to all this trouble? First of all it requires (slightly) less operations to calculate the LU decomposition and doing the forward and backward substitution than the Gauss-Jordan procedure discussed earlier. Secondly, and more importantly, is the fact that in many cases one would like to calculate the solution for different values of the $\mathbf{b}$ vector in equation (ref{eq:lin:matb}). If we do the LU decomposition first we can calculate the solution quite fast using backward and forward substitution for any value of the $\mathbf{b}$ vector.

The NumPy function "`solve`":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html", uses LU decomposition and partial pivoting, and we can find the solution to our previous problem simply by the following code:
@@@CODE src-lin/nlin_sym.py fromto: from numpy.linalg@#end

======= Iterative methods  =======
The methods described so far are what is called *direct* methods. The direct methods for very large systems might suffer from round off errors. That means that even if the computer has found a solution, the solution is "polluted" by round off errors, or stated more clearly: your solution for $\mathbf{x}$, when entered into the original equation $\mathbf{A}\mathbf{x}\neq\mathbf{b}$. Below we will describe one trick, and two alternative methods to the direct methods.
===== Iterative improvement =====
The first method cite{press2001} assumes that we already have solved the matrix equation (ref{eq:lin:mat}), and obtained an *estimate* $\mathbf{\hat{x}}$ of the true solution $\mathbf{x}$. Assume that $\mathbf{\hat{x}}=\mathbf{x}+\delta\mathbf{x}$, and that
!bt
\begin{equation}
\mathbf{A}\cdot\mathbf{\hat{x}}=\mathbf{A}\cdot(\mathbf{x}+\delta\mathbf{x})=\mathbf{b}+\delta\mathbf{b},
label{eq:lin:itb}
\end{equation}
!et
subtracting equation (ref{eq:lin:mat}) we get
!bt
\begin{equation}
\mathbf{A}\cdot\delta\mathbf{x}=\delta\mathbf{b}.
label{eq:lin:itb2}
\end{equation}
!et
Solving equation (ref{eq:lin:itb}) for $\delta\mathbf{b}$ an inserting in the equation above, we get
!bt
\begin{equation}
\mathbf{A}\cdot\delta\mathbf{x}=\mathbf{A}\cdot\mathbf{\hat{x}}-\mathbf{b}.
label{eq:lin:itb3}
\end{equation}
!et
The usefulness of this method assumes that we have already obtained the LU decomposition of $\mathbf{A}$, and if possible one should use a higher precision to calculate the right hand side, since there will be a lot of cancellations. Then the whole computational process it is simply to calculate the right hand side and backsubstitute. The improved solution is then obtained by subtracting $\delta\mathbf{x}$ from $\mathbf{\hat{x}}$.

===== The Jacobi method =====
idx{Jacobi method}
A completely different approach is the Jacobian method, which is simply to decompose the $\mathbf{A}$ matrix in the following way
!bt
\begin{equation}
\mathbf{A}=\mathbf{D}+\mathbf{R}
label{eq:lin:DR}
\end{equation}
!et

!bt
\begin{align}
&\begin{pmatrix}
a_{0,0}&a_{0,1}&a_{0,2}&a_{0,3}\\
a_{1,0}&a_{1,1}&a_{1,2}&a_{1,3}\\
a_{2,0}&a_{2,1}&a_{2,2}&a_{2,3}\\
a_{3,0}&a_{3,1}&a_{3,2}&a_{3,3}
\end{pmatrix}
\no\\
&=
\begin{pmatrix}
a_{0,0}&0&0&0\\
0&a_{1,1}&0&0\\
0&0&a_{2,2}&0\\
0&0&0&a_{3,3}
\end{pmatrix}
+
\begin{pmatrix}
0&a_{0,1}&a_{0,2}&a_{0,3}\\
a_{1,0}&0&a_{1,2}&a_{1,3}\\
a_{2,0}&a_{2,1}&0&a_{2,3}\\
a_{3,0}&a_{3,1}&a_{3,2}&0
\end{pmatrix}.
label{eq:lin:DR2}
\end{align}
!et
We can then write equation (ref{eq:lin:mat}) as
!bt
\begin{equation}
\mathbf{D}\mathbf{x}=\mathbf{b}-\mathbf{R}\cdot\mathbf{x}.
label{eq:lin:jc}
\end{equation}
!et
How does this help us? First of all, the matrix $\mathbf{D}$ is easy to invert as it is diagonal, the inverse can be found by simply replace $a_{ii}\to 1/a_{ii}$. But $\mathbf{x}$ is still present on the right hand side? This is where the *iterations* comes into play, we simply guess at an initial solution $\mathbf{x}^k$, and then we use equation (ref{eq:lin:jc}) to calculate the next solution $\mathbf{x}^{k+1}$, and so on
!bt
\begin{equation}
\mathbf{x}^{k+1}=\mathbf{D}^{-1}(\mathbf{b}-\mathbf{R}\cdot\mathbf{x}^{k}).
label{eq:lin:jc2}
\end{equation}
!et
Lets write it out on component form for a $4\times4$ matrix to see what is going on
!bt
\begin{align}
x_0^{k+1} &=\frac{1}{a_{00}}(b_0-a_{01}x_1^k-a_{02}x_2^k-a_{03}x_3^k),
label{eq:lin:jc3a}\\
x_1^{k+1} &=\frac{1}{a_{11}}(b_1-a_{10}x_0^k-a_{12}x_2^k-a_{13}x_3^k),
label{eq:lin:jc3b}\\
x_2^{k+1} &=\frac{1}{a_{22}}(b_2-a_{20}x_0^k-a_{21}x_1^k-a_{23}x_3^k),
label{eq:lin:jc3c}\\
x_3^{k+1} &=\frac{1}{a_{33}}(b_3-a_{30}x_0^k-a_{31}x_1^k-a_{32}x_2^k).
label{eq:lin:jc3d}
\end{align}
!et
Below is a Python implementation
!bc pycod
def solve_jacobi(A,b,x=-1,w=1,max_iter=1000,EPS=1e-6):
    """
    Solves the linear system Ax=b using the Jacobian method, stops if
    solution is not found after max_iter or if solution changes less 
    than EPS
    """
    if(x==-1): #default guess 
        x=np.zeros(len(b))
    D=np.diag(A)
    R=A-np.diag(D)
    eps=1
    x_old=x
    iter=0
    w=0.1
    while(eps>EPS and iter<max_iter):
        iter+=1
        x=w*(b-np.dot(R,x_old))/D + (1-w)*x_old
        eps=np.sum(np.abs(x-x_old))
        x_old=x
    print('found solution after ' + str(iter) +' iterations')
    return x
!ec
A sufficient criteria for the Jacobian method to converge is if the matrix $A$ is diagonally dominant. In the implementation above we have included a weight, which sometimes can help in the convergence even if the matrix is not diagonally dominant. 
% if FORMAT == 'ipynb':
Test out the following examples, note that by rearranging the problem we can achieve convergence. 
!bc pycod
A = np.array([[10, -1, 2,0],[-1, 11, -1,3],
               [2, -1, 10, -1],[0, 3, -1, 8 ]],float)
b = np.array([6,25,-11,15],float)
print(A)
s1=solve_jacobi(A,b)
print(s1)

A = np.array([[2, 1, 1, 3],[1, 1, 3, 1],
              [1, 4, 1, 1],[1, 1, 2, 2 ]],float)
b = np.array([1,-3,2,1],float)
#try w=1, and w=0.01
s2=solve_jacobi(A,b,w=1)
print(s2)

#exchange row 3 and 4, and put w=0.1
A = np.array([[2, 1, 1, 3,],[1, 4, 1, 1, ],
              [1, 1, 3, 1],[1, 1, 2, 2 ]],float)
b = np.array([1,2,-3,1],float)
s3=solve_jacobi(A,b,w=0.1)
print(s3)
!ec
% endif

The iterative method can be appealing if we do not need a high accuracy, we can choose to stop whenever $|\mathbf{x}^{k+1}-\mathbf{x}^k|$ is small enough. For the direct method we have to follow through all the way.
!bnotice Convergence
The Jacobi method converges if the matrix $\mathbf{A}$ is strictly diagonally dominant. Strictly diagonally dominant means that the absolute value of each entry on the diagonal is greater than the sum of the absolute values of the other entries in the same row, i.e if $|a_{00}|>|a_{01}+a_{02}+\cdots|$. In general it can be shown that a iterative scheme $\mathbf{x}^{k+1}=\mathbf{P}\cdot \mathbf{x}^k+\mathbf{q}$ is convergent *if and only if* every eigenvalue, $\lambda$, of $\mathbf{P}$ satisfies $|\lambda|<1$, i.e. the *spectral radius* $\rho(\mathbf{P})<1$.
!enotice

===== The Gauss-Seidel method =====
idx{Gauss-Seidel method}
It is tempting in equation (ref{eq:lin:jc3a}) to use our estimate of $x_0^{k+1}$ in the next equation, equation (ref{eq:lin:jc3b}), instead of $x_0^k$. After all our estimate $x_0^{k+1}$ is an *improved* estimate. This is actually the Gauss-Seidel method. This method also has the advantage that if there are memory issues, one can overwrite the old value of $x_i^k$. Usually the Gauss-Seidel method converges faster, but not always. A plus for the Jacobi method is that is can be  parallelised, as the calculations is only dependent on the old values and do not require information about the new values as for the Gauss Seidel method. Below is a Python implementation of the Gauss-Seidel method   

!bc pycod
def solve_GS(A,b,x=-1,max_iter=1000,EPS=1e-6):
    """
    Solves the linear system Ax=b using the Gauss-Seidel method, stops if
    solution is not found after max_iter or if solution changes less 
    than EPS
    """
    if(x==-1):
        x=np.zeros(len(b))
    D=np.diag(A)
    R=A-np.diag(D)
    eps=1
    iter=0
    while(eps>EPS and iter<max_iter):
        iter+=1
        eps=0.
        for i in range(len(x)):
            tmp=x[i]
            x[i]=(b[i]- np.dot(R[i,:],x))/D[i]
            eps+=np.abs(tmp-x[i])
    print('found solution after ' + str(iter) +' iterations')
    return x
!ec

% if FORMAT == 'ipynb':
Run the code below to test the Gauss-Seidel method 
!bc pycod
A = np.array([[2, 1, 1, 3,],[1, 4, 1, 1, ],
              [1, 1, 3, 1],[1, 1, 2, 2 ]],float)
b = np.array([1,2,-3,1],float)

s3b=solve_GS(A,b)
print(s3b)
!ec
% endif
======= Example: Linear regression =======
idx{linear regression}
In the previous section, we considered a system of $N$ equations and $N$ unknown ($x_0, x_1,\ldots, x_N$). In general we might have more equations than unknowns or more unknowns than equations. An example of the former is linear regression, we might have many data points and we would like to fit a line through the points. How do you fit a single lines to more than two points that does not line on the same line? One way to do it is to minimize the distance from the line to the points, as illustrated in figure ref{fig:lin:reg}.
FIGURE: [fig-lin/reg.png, width=800 frac=.9] Linear regression by minimizing the total distance to all the points. label{fig:lin:reg}
Mathematically we can express the distance between a data point $(x_i,y_i)$ and the line $f(x)$ as $y_i-f(x_i)$. Note that this difference can be negative or positive depending if the data point lies below or above the line. We can then take the absolute value of all the distances, and try to minimize them. When we minimize something we take the derivative of the expression and put it equal to zero.  As you might remember from Calculus it is extremely hard to work with the derivative of the absolute value, because it is discontinuous. A much better approach is to square each distance and sum them:
!bt
\begin{equation}
S=\sum_{i=0}^{N-1}(y_i-f(x_i))^2=\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)^2.
label{eq:lin:lsq}
\end{equation}
!et
(For the example in figure ref{fig:lin:reg}, $N=5$.) This is the idea behind *least square*, and linear regression. One thing you should be aware of is that points lying far from the line will contribute more to equation (ref{eq:lin:lsq}). The underlying assumption is that each data point provides equally precise information about the process, this is often not the case. When analyzing experimental data, there may be points deviating from the expected behaviour, it is then important to investigate if these points are more affected by measurements errors than the others. If that is the case one should give them less weight in the least square estimate, by extending the formula above:
!bt
\begin{equation}
S=\sum_{i=0}^{N-1}\omega_i(y_i-f(x_i))^2=\sum_{i=0}^3\omega_i(y_i-a_0-a_1x_i)^2,
label{eq:lin:lsqm}
\end{equation}
!et
$\omega_i$ is a weight factor.

===== Solving least square, using algebraic equations =====
Let us continue with equation (ref{eq:lin:lsq}), the algebraic solution is to simply find the value of $a_0$ and $a_1$ that minimizes $S$:
!bt
\begin{align}
\frac{\partial S}{\partial a_0} &=-2\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)=0,
label{eq:lin:ls1} \\
\frac{\partial S}{\partial a_1} &=-2\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)x_i=0.
label{eq:lin:ls2}
\end{align}
!et
Defining the mean value as $\overline{x}=\sum_ix_i/N$ and $\overline{y}=\sum_iy_i/N$, we can write equation (ref{eq:lin:ls1}) and (ref{eq:lin:ls2})  as:
!bt
\begin{align}
\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)&=N\overline{y}-a_0N-a_1N\overline{x}=0,
label{eq:lin:ls1a} \\
\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)x_i&=\sum_iy_ix_i-a_0N\overline{x}-a_1\sum_ix_ix_i=0.
label{eq:lin:ls2b}
\end{align}
!et
Solving equation (ref{eq:lin:ls1a}) with respect to $a_0$, and inserting the expression into equation (ref{eq:lin:ls2b}), we find:
!bt
\begin{align}
a_0&=\overline{y}-a_1\overline{x},label{eq:lin:ls1c} \\
a_1&=\frac{\sum_iy_ix_i-N\overline{x}\overline{y}}{\sum_ix_i^2-N\overline{x}^2}
=\frac{\sum_i(x_i-\overline{x})(y_i-\overline{y})}{\sum_i(x_i-\overline{x})^2}.
label{eq:lin:ls2d}
\end{align}
!et
We leave it as an exercise to show the last expression for $a_1$.  
Clearly the equation (ref{eq:lin:ls2d}) above will in most cases have
a solution. But in addition to a solution, it would be good to have an
idea of the goodness of the fit. Intuitively it make sense to add all
the distances (residuals) $d_i$ in figure ref{fig:lin:reg}. This is
basically what is done when calculating $R^2$ (R-squared). However, we
would also like to compare the $R^2$ between different
datasets. Therefor we need to normalize the sum of residuals, and
therefore the following form of the $R^2$ is used:
!bt
\begin{equation}
R^2=1-\frac{\sum_{i=0}^{N-1}(y_i-f(x_i))^2}{\sum_{i=0}^{N-1}(y_i-\overline{y})^2}.
label{eq:lin:r2}
\end{equation}
!et
In python we can implement equation (ref{eq:lin:ls1c}), (ref{eq:lin:ls2d}) and (ref{eq:lin:r2}) as:
@@@CODE src-lin/regr.py fromto: def OLS@def plot

===== Least square as a linear algebra problem =====
It turns out that the least square problem can be formulated as a
matrix problem. (Two great explanations see "linear regression by
matrices":"https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b",
and
"$R^2$-squared":"https://medium.com/@andrew.chamberlain/a-more-elegant-view-of-r-squared-a0a14c177dc3".)
If we define a matrix $\mathbf{X}$ containing the observations $x_i$
as:
!bt
\begin{align}
\mathbf{X} &=
\begin{pmatrix}
1&x_0\\
1&x_1\\
\vdots&\vdots\\
1&x_{N-1}
\end{pmatrix}.
label{eq:lin:mreg1}
\end{align}
!et
We introduce a vector containing all the response $\mathbf{y}$, and the
regression coefficients $\mathbf{a}=(a_0,a_1)$. Then we can write
equation (ref{eq:lin:lsqm}) as a matrix equation:
!bt
\begin{equation}
S=(\mathbf{y}-\mathbf{X\cdot a})^T(\mathbf{y}-\mathbf{X\cdot a}).
label{eq:lin:mregS}
\end{equation}
!et
*Note that this equation can easily be extended to more than one
observation variable $x_i$*. By simply differentiating equation
(ref{eq:lin:mregS}) with respect to $\mathbf{a}$, we can show that
the derivative has a minimum when (see proof below):
!bt
\begin{equation}
\mathbf{X}^T\mathbf{X a}=\mathbf{X}^T\mathbf{y}
label{eq:lin:mregS2}
\end{equation}
!et
Below is a python implementation of equation (ref{eq:lin:mregS2}).
@@@CODE src-lin/regr.py fromto: def OLSM@def main

===== Working with matrices on component form =====
Whenever you want to do some manipulation with matrices, it is very useful to simply write them on component form. If we multiply two matrices $\mathbf{A}$ and $\mathbf{B}$ to form a new matrix $\mathbf{C}$, the components of the new matrix is simply $\mathbf{C}_{ij}=\sum_k\mathbf{A}_{ik}\mathbf{B}_{kj}$. The strength of doing this is that the elements of a matrix, e.g. $\mathbf{A}_{ik}$ are *numbers*, and we can move them around. Proving that e.g. $(\mathbf{A}\mathbf{B})^T=\mathbf{B}^T\mathbf{A}^T$ is straight forward using the component form. The transpose of a matrix is simply to exchange columns and rows, hence $\mathbf{C}_{ij}^T=\mathbf{C}_{ji}$
!bt
\begin{equation}
\mathbf{C}_{ij}^T=\mathbf{C}_{ji}=\sum_k\mathbf{A}_{jk}\mathbf{B}_{ki}=\sum_k\mathbf{B}^T_{ik}\mathbf{A}^T_{kj}
=(\mathbf{B}^T\mathbf{A}^T)_{ij},
label{eq:lin:trans}
\end{equation}
!et
thus $\mathbf{C}^T=\mathbf{B}^T\mathbf{A}^T$. To derive equation (ref{eq:lin:mregS2}), we need to take the derivative of equation (ref{eq:lin:mregS2}) with respect to $\mathbf{a}$.
What we mean by this is that we want to evaluate $\partial S/\partial a_k$ for all the components of $\mathbf{a}$.
A useful rule is $\partial a_i/\partial a_k=\delta_{ik}$, where $\delta_{ik}$ is the Kronecker delta, it takes the value of one if $i=k$ and zero otherwise. We can write $S=\mathbf{y}^T\mathbf{y}-\mathbf{y}\mathbf{X\cdot a}
-(\mathbf{X\cdot a})^T\mathbf{y}-(\mathbf{X\cdot a})^T\mathbf{X\cdot a}$. All terms that do not contain $\mathbf{a}$ are zero, thus we only need to evaluate the following terms
!bt
\begin{align}
\frac{\partial}{a_k}(\mathbf{X\cdot a})^T\mathbf{y} &=\frac{\partial}{a_k}(\mathbf{a}^T\cdot \mathbf{X}^T\mathbf{y})=\frac{\partial}{a_k}\sum_{ij}\mathbf{a}^T_i\mathbf{X}^T_{ij}\mathbf{y}_j
=\sum_{ij}\delta_{ik}\mathbf{X}^T_{ij}\mathbf{y}_j\no\\
&=\sum_{j}\mathbf{X}^T_{kj}\mathbf{y}_j=\mathbf{X}^T\mathbf{y} \\
\frac{\partial}{a_k}\mathbf{y}^T\mathbf{X\cdot a}&=\frac{\partial}{a_k}\sum_{ij}\mathbf{y}^T_i\mathbf{X}_{ij}\mathbf{a}_j
=\sum_{ij}\mathbf{y}^T_i\mathbf{X}_{ij}\delta_{jk}=\sum_{j}\mathbf{y}^T_{i}\mathbf{X}_{ik}\no\\
&=\sum_{j}\mathbf{y}^T_{i}\mathbf{X}^T_{ki}=\mathbf{X}^T\mathbf{y} \\
\frac{\partial}{a_k} (\mathbf{X\cdot a})^T\mathbf{X\cdot a}&=
\frac{\partial}{a_k}\sum_{ijl} \mathbf{a}^T_i\mathbf{X}^T_{ij}\mathbf{X}_{jl}\mathbf{a}_l=
\sum_{ijl}(\delta_{ik}\mathbf{X}^T_{ij}\mathbf{X}_{jl}\mathbf{a}_l+\mathbf{a}^T_i\mathbf{X}^T_{ij}\mathbf{X}_{jl}\delta_{lk})\no\\&=\sum_{jl}\mathbf{X}^T_{kj}\mathbf{X}_{jl}
\mathbf{a}_l+\sum_{ij}\mathbf{a}^T_i\mathbf{X}^T_{ij}\mathbf{X}_{jk}\no\\
&=\mathbf{X}^T\mathbf{X}\mathbf{a}+\sum_{ij}\mathbf{X}^T_{kj}\mathbf{X}_{ji}\mathbf{a}_i
= 2\mathbf{X}^T\mathbf{X}\mathbf{a}.
label{}
\end{align}
!et
It then follows that $\partial S/\partial \mathbf{a} = 0$ when
!bt
\begin{equation}
\mathbf{X}^T\mathbf{X a}=\mathbf{X}^T\mathbf{y}.
label{eq:lin:matpr}
\end{equation}
!et

======= Sparse matrices and Thomas algorithm =======
idx{sparse matrix}
idx{Thomas algorithm}
In many practical examples, such as solving partial differential
equations the matrices could be quite large and also contain a lot of
zeros. A very important class of such matrices are *banded matrices*
this is a type of *sparse matrices* containing a lot of zero elements,
and the non-zero elements are confined to diagonal bands. In the
following we will focus on one important type of sparse matrix the
tridiagonal. In the next section we will show how it enters naturally
in solving the heat equation. It turns out that solving banded
matrices is quite simple, and can be coded quite efficiently. As with
the Gauss-Jordan example, lets consider a concrete example:
!bt
\begin{align}
&\left(
\begin{array}{ccccc|c}
b_0&c_0&0&0&0&r_0\\
a_1&b_1&c_1&0&0&r_1\\
0&a_2&b_2&c_2&0&r_2\\
0& 0&a_3&b_3&c_3&r_3\\
0& 0& 0&a_4&b_4&r_4
\end{array}
\right)
\end{align}
!et
The right hand side is represented with $r_i$. The first Gauss-Jordan
step is simply to divide by $b_0$, then we multiply with $-a_1$ and
add to second row:
!bt
\begin{align}
\to \left(
\begin{array}{ccccc|c}
1&c_0^\prime&0&0&0&r_0^\prime\\
0&b_1-a_1c_0^\prime&c_1&0&0&r_1-a_0r_0^\prime\\
0&a_2&b_2&c_2&0&r_2\\
0& 0&a_3&b_3&c_3&r_3\\
0& 0& 0&a_4&b_4&r_4
\end{array}
\right),
\end{align}
!et
Note that we have introduced some new symbols to simplify the
notation: $c_0^\prime=c_0/b_0$ and $r_0^\prime=r_0/b_0$. Then we
divide by $b_1-a_1c_0^\prime$:
!bt
\begin{align}
&\left(
\begin{array}{ccccc|c}
1&c_0^\prime&0&0&0&r_0^\prime\\
0&1&c_1^\prime&0&0&r_1^\prime\\
0&a_2&b_2&c_2&0&r_2\\
0& 0&a_3&b_3&c_3&r_3\\
0& 0& 0&a_4&b_4&r_4
\end{array}
\right),
\end{align}
!et
where $c_1^\prime=c_1/(b_1-a_1c_0^\prime)$ and
$r_1^\prime=(r_1-a_0r_0^\prime)/(b_1-a_1c_0^\prime)$. If you continue
in this manner, you can easily convince yourself that to transform a
tridiagonal matrix to the following form:
!bt
\begin{align}
\to \left(
\begin{array}{ccccc|c}
1&c_0^\prime&0&0&0&r_0^\prime\\
0&1&c_1^\prime&0&0&r_1^\prime\\
0&0&1&c_2^\prime&0&r_2^\prime\\
0& 0&0&1&c_3^\prime&r_3^\prime\\
0& 0& 0&0&1&r_4^\prime
\end{array}
\right),
\end{align}
!et
where:
!bt
\begin{align}
c_0^\prime =\frac{c_0}{b_0} \qquad &r_0^\prime=\frac{r_0}{b_0}
label{eq:lin:th0} \\
c_i^\prime
=\frac{c_i}{b_i-a_ic_{i-1}^\prime}\qquad
&r_i^\prime=\frac{r_i-a_ir_{i-1}^\prime}{b_i-a_ic_{i-1}^\prime}
\quad\text{, for }i=1,2,\ldots,N-1label{eq:lin:thi} 
\end{align}
!et
Note that we where able to reduce the tridiagonal matrix to an *upper
triangular* matrix in only *one* Gauss-Jordan step. This equation can
readily be solved using back-substitution, which can also be
simplified as there are a lot of zeros in the upper part. Let us
denote the unknowns $x_i$ as we did for the Gauss-Jordan case, now we
can find the solution as follows:
!bt
\begin{align}
x_{N-1} & = r_{N-1}^\prime label{eq:lin:this0} \\
x_i     & = r_i^\prime-x_{i+1}c_i^\prime\quad\text{, for } i=N-2,N-3,\ldots,0
label{eq:lin:thisi}
\end{align}
!et
Equation (ref{eq:lin:th0}), (ref{eq:lin:thi}), (ref{eq:lin:this0})
and (ref{eq:lin:thisi}) is known as the Thomas algorithm after
Llewellyn Thomas. 
!bnotice
Clearly tridiagonal matrices can be solved much more efficiently with
the Thomas algorithm than
using a standard library, such as LU-decomposition. This is
because the solution method takes advantages of the *symmetry* of the
problem. We will not show it here, but it can be shown that the Thomas
algorithm is stable whenever $|b_i|\ge |a_i|+|c_i|$. If the algorithm
fails, an advice is first to use the standard `solve` function in
python. If this gives a solution, then *pivoting* combined with the
Thomas algorithm might do the trick. 
!enotice
======= Example: Solving the heat equation using linear algebra =======
===== Exercise: Conservation Equation or the Continuity Equation =====
FIGURE: [fig-lin/heat.png, width=700 frac=.9] Conservation of energy and the continuity equation. label{fig:nlin:heat}

In figure ref{fig:nlin:heat}, the continuity equation is derived for
heat flow.
=== Heat equation for solids ===
As derived in the beginning of this chapter the heat equation for a solid is
!bt
\begin{equation}
\frac{d^2T}{dx^2}+\frac{\dot{\sigma}}{k}=\frac{\rho c_p}{k}\frac{dT}{dt},
label{eq:nlin:heateq}
\end{equation}
!et
where $\dot{\sigma}$ is the rate of heat generation in the solid. This
equation can be used as a starting point for many interesting
models. In this exercise we will investigate the *steady state*
solution, *steady state* is just a fancy way of expressing that we
want the solution that *does not change with time*. This is achieved
by ignoring the derivative with respect to time in equation
(ref{eq:nlin:heateq}). We want to study a system with size $L$, and is
it good practice to introduce a dimensionless variable: $y=x/L$. 
<% counter  = 0 %>
<% counter += 1 %>
__Part ${counter}.__

Show that equation (ref{eq:nlin:heateq}) now takes the following form:
!bt
\begin{equation}
\frac{d^2T }{dy^2}+\frac{\dot{\sigma}L^2}{k}=0
label{eq:nlin:heat2}
\end{equation}
!et



===== Exercise: Curing of Concrete and Matrix Formulation =====
Curing of concrete is one particular example that we can investigate
with equation (ref{eq:nlin:heat2}). When concrete is curing, there are
a lot of chemical reactions happening, these reactions generate
heat. This is a known issue, and if the temperature rises too much 
compared to the surroundings, the concrete may fracture.  In the
following we will, for simplicity, assume that the rate of heat
generated during curing is constant, $\dot{\sigma}=$100 W/m$^3$. The
left end (at $x=0$) is insulated, meaning that there is no flow of
heat over that boundary, hence $dT/dx=0$ at $x=0$. On the right hand
side the temperature is kept constant, $x(L)=y(1)=T_1$, assumed to be
equal to the ambient temperature of $T_1=25^\circ$C.  The concrete
thermal conductivity is assumed to be $k=1.65$ W/m$^\circ$C.

<% counter  = 0 %>
<% counter += 1 %>
__Part ${counter}.__

Show that the solution to equation (ref{eq:nlin:heat2}) in this case is:
!bt
\begin{equation}
T(y)=\frac{\dot{\sigma}L^2}{2k}(1-y^2)+T_1.
label{eq:nlin:heatsol}
\end{equation}
!et

<% counter += 1 %>
__Part ${counter}.__
In order to solve equation (ref{eq:nlin:heat2}) numerically, we need to discretize
it. Show that equation (ref{eq:nlin:heat2}) now takes the following form:

!bt
\begin{equation}
T_{i+1}+T_{i-1}-2T_i=-h^2\beta,
label{eq:nlin:heat3}
\end{equation}
!et
where $\beta=\dot{\sigma}L^2/k$.
FIGURE: [fig-lin/heat_grid.png, width=200 frac=.5] Finite difference
grid for $N=4$. label{fig:nlin:hgrid}

In figure ref{fig:nlin:hgrid}, the finite difference grid is shown for
$N=4$.
<% counter += 1 %>
__Part ${counter}.__

Show that equation (ref{eq:nlin:heat3}) including the boundary conditions for $N=4$ can be written as the following matrix equation
!bt
\begin{align}
\left(
\begin{array}{cccc}
-\gamma&\gamma&0&0\\
1&-2&1&0\\
0&1&-2&1\\
0&0&1&-2\\
\end{array}
\right)
\left(
\begin{array}{c}
T_0\\
T_1\\
T_2\\
T_3\\
\end{array}
\right)
=
\left(
\begin{array}{c}
-h^2\beta\\
-h^2\beta\\
-h^2\beta\\
-h^2\beta-25
\end{array}
\right).
\end{align}
label{eq:lin:heats}
!et
where $\gamma=2$ for the central difference scheme and 1 for the forward difference scheme.

<% counter += 1 %>
__Part ${counter}.__
* Solve the set of equations in equation (ref{eq:lin:heats}) using "`numpy.linalg.solve`":"https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html".
* Write the code so that you can easily switch between the central difference scheme and forward difference
* Evaluate the numerical error as you change $h$, how does it scale? Is it what you expect?

!bc pycod
import numpy as np
import scipy as sc
import scipy.sparse.linalg
from numpy.linalg import solve
import matplotlib.pyplot as plt
!ec

!bc pycod

central_difference=False
# set simulation parameters
h=0.25
L=1.0
n = int(round(L/h))
Tb=25 #rhs
sigma=100
k=1.65 
beta = sigma*L**2/k

y = np.arange(n+1)*h

def analytical(x):
    return beta*(1-x*x)/2+Tb
def tri_diag(a, b, c, k1=-1, k2=0, k3=1):
    """ a,b,c diagonal terms
        default k-values for 4x4 matrix:
        | b0 c0 0  0 |
        | a0 b1 c1 0 |
        | 0  a1 b2 c2|
        | 0  0  a2 b3|
    """
    return np.diag(a, k1) + np.diag(b, k2) + np.diag(c, k3)
# defina a, b and c vector
a=np.ones(n-1)
b=..
c=..

##lhs boundary condition
if central_difference:
    c[0]= ...
else:
    b[0]=...

A=tri_diag(a,b,c)
print(A) # view matrix - compare with N=4 to make sure no bugs
# define rhs vector
d=...
#rhs boundary condition
d[-1]=...

Tn=np.linalg.solve(A,d)
print(Tn)
!ec
The correct solution for $L=1$ m, and $h=1/4$, is: $[T_0,T_1.T_2,T_3]$=[55.3030303 , 53.40909091, 47.72727273, 38.25757576] (central difference) and $[T_0,T_1.T_2,T_3]$=[62.87878788, 59.09090909, 51.51515152, 40.15151515] (forward difference)

===== Exercise: Solve the full heat equation =====
<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__
Replace the time derivative in equation (ref{eq:nlin:heateq}) with
!bt
\begin{equation}
\frac{dT}{dt}\simeq\frac{T(t+\Delta t)-T(t)}{\Delta t}=\frac{T^{n+1}-T^n}{\Delta t}, 
label{eq:lin:dt}
\end{equation}
!et
and show that by using an *implicit formulation* (i.e. that the second derivative with respect to $x$ is to be evaluated at $T(t+\Delta t)\equiv T^{n+1}$) that equation (ref{eq:nlin:heateq}) can be written
!bt
\begin{equation}
T_{i+1}^{n+1}+T_{i-1}^{n+1}-(2+\frac{\alpha h^2}{\Delta t})T_i^{n+1}=-h^2\beta-\frac{\alpha h^2 }{\Delta t}T_i^n,
label{eq:lin:imp} 
\end{equation}
!et
where $\alpha\equiv\rho c_p/k$.
<% counter += 1 %>
__Part ${counter}.__

Use the central difference formulation for the boundary condition and show that for four nodes we can formulate equation (ref{eq:lin:imp}) as the following matrix equation
!bt
\begin{align}
&\left(
\begin{array}{cccc}
-(2+\frac{\alpha h^2}{\Delta t})&2&0&0\\
1&-(2+\frac{\alpha h^2}{\Delta t})&1&0\\
0&1&-(2+\frac{\alpha h^2}{\Delta t})&1\\
0&0&1&-(2+\frac{\alpha h^2}{\Delta t})\\
\end{array}
\right)
\left(
\begin{array}{c}
T_0^{n+1}\\
T_1^{n+1}\\
T_2^{n+1}\\
T_3^{n+1}\\
\end{array}
\right)\no\\
&=
\left(
\begin{array}{c}
-h^2\beta\\
-h^2\beta\\
-h^2\beta\\
-h^2\beta-25
\end{array}
\right)
-\frac{\alpha h^2 }{\Delta t}
\left(
\begin{array}{c}
T_0^n\\
T_1^n\\
T_2^n\\
T_3^n\\
\end{array}
\right)
\end{align}
label{eq:lin:heatfull}
!et

<% counter += 1 %>
__Part ${counter}.__
Assume that the initial temperature in the concrete is $25^\circ$C, $\rho$=2400 kg/m$^3$, a specific heat capacity $c_p=$ 1000 W/kg K, and a time step of $\Delta t=86400$ s (1 day). Solve equation (ref{eq:lin:heatfull}), plot the result each day and compare the result after 50 days with the steady state solution in equation (ref{eq:nlin:heatsol}).

===== Exercise: Using sparse matrices in python  =====
In this part we are going to create a sparse matrix in python and use `scipy.sparse.linalg.spsolve` to solve it. The matrix is created using `scipy.sparse.spdiags`.
<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__
Extend the code you developed in the last exercises to also be able to use sparse matrices, by e.g. a logical switch. Sparse matrices may be defined as follows
!bc pypro
import scipy.sparse.linalg

#right hand side
# rhs vector
d=np.repeat(-h*h*beta,n)
#rhs - constant temperature
Tb=25
d[-1]=d[-1]-Tb
#Set up sparse matrix
diagonals=np.zeros((3,n))
diagonals[0,:]= 1
diagonals[1,:]= -2  
diagonals[2,:]= 1
#No flux boundary condition
diagonals[2,1]= 2
A_sparse = sc.sparse.spdiags(diagonals, [-1,0,1], n, n,format='csc')
# to view matrix - do this and check that it is correct!
print(A_sparse.todense())
# solve matrix
Tb = sc.sparse.linalg.spsolve(A_sparse,d)

# if you like you can use timeit to check the efficiency
# %timeit sc.sparse.linalg.spsolve( ... )
!ec

* Compare the sparse solver with the standard Numpy solver using
  `%timeit`, how large must the linear system be before an improvement
  in speed is seen?



======= CO$_2$ diffusion into aquifers =======
The transport of CO$_2$ into aquifers can be described according to the diffusion equation
!bt
\begin{equation}
\frac{\partial C(z,t)}{\partial t}=\frac{\partial }{\partial z}\left(K(z)\frac{\partial C(z,t)}{\partial z}\right),
label{eq:lin:co2_diff}
\end{equation}
!et
where $C(z,t)$ is the concentration of \co\ as a function of depth ($z$) and time $t$, and $K(z)$ is the diffusion constant of \co\ as a function of depth. This equation can be discretized using standard techniques, to help in that respect consider figure ref{fig:lin:co2_diff}.

FIGURE: [fig-lin/co2_diff.png, width=400 frac=1.0] Discretization for diffusion of \co\ into an aquifer, including boundary conditions. label{fig:lin:co2_diff}

In the following we will assume that there are only four nodes ($i=0\ldots 3$) in the physical domain, and two ghost nodes $i=-1$, and $i=4$. There are many ways to attack this problem, but in the following we will borrow ideas from Finite Volume. Finite volume methods is a way of discretizing equations such that we *conserve mass*. The diffusion equation as it is derived in figure ref{fig:nlin:heat}, express that the flux of something (heat, particles, etc) leaving the box surface minus the flux entering the surface of the box is equal to the rate of change of something inside the box. We can formulate this mathematically as:
!bt
\begin{equation}
\frac{\partial C(z,t)}{\partial t}\simeq\frac{1}{h}\left[\left.K(z)\frac{\partial C(z,t)}{\partial z}\right|_{i+1/2}
-\left.K(z)\frac{\partial C(z,t)}{\partial z}\right|_{i-1/2}\right]
label{eq:lin:co2fv}
\end{equation}
!et
The notation $i\pm1/2$, means that the flux is to be evaluated *at the surface* of the box (i.e. halfway between the red dots in figure ref{fig:lin:co2_diff}). $K(z)$ is the diffusion constant, and it is known everywhere, so this is simple to evaluate at the surface. The concentrations are only known at the center of each box, the red dots in figure ref{fig:lin:co2_diff}. The derivative of the concentration can be evaluated using the central difference formula (remember that the distance between the red dot and edge of the box is $h/2$), hence
!bt
\begin{equation}
\frac{C_i^{n+1}-C_i^n}{\Delta t}=\frac{1}{h}\left[K_{i+1/2}\frac{ C_{i+1}-C_{i}}{h}-K_{i-1/2}\frac{ C_{i}-C_{i-1}}{h}\right],
label{eq:lin:co2fv2}
\end{equation}
!et
notice that we have discretized the time derivative, and that we have introduced $n$ to indicate the time step. On the right hand side there are is no time indicated, it turns out that we have a choice to put time step $n$ or $n+1$ on the concentrations on the right hand side. If we put $n$ the scheme is said to be explicit, if we put $n+1$, the scheme is implicit. Implicit schemes are stable compared to explicit schemes, whereas explicit schemes has slightly higher numerical accuracy [TO DO: show this!]. In general we can write
!bt
\begin{align}
\frac{C_i^{n+1}-C_i^n}{\Delta t}&=\frac{\theta}{h}\left[K_{i+1/2}
\frac{C^n_{i+1}-C^n_{i}}{h}-K_{i-1/2}\frac{C^n_{i}-C^n_{i-1}}{h}\right]\\
&+\frac{1-\theta}{h}\left[K_{i+1/2}
\frac{C_{i+1}^{n+1}-C_{i}^{n+1}}{h}-K_{i-1/2}\frac{ C_{i}^{n+1}-C_{i-1}^{n+1}}{h}\right],
label{eq:lin:co2fv23}
\end{align}
!et
hence if $\theta=1$ the scheme is explicit, if $\theta=0$ the scheme is implicit, and if $\theta=1/2$, the scheme is called the Crank-Nicolson method. The first and last boundary are special, let us first consider the $i=0$, this is where the sea is in contact with the \co\ in the atmosphere, and the flux is $k_w(C_0-C_{eq})$, hence
!bt
\begin{align}
\frac{C_0^{n+1}-C_0^n}{\Delta t}&=\frac{\theta}{h}\left[K_{i+1/2}
\frac{C^n_{1}-C^n_{0}}{h}-k_w(C_0^n-C_{eq}^n)\right]\\
&+\frac{1-\theta}{h}\left[K_{i+1/2}
\frac{C_{1}^{n+1}-C_{0}^{n+1}}{h}-k_w(C_0^{n+1}-C_{eq}^{n+1})\right].
label{eq:lin:co2fv23b}
\end{align}
!et
For the last block the flux is zero towards the seafloor, and equation (ref{eq:lin:co2fv23}) can be written
!bt
\begin{align}
\frac{C_3^{n+1}-C_3^n}{\Delta t} & =\frac{\theta}{h}\left[-K_{5/2}\frac{C^n_{3}-C^n_{2}}{h}\right]\\
&+\frac{1-\theta}{h}\left[-K_{5/2}\frac{ C_{3}^{n+1}-C_{2}^{n+1}}{h}\right].
label{eq:lin:co2fv23c}
\end{align}
!et
For the blocks $i=1\ldots2$, we can collect all terms with $n+1$ on one side and terms with $n$ on the other side and rewrite equation (ref{eq:lin:co2fv23})
!bt
\begin{align}
\left[1+(1-\theta)\right. &\left.\alpha(K_{i+1/2}+K_{i-1/2})\right]C_i^{n+1}\no\\
&-(1-\theta)\alpha K_{i+1/2}C_{i+1}^{n+1}
-(1-\theta)\alpha K_{i-1/2}C_{i-1}^{n+1} \no\\
=\left[1-\theta\right. &\left.\alpha(K_{i+1/2}+K_{i-1/2})\right]C_i^{n}\no\\
&+\theta\alpha K_{i+1/2}C_{i+1}^{n}
+\theta\alpha K_{i-1/2}C_{i-1}^{n}, 
label{eq:lin:co2fv4}
\end{align}
!et
where $\alpha\equiv\Delta t/h^2$.
Next, we want to write down the corresponding matrix equations for four grid nodes as indicated in figure ref{fig:lin:co2_diff}. Notice that we need to use the equations in figure ref{fig:lin:co2_diff}, for $C_{-1}$, and $C_4$. The left and right hand coefficient matrix $\mathbf{L}$, and $\mathbf{R}$ are given as
!bt
{\tiny
\begin{align}
\begin{pmatrix}
1+(1-\theta)\alpha(K_{1/2}+hk_w)&-(1-\theta)\alpha K_{1/2}&0&0\\
-(1-\theta)\alpha K_{1/2}&1+(1-\theta)\alpha(K_{3/2}+K_{1/2})&-(1-\theta)\alpha K_{3/2} &0\\
0&-(1-\theta)\alpha K_{3/2}&1+(1-\theta)\alpha(K_{5/2}+K_{3/2})&-(1-\theta)\alpha K_{5/2} \\
0&0&-(1-\theta)\alpha K_{5/2}&1+(1-\theta)\alpha K_{5/2}\no
\end{pmatrix},
\end{align}
}
!et
!bt
{\tiny
\begin{align}
\begin{pmatrix}
1-\theta\alpha(K_{1/2}+hk_w)&+\theta\alpha K_{1/2}&0&0\\
\theta\alpha K_{1/2}&1-\theta\alpha(K_{3/2}+K_{1/2})&\theta\alpha K_{3/2} &0\\
0&\theta\alpha K_{3/2}&1-\theta\alpha(K_{5/2}+K_{3/2})&\theta\alpha K_{5/2} \\
0&0&\theta\alpha K_{5/2}&1-\theta\alpha K_{5/2}\no
\end{pmatrix},
\end{align}
}
!et
respectively. Introducing $\mathbf{S}=\left[k_wC_{eq}\Delta t/h,0,0,0\right]^T$, we can finally write the diffusion equation (ref{eq:lin:co2_diff}) as
!bt
\begin{equation}
\mathbf{L}\mathbf{C}^{n+1}=\mathbf{R}\mathbf{C}^n+\theta\mathbf{S}^n+(1-\theta)\mathbf{S}^{n+1}
label{eq:lin:discdif}
\end{equation}
!et

More stuff to do:
o Assume zero flux over the air water interface ($k_w$=0), show from the equations above that if we start with a uniform concentration in the sea ($\mathbf{C}^n$=constant) that $\mathbf{C}^{n+1}$ does not change (as it should).
o Assume that if the concentration at a specific time $n$ in the sea is equal to $\mathbf{C}_{eq}$ then the concentration stays constant at all later times
o Add chemical reactions
###======= Singular Value Decomposition =======

###======= QR factorization =======

###======= Solving nonlinear equations =======
###The purpose of this section is to introduce a handful of techniques for solving a nonlinear equation. In many cases a combination of methods must be used, and the algorithm must be adopted to your specific problem. 

!split
========= Optimization and nonlinear systems =========
label{ch:nlin}
# Contrary to linear equations, you will most likely find that the functions available in
# various Python library will *not* cover your needs and in many cases fail to give you
# the correct solution. The reason for this is that the solution of a nonlinear equation # is greatly
# dependent on the starting point, and a combination of various techniques  must be used.  


In this chapter we will cover some theory related to the solution of nonlinear equations, and introduce the most used methods. A nonlinear problem is represented as a single equation or a system of equations, where the response is not changing proportionally to the input.  Almost all physical systems are nonlinear, and one frequent use of the methods presented in this chapter is to determine model parameters by matching a nonlinear model to data. 

Numerical methods that is guaranteed to find a solution (if it exists) are called *closed methods*, and *open* other vise. In many cases the closed methods requires more iterations for well behaved functions than the open methods. For one dimensional problems we will cover: fixed point iteration, bisection, Newton's method, and the secant method.
For  multidimensional problems we will cover Newton-Rapson method, which is a direct extension of Newton's method in one
dimension, and the steepest decent. The main challenge is that there are (usually) more than one solution, the solution that
*you* want for a specific problem is usually dictated by the underlying physics. If computational speed is not an issue, the
 method of choice is usually the bisection method. It is guaranteed to give an answer, but it might be slow. If speed is an issue, usually Newton's or the secant method will be the fastest (but it depends on the starting point). The secant method is sometimes preferred if the derivative of the function is costly to evaluate. Brents method is a method that combine the secant and bisection method (not covered), and is guaranteed to find a solution if the root is bracketed. 

In many practical, engineering, applications one usually implements some of the methods described below directly inside functions. This is because it is usually faster than calling a separate all purpose nonlinear solver, and that one usually has a very good idea of what a good starting point for the nonlinear solver is. 

======= Nonlinear equations  =======
A nonlinear equation is simply an equation that is not linear. That means that when the variables changes the response is not changing proportional to the values of the variables. Solving a nonlinear equation always proceeds by *iterations*, we start with one or several initial guesses and then search for the solution. In many cases we do not know beforehand if the equation actually has a solution, or multiple solutions. An example of a nonlinear problem is:
!bt
\begin{equation}
e^{-x}=x^2.
label{eq:nlin:exp}
\end{equation}
!et
Traditionally one collect all the terms on one side, to solve an equation of the form
!bt
\begin{equation}
f(x)=x^2-e^{-x}=0.
label{eq:nlin:fx}
\end{equation}
!et
In figure ref{fig:nlin:fx}, the solution is shown graphically. Note that in one case the solution is when the graph of $e^{-x}$, and $x^2$ intersect, whereas in the other case the root is located when $x^2-e^{-x}$ intersect the $x-$axis. 

FIGURE: [fig-nlin/fx.png, width=400 frac=.5] Notice that the root is located at the same place ($x=0.703467417$) label{fig:nlin:fx}

In the case of more than one unknown, or a set of equations that must be satisfied simultaneously, equation (ref{eq:nlin:fx}) is replaced with a vector equation
!bt
\begin{equation}
\mathbf{f}(\mathbf{x})=\mathbf{0}.
label{eq:nlin:fvec}
\end{equation}
!et
Although this equation looks quite similar to equation (ref{eq:nlin:fx}), this equation is *much* harder to solve. The only methods we will cover is the Newton Rapson method, which is a very good method if a good starting point is given. If you have a multidimensional problem, the advice is to try Newton-Raphson, if this method fails you need to try more advanced method, see e.g. cite{press2001}.

======= Example: van der Waals equation of state =======
Before we begin with the numerical algorithms, let us consider an example: the van der Waals equation of state. The purpose is to illustrate some of the typical challenges. You are probably familiar with the ideal gas law:
!bt
\begin{equation}
P\nu=R_gT,
label{eq:nlin:pvt}
\end{equation}
!et
where $\nu=V/n$ is the molar volume of the gas, $P$ is the pressure, $V$ is the volume, $T$ is the temperature, $n$ is the number of moles of the gas, and $R_g$ is the ideal gas constant.  This equation is an example of an *equation of state* (EOS), it relates $P$, $T$, and $\nu$. Thus if we know the pressure and temperature of the gas, we can calculate $\nu$. Equation (ref{eq:nlin:pvt}) assumes that there are no interactions between the molecules in the gas. Clearly, this is too simplistic, and because of this one normally uses an EOS that better reflect the physical properties of the substance. A very famous EOS is the van der Waal EOS, which is a slight modification of equation (ref{eq:nlin:pvt}):
!bt
\begin{equation}
\left(P+\frac{a}{\nu^2}\right)\left(\nu-b\right)=R_gT.
label{eq:nlin:vdw}
\end{equation}
!et
$a$ and $b$ are material constants that needs to be determined experimentally. This equation is *not* used in industrial design, but most equations used in practice are based on equation (ref{eq:nlin:vdw}). Multiplying equation (ref{eq:nlin:vdw}) with $\nu^2$, we get a non linear equation that is cubic in the molar volume. It turns out that cubic EOS are a class of equations that are quite successful in modeling the behavior of real systems cite{peng1976new}. However equation (ref{eq:nlin:vdw}) is a good starting point for more complex and realistic equations.

It is common practice to rescale EOS with respect to the critical point. At the critical point we have [ref]:
!bt
\begin{align}
\left.\frac{\partial P}{\partial \nu}\right|_{T_c,P_c} &=0
label{eq:nlin:crit1} \\
\left.\frac{\partial^2 P}{\partial \nu^2}\right|_{T_c,P_c} &=0
label{eq:nlin:crit2} 
\end{align}
!et
From equation (ref{eq:nlin:crit1}),  (ref{eq:nlin:crit2}), and (ref{eq:nlin:vdw}), it follows:
!bt
\begin{equation}
\nu_c=3b\quad,P_c=\frac{a}{27b^2}\quad,R_gT_c=\frac{8a}{27b^2}.
label{eq:nlin:crit3}
\end{equation}
!et
Inserting these equations into equation (ref{eq:nlin:vdw}), and defining the *reduced* quantities $\hat{P}=P/P_c$, $\hat{T}=T/T_c$, $\hat{\nu}=\nu/\nu_c$, we get
!bt
\begin{equation}
\left(\hat{P}+\frac{3}{\hat{\nu}^2}\right)\left(3\hat{\nu}-1\right)=8\hat{T}.
label{eq:nlin:vdwr}
\end{equation}
!et

FIGURE: [fig-nlin/vdw.png, width=400 frac=1.0] van der Waal isotherms. label{fig:nlin:vdw}

In figure ref{fig:nlin:vdw}, we have plotted the isotherms. Note that if $\hat{T}<1$ ($T<T_c$), there might be more than one solution for the molar volume. This is clearly unphysical and additional constraints are needed. For the curve $\hat{T}=0.9$, the dashed lined shows that for $\hat{P}=0.7$, there are three solutions. This is a typical behavior of the cubic EOS, and physically it corresponds to the saturated case, where the vapor and liquid phase co-exist. The left root is the liquid state and the right root is the vapor state. The root in the middle represents a meta stable state.

!bnotice It never hurts to look at your function
The example in figure ref{fig:nlin:vdw} illustrates some important points. Solving a nonlinear problem might be very easy in part of the parameter space (e.g. when $T>T_c$ there are only one solution), but extremely hard in other part of the parameter space (e.g. when $T<T_c$, where there are multiple solutions). However, much of the trick to find a solution is to choose a good starting point. When there are multiple solutions we need to start close to the physical solution. 
!enotice
===== Exercise: van der Waal EOS and CO$_2$ =====

Use equation (ref{eq:nlin:vdw}), and the parameters for CO$_2$: a=3.640 L$^2$bar/mol, and b=0.04267 L/mol, to test the van der Waal EOS in equation (ref{eq:nlin:vdw}). Use that at 2 MPa and 100 $^\circ$C, CO$_2$ has a specific volume of 0.033586 m$^3$/kg.  
!bsol
The calculation is straight forward, but it is easy to get an error due to units. We will use SI units: a=0.3640 m$^6$Pa/mol, b=4.267$\cdot10^{-5}$ m$^3$/mol, $R$=8.314J/mol K.  The molar volume is obtained by multiplying by the molar weight of CO$_2$: $M_w$ = 44 g/mol, hence $\nu=1.478\cdot10^{-3}$m$^3$/mol. Using $P=RT/(\nu-b)-a/\nu^2=1.993$ MPa, or an error of $0.3\%$.
!esol

======= Fixed-point iteration =======
idx{fixed-point iteration}
A simple (but not always possible) way of solving a nonlinear equation is to reformulate the problem $f(x)=0$ to a problem of the form
!bt
\begin{equation}
x=g(x).
label{eq:nlin:g}
\end{equation}
!et
The algorithm for solving this equation is to guess at a starting point, $x_0$, evaluate $x_1=g(x_0)$, $x_2=g(x_1)$, and so on. In some circumstances we might end up at a stable point, where $x$ does not change. This point is termed a *fixed point*.

Note that the form of $g(x)$ is not uniquely determined. For our function defined in equation (ref{eq:nlin:exp}), we can solve for $x$ directly
!bt
\begin{equation}
x=e^{-x/2},
label{eq:nlin:g2}
\end{equation}
!et
or we could write:
!bt
\begin{equation}
x=x-x^2+e^{-x}.
label{eq:nlin:g3}
\end{equation}
!et
These functions are illustrated in figure ref{fig:nlin:fg}, by visual inspection they look very similar, but as we will show in the next exercise the convergence is quite different. 

FIGURE: [fig-nlin/f_g_comb.png, width=400 frac=1] Two examples of iterative functions, that will give the same solution. label{fig:nlin:fg}

===== Exercise: Implement the fixed point iteration =====
Write a Python function that utilizes the fixed point algorithm in the previous section, find the root of $f(x)=x^2-e^{-x}$. In one case use $g(x)=e^{-x/2}$, and in the other case use $g(x)=x-x^2+e^{-x}$. How many iterations does it take in each case?

!bsol
Below is a straight forward (vanilla) implementation:
@@@CODE src-nlin/iterative_simple.py fromto:def iterative@#end
If we start at $x=0$, it will take 174 iterations using $x-x^2+e^{-x}$ ($g(x)$) and only 19 for $e^{-x/2}$ ($h(x)$), the root is $x$=0.70346742. 
!esol

===== Exercise: Finding the molar volume from the van der Waal EOS by fixed point iteration =====
Extend the code above to take as argument the van der Waal EOS. For simplicity we will use the rescaled EOS in equation (ref{eq:nlin:vdwr}). Show that for the reduced temperature, $\hat{T}$=1.2, and pressure, $\hat{P}$=1.5, the reduced molar volume $\hat{nu}$ is 1.3522091.

!bsol
First we rewrite equation (ref{eq:nlin:vdwr}) in a more useful form
!bt
\begin{equation}
\hat{\nu}=\frac{1}{3}(1+\frac{8\hat{T}}{\hat{P}+3/\hat{\nu}^2})
label{eq:nlin:sp}
\end{equation}
!et
The right hand side will play the same role as $g(x)$ above, where $x$ now is the reduced molar volume, and can be implemented in Python as:
@@@CODE src-nlin/vdw_simple.py fromto: def dvdwEOS@def dCO2EOS
Note that this function requires the values of $\hat{P}$ and $\hat{T}$, in addition to $\hat{\nu}$ to return a value. Thus in order to use the fixed point iteration method implemented above, we need to pass arguments to our function. This can easily be achieved by taking advantage of Pythons `*args` functionality. By simply rewriting our implementation slightly:
@@@CODE src-nlin/vdw_simple.py fromto: iterative@#end
We can find the root by calling the function as:
!bc pypro
iterative(1,dvdwEOS,1.2,1.5)
!ec
The program returns the correct solution after 71 iterations.
!esol

===== When does the fixed point method fail? =====
label{sec:nlin:fp}
If we replace $e^{-x}$ with $e^{1-x^2}$ in equation (ref{eq:nlin:g3}), our method will not give a solution. You can easily verify that the $x=1$ is a solution, so why does our method fail? To investigate this in a bit more detail, we turn to Taylors formula (once again). Assume that the root is located at $x^*$, and our guess is $x_k$, then the next $x$-value will be
!bt
\begin{equation}
x_{k+1}=g(x_0)=g(x^*)+g^\prime(x^*)(x_k-x^*)+\cdots
label{eq:nlin:t1}
\end{equation}
!et
The true solution is $x^*$, hence $x^*=f(x^*)$, and we can write
!bt
\begin{equation}
x_{k+1}-x^*=g^\prime(x^*)(x_k-x^*),
label{eq:nlin:t2}
\end{equation}
!et
where we have neglected higher order terms. The point is: at each iteration we want the distance $x_1-x^*$ to decrease, i.e. to be smaller than $x_0-x^*$. This can only be achieved if
!bt
\begin{equation}
|g^\prime(x^*)|<1. 
label{eq:nlin:fpi}
\end{equation}
!et
In our example above we saw that if $g(x)=x-x^2+e^{-x}$, we used 172 iterations and only 19 iterations if we replaced $g(x)$ with $h(x)=e^{-x/2}$ to converge to the *same* root $x$=0.70346742. We can now understand this, because $g^\prime(x)=1-2x-e^{-x}$ and $g(x^*)\simeq-0.90$, whereas $h^\prime(x)=-e^{-x/2}/2$, and $h^\prime(x^*)\simeq0.35$. We expect the number of iterations, $n$, needed to reach a certain precision, $\varepsilon$, to scale as
!bt
\begin{equation}
|g^\prime(x^*)|^n=\varepsilon.
label{eq:nlin:scale}
\end{equation}
!et
We expect to use $\log|h^\prime(x^*)|/\log|g^\prime(x^*)|\simeq10$ more iterations using $g(x)$ compared to $h(x)$, which is close to the observed value of 172/19$\simeq 9$.
===== What to do when the fixed point method fails  =====
As discussed in cite{newman2013}, there might be an elegant solution whenever $|g^\prime(x^*)|>1$. If it is possible to invert the $g(x)$, we can show that the derivative of the inverse function
$ { g^\prime }^{-1} (x^*)  = 1/g^\prime (x^*) $. Why is this useful? Because if $x^*=g(x^*)$ is the solution we are searching for, then this is equivalent to $x^*={g}^{-1}(x^*)$ *if and only if* we can invert $g(x)$. Note that in many cases it is not possible to invert $g(x)$. Let us first show that $ { g^\prime }^{-1} (x^*)  = 1/g^\prime (x^*) $. For simplicity write
!bt
\begin{equation}
y = g(x)\Leftarrow x=g^{-1}(y),
\end{equation}
!et
taking the derivative with respect to x gives
!bt
\begin{align}
\frac{d}{dx}g^{-1}(y)&=\frac{dx}{dx}=1,label{eq:nlin:fpi1}\\
\frac{dg^{-1}(y)}{dy}\frac{dy}{dx}&=\frac{dx}{dx}=1,label{eq:nlin:fpi2}\\
\frac{dg^{-1}(y)}{dy}&=\frac{1}{\frac{dy}{dx}}=\frac{1}{g^{\prime}(x)}
=\frac{1}{g^{\prime}(g^{-1}(y))}.label{eq:nlin:fpi3}
\end{align}
!et
Going from equation (ref{eq:nlin:fpi1}) to (ref{eq:nlin:fpi2}), we have used the chain rule. Equation (ref{eq:nlin:fpi3}) is general, let us now specify to our fixed point iteration. Then we can use $x^*=g(x^*)=y^*$, and $x^*=g^{-1}(y^*)=g^{-1}(x^*)$ hence we can write the last equation as
!bt
\begin{equation}
\frac{d}{dx}g^{-1}(x^*)=\frac{1}{g^{\prime}(x^*)}.
label{eq:nlin:fpif}
\end{equation}
!et
===== Exercise: Solve $x=e^{1-x^2}$ using fixed point iteration =====
The solution to $x=e^{1-x^2}$ is clearly $x=1$.

* First try the fixed point method using $g(x)=e^{1-x^2}$ to find the root $x=1$. Try to start very close to the true solution $x=1$. What is the value of $g^\prime(x^*)$?
* Next, invert $g(x)$, what is the derivative of $g^{-1}(x^*)$? Try the fixed point method using $g^{-1}(x^*)$

!bsol
First, we calculate the derivative of $g(x)$, $g^\prime(x)=-2xe^{1-x^2}$, hence $g^\prime(x^*)=-2$ and $|g^\prime(x^*)|>1$. This is an unstable fixed point, and if we start a little bit off from this point we will spiral away from it.

Inverting $y=g(x)$ gives us $ g^{-1} (y)=\sqrt{1-\ln y}$. Note that $y^*=x^*=1$ is a solution to this equation as it should be. The derivative is
!bt
\begin{equation}
{g^{-1}}^\prime(y)=-\frac{1}{2\sqrt{1-\ln y}},
\end{equation}
!et
and $ {g^{-1}}^\prime(y^*)=-1/2 $.
It takes about 30 iterations to reach the correct solution $y^*=1$, when the starting point is $y=0$. 
!esol


======= Rate of convergence =======
idx{rate of convergence}
The rate of convergence is the speed at which a *convergent* sequence approach the limit. Assume that our sequence $x_{k}$ converges to the number $x^*$, the sequence is said to *converge linearly* to $x^*$ if there exists a number $\mu\in<0,1>$, such that
!bt
\begin{equation}
\lim_{k\to\infty}=\frac{|x_{k+1}-x^*|}{|x_k-x^*|}=\mu
label{eq:nlin:linconv}
\end{equation}
!et
Inserting equation (ref{eq:nlin:t2}) in equation (ref{eq:nlin:linconv}), we get:
!bt
\begin{equation}
\lim_{k\to\infty}=\frac{|x_{k+1}-x_k|}{x_k-x^*}
=\frac{|g^\prime(x^*)(x_k-x^*)|}{|x_k-x^*|}=|g^\prime(x^*)|.
label{eq:nlin:ling}
\end{equation}
!et
Hence the fixed point iteration is expected to converge *linearly* to the correct solution. The definition in equation (ref{eq:nlin:linconv}), can be extended to include the definition of quadratic, cubic, etc. convergence:
!bt
\begin{equation}
\lim_{k\to\infty}=\frac{|x_{k+1}-x^*|}{|x_k-x^*|^q}=\mu.
label{eq:nlin:qconv}
\end{equation}
!et
If $q=2$ the convergence is said to be quadratic and so on.

======= The bisection method =======
idx{bisection method}
The idea behind bisection is that the root is bracketed, i.e. that there exists two points $a$ and $b$, such that $f(a)\cdot f(b)<0$. In practice it might be a challenge to find these two points. However, if you know that the function has a only root between two values, and that speed is not a big issue this method guarantees that the root will be found within a finite number of steps. The basic idea behind the method is to divide the interval into two (i.e. bisecting the interval). The method only works if the function is continuous on the interval. 

FIGURE: [fig-nlin/bisection.png, width=400 frac=1] Illustration of the bisection method for the van der Waal EOS. label{fig:nlin:bisection}


The algorithm is as follows:
* Test if $f(a)\cdot f(b)<0$, if not return an error message
* Calculate the midpoint $c=(a+b)/2$. If $f(a)\cdot f(c)<0$ the root is in the interval $[a,c]$, else the root is in the interval $[c,b]$
* Half the interval, and test in which interval the root lies, and continue until a convergence criterion.

In figure ref{fig:nlin:bisection}, there is a graphical illustration.
Below is an implementation of the bisection method.
@@@CODE src-nlin/bisection.py fromto: def bisection@#end

!bnotice Warnings 
Note that the implementation of the bisection algorithm is only a few lines of code, and most of the code is to give warnings to the user. In this case it is important to do additional checking, and give the user warnings. If $f(c)$=0, then we must stop and return the exact solution. If we only test if $f(a)\cdot f(c)$ is greater or lower than zero the algorithm would fail. 
!enotice

===== Rate of convergence =====
idx{rate of convergence}
If $c_n$ is the midpoint after $n$ steps, the difference between the solution $x^*$ and $c_n$ is
!bt
\begin{equation}
|c_n-x^*| \le \frac{|b-a|}{2^n}
label{eq:nlin:bisec}
\end{equation}
!et
Using our previous definition in equation (ref{eq:nlin:qconv}), we find that
!bt
\begin{equation}
\lim_{k\to\infty}=\frac{|c_{k+1}-x^*|}{|c_k-x^*|}\le\frac{|b-a|/2^{n+1}}{|b-a|/2^n}=\frac{1}{2},
label{eq:nlin:bsc1}
\end{equation}
!et
hence the bisection method converges linearly.
======= Newton's method =======
idx{Newtons method}
Newtons method is one of the most used methods. If it converges, it converges quadratically to the correct solution. The drawback is that contrary to the bisection method it may fail if a bad starting point is given. Newtons method for finding the root of a function $f(x)=0$ is illustrated in figure ref{fig:nlin:newton}. The main idea is to use more information about the function in the search of the root. In this case we want to find the point where the tangent of the function in $x_k$ intersect the $x-$axis, and take that as our next point, $x_{k+1}$. 

FIGURE: [fig-nlin/newton_comb.png, width=400 frac=1.0] Illustration of Newtons method for the van der Waals EOS.label{fig:nlin:newton}

We can easily derive the algorithm by finding the formula for the tangent line. Using $y=ax+b$ for the tangent line, we immediately know that $a=f^\prime(x_k)$. $b$ can be found as we know that the line intersects $(x_k,f(x_k))$: $f(x_k)=f^\prime(x_k)x_k+b$, hence the equation for the tangent line is $y=f^\prime(x_k)x+f(x_k)-f^\prime(x_k)x_k$. The next point is located where $y$ crosses the $x$-axis, hence $0=f^\prime(x_k)x_{k+1}+f(x_k)-f^\prime(x_k)x_k$. Rearranging this equation, we can write Newtons method in the standard form
!bt
\begin{equation}
x_{k+1}=x_k-\frac{f(x_k)}{f^\prime(x_k)}.
label{eq:nlin:newton}
\end{equation}
!et
Note that the derivative of $f(x)$ enters in equation (ref{eq:nlin:newton}), which means that if our function has a extremal value in our search domain, Newtons method most likely will fail. In particular $x_1$, and $x_4$ in the figure to the right in figure ref{fig:nlin:newton2} are bad starting point for Newtons method.

FIGURE: [fig-nlin/newton2.png, width=400 frac=1.0]  Illustration of some of the possible challenges with Newtons method. Note that if the derivative is zero somewhere in the search interval, Newtons method will fail. label{fig:nlin:newton2}

An implementation is shown below.
@@@CODE src-nlin/newton.py fromto: def newton@#end

Comparing figure ref{fig:nlin:bisection} and ref{fig:nlin:newton}, you immediately get the sense that Newtons method converges faster, and indeed it does. 

===== Rate of convergence =====
idx{Newtons method, rate of convergence}
 Newtons method is similar to the fixed point method, but where we do not use $g(x)=x-f(x)$, but $g(x)=x-\frac{f(x)}{f^\prime(x)}$. We will now analyze Newtons method, using the same approach as in section ref{sec:nlin:fp}. First we expand $g(x)$ around the root $x^*$
!bt
\begin{equation}
x_{k+1}=g(x_k)=g(x^*)+g^\prime(x^*)(x_k-x^*)+\frac{1}{2}g^{\prime\prime}(x^*)(x_k-x^*)^2,
label{eq:nlin:nsec}
\end{equation}
!et
where we have skipped all higher order terms. You can easily verify that
!bt
\begin{align}
g^\prime(x) &=\frac{f^{\prime\prime}(x)f(x)}{f^\prime(x)^2}
label{eq:nlin:gn2} \\
g^{\prime\prime}(x) &=\frac{(f^{\prime\prime\prime}(x)f^\prime(x)-2f^{\prime\prime}(x)^2f^\prime(x))f(x)
+f^{\prime\prime}(x)f^\prime(x)^2}{f^\prime(x)^4}.
label{eq:nlin:gn3}
\end{align}
!et
$x^*$ is a solution, hence $f(x^*)=0$, we then find from equation (ref{eq:nlin:gn2}) and (ref{eq:nlin:gn3}) that $g^\prime(x^*)=0$, and $g^{\prime\prime}(x^*)=f^{\prime\prime}(x^*)/f^{\prime}(x^*)^2$. Thus from equation (ref{eq:nlin:nsec}) we get
!bt
\begin{equation}
x_{k+1}=x^*+\frac{1}{2}\frac{f^{\prime\prime}(x^*)}{f^{\prime}(x^*)^2}(x_k-x^*)^2,
label{eq:nlin:nsecn}
\end{equation}
!et
or equivalently:
!bt
\begin{equation}
\frac{x_{k+1}-x^*}{(x-x^*)^2}=\frac{1}{2}\frac{f^{\prime\prime}(x^*)}{f^{\prime}(x^*)^2}.
label{eq:nlin:nsecn2}
\end{equation}
!et
The denominator has a power of two, and hence Newtons method is *quadratic* convergent (assuming that the sequence $x_{k+1}$ is a convergent sequence). Note that it also follows from the analyses above that Newtons method will fail if the derivative at the root, $f^\prime(x^*)$, is zero.
===== Exercise: Compare Newtons, Bisection and the Fixed Point method =====
Find the root of $f(x)=x^2-e^{-x}$ using bisection, fixed point,  and Newtons method, start at $x=0$. How many iterations do you need to use reach a precision of $10^{-8}$? What happens if you widen the search domain or start further away from the root?
!bsol
The root is located at $x^*=0.70346742$.
* Fixed point method: we saw earlier that using $g(x)=x-f(x)$ used 174 iterations, and $g(x)=\sqrt{x^2-f(x)}$ used 19 iterations. If we start at $x=-100$, $g(x)=x-f(x)$ fails, and  $g(x)=\sqrt{x^2-f(x)}$ uses only 21 iterations, and at $x=100$ we use 20 iterations.
* Bisection method: it use 25 iterations for $a=0$, and $b=1$ (implementation shown earlier in the chapter). Choosing $a=-b=-100$ we use 33 iterations.
* Newtons method: it use only 5 function evaluations (implementation above) starting at  $x=0$. Starting at $x=-100$, it uses 106 iterations. Newtons method is slow in this case because the function is very steep around the starting point, see figure ref{fig:nlin:newton_bad}. Starting at $x=100$, we only use 10 iterations.

FIGURE: [fig-nlin/newton_bad.png, width=400 frac=1.0] Newtons method performs poorly far away due to the shape of the function close to $x=-100$, bisection performs much better while the fixed point method fails. label{fig:nlin:newton_bad}

!bnotice A good starting point is crucial
Note that it is not given which method is best, but if we are ''close'' to the root Newtons method is usually superior. If we are far away, other methods might work better. In many cases one uses a more stable method far away from the root, and then ''polish up'' the root by a couple of Newton iterations cite{press2001}. See also Brents method which combines bisection and linear interpolation (secant method) cite{press2001}.  
!enotice
!esol

======= Secant method =======
idx{secant method}
The Newtons method is very good if you can choose a good starting point, and you can give in an analytical formula for the derivative. In some cases it is not possible to calculate the derivative analytically, then a very good method of choice is the secant method. It can be derived by simply replacing the derivative in Newtons method by the finite difference approximation
!bt
\begin{equation}
f^\prime(x_k)\to \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}.
label{eq:nlin:sec1}
\end{equation}
!et
Inserting this equation into equation (ref{eq:nlin:newton}), we get
!bt
\begin{align}
x_{k+1}&=x_k-f(x_k)\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}\no\\
       &=\frac{x_{k-1}f(x_k)-x_kf(x_{k-1})}{f(x_k)-f(x_{k-1})}. label{eq:nlin:sec2}
\end{align}
!et
For a graphical illustration see figure ref{fig:nlin:secant}
FIGURE: [fig-nlin/secant.png, width=400 frac=1.0] A graphical illustration of the secant method. Note that the starting points $x_0$ and $x_1$ do not need to be close. The next point is where the (secant) line crosses the $x$-axis. label{fig:nlin:secant}

===== Rate of convergence =====
idx{secant method, rate of convergence}
The derivation of the rate of convergence for the secant method is a bit more involved. To simplify the notation we introduce the notation $\varepsilon_k\equiv x_k-x^*$, where $x^*$ is the exact solution. Subtracting $x^*$ from each side of equation (ref{eq:nlin:sec2}) we get
!bt
\begin{align}
\varepsilon_{k+1}&=x_{k+1}-x^*=\frac{x_{k-1}f(x_k)-x_kf(x_{k-1})}{f(x_k)-f(x_{k-1})}-x^*, \no\\
\varepsilon_{k+1}&=\frac{\varepsilon_{k-1}f(x_k)-\varepsilon_k f(x_{k-1})}{f(x_k)-f(x_{k-1})},
label{eq:nlin:sec3}
\end{align}
!et
we now make a Taylor expansion of $f(x_k)$ and $f(x_{k-1})$ about the root $x^*$
!bt
\begin{align}
f(x_k) &=f(x^*)+f^\prime(x^*)(x_k-x^*)+\frac{1}{2}f^{\prime\prime}(x^*)(x_k-x^*)^2+\cdots ,\no\\
       &=f^\prime(x^*)\varepsilon_k+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_k^2+\cdots .\\
f(x_{k-1}) &=f(x^*)+f^\prime(x^*)(x_{k-1}-x^*)+\frac{1}{2}f^{\prime\prime}(x^*)(x_{k-1}-x^*)^2+\cdots,\no\\
       &=f^\prime(x^*)\varepsilon_{k-1}+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_{k-1}^2+\cdots ,
\end{align}
!et
where we have used the fact that $f(x^*)=0$. Inserting these equations into equation (ref{eq:nlin:sec3}) and neglecting terms of order $\varepsilon_k^3$ we get
!bt
\begin{align}
\varepsilon_{k+1}&=\frac{\varepsilon_{k-1}\left[f^\prime(x^*)\varepsilon_k+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_k^2\right] -\varepsilon_k\left[ f^\prime(x^*)\varepsilon_{k-1}+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_{k-1}^2\right]}{f^\prime(x^*)\varepsilon_k+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_k^2-\left[ f^\prime(x^*)\varepsilon_{k-1}+\frac{1}{2}f^{\prime\prime}(x^*)\varepsilon_{k-1}^2\right]},\no\\
&=\frac{\varepsilon_k\varepsilon_{k-1}\left[\varepsilon_k-\varepsilon_{k-1}\right]}{\left[f^\prime(x^*)+\frac{1}{2}f^{\prime\prime}(x^*)(\varepsilon_k+\varepsilon_{k-1})\right](\varepsilon_k-\varepsilon_{k-1})},\no\\
&=\frac{f^{\prime\prime}(x^*)}{2f^\prime(x^*)}\varepsilon_k\varepsilon_{k-1},label{eq:nlin:sec4}
\end{align}
!et
where we have neglected higher powers of $\varepsilon$. We are searching for a solution of the form $\varepsilon_{k+1}=K\varepsilon_k^q$, $q$ is the rate of convergence. We can invert this equation to get $\varepsilon_k=K^{-1/q}\varepsilon_{k+1}^{1/q}$, or alternatively $\varepsilon_{k-1}=K^{-1/q}\varepsilon_{k}^{1/q}$ (just set $k\to k-1$). Inserting these equations into equation (ref{eq:nlin:sec4})
!bt
\begin{equation}
\varepsilon_k^q=\frac{f^{\prime\prime}(x^*)}{2f^\prime(x^*)}\varepsilon_kK^{-1/q}\varepsilon_{k}^{1/q}.
label{eq:nlin:sec5}
\end{equation}
!et
Clearly, if this equation is to have a solution we must have
!bt
\begin{align}
\frac{f^{\prime\prime}(x^*)}{2f^\prime(x^*)}K^{-1/q} &=1\no\\
\varepsilon_k^q=\varepsilon_k\varepsilon_{k}^{1/q}=\varepsilon_{k}^{1+1/q},
\end{align}
!et
or $q=1+1/q$. Solving this equation we get $q=(1\pm\sqrt{5})/2$, neglecting the negative solution, we find the rate of convergence for the secant method $q=(1+\sqrt{5})/2\simeq 1.618$.

======= Newton Rapson method =======
idx{Newton Rapson method}
The derivation of Newtons method, equation (ref{eq:nlin:newton}), done in the previous section was based on figure ref{fig:nlin:newton}. We will now derive it using a slightly different approach, but which lends itself easier to extend Newtons method to higher dimensions. The starting point is to expand the function around $x_k$, using Taylors formula
!bt
\begin{equation}
f(x)=f(x_k)+f^\prime(x_k)(x-x_k) + \cdots\,.
label{eq:nlin:nt}
\end{equation}
!et
Equation (ref{eq:nlin:newton}) can be derived from equation (ref{eq:nlin:nt}) by simply demanding that we keep the linear terms, and that the next point $x_{k+1}$ is located where the linear approximation intersects the $x$-axis, i.e. simply set $f(x)=0$, and $x=x_{k+1}$ in equation (ref{eq:nlin:nt}).

In higher order dimensions, we solve equation (ref{eq:nlin:fvec}), and equation (ref{eq:nlin:nt}) is
!bt
\begin{equation}
\mathbf{f}(\mathbf{x})=\mathbf{f}(\mathbf{x}_k)+ \mathbf{J}(\mathbf{x}_k)(\mathbf{x}-\mathbf{x}_k) + \cdots\,.
label{eq:nlin:ntd}
\end{equation}
!et
$\mathbf{J}(\mathbf{x}_k)$ is the Jacobian. As before, we simply set  $\mathbf{f}(\mathbf{x})=\mathbf{0}$, $\mathbf{x}=\mathbf{x}_{k+1}$, and keep the linear terms, hence
!bt
\begin{equation}
\mathbf{x}_{k+1}=\mathbf{x}_k-\mathbf{J}^{-1}(\mathbf{x}_k)\mathbf{f}(\mathbf{x}_k). 
label{eq:nlin:ntd2}
\end{equation}
!et
To make the mathematics a bit more clear, let us specify to $2D$. Assume that
$\mathbf{f}(\mathbf{x})=[f_x(x,y),f_y(x,y)]$, then the Jacobian is
!bt
\begin{equation}
\mathbf{J}(\mathbf{x}_k)=
\left(
\begin{array}{cc}
\frac{\partial f_x}{\partial x}&\frac{\partial f_x}{\partial y}\\
\frac{\partial f_y}{\partial x}&\frac{\partial f_y}{\partial y}
\end{array}
\right).
label{eq:nlin:jac}
\end{equation}
!et



======= Gradient descent =======
idx{gradient descent}
This method used is to minimize functions (does not work for root finding). In many nonlinear problems, we would like to minimize (or maximize) a function. An ideal 2D example is shown in figure ref{fig:nlin:grad}. The algorithm moves in the direction of steepest descent. Note that the step size might change towards the search. 

FIGURE: [fig-nlin/steepest_descent.png, width=400 frac=1.0] A very simple example of the gradient descent method. label{fig:nlin:grad}

Assume that we have a function $\mathbf{f}(\mathbf{x})$, that we would like to minimize. The gradient descent algorithm is simply to update parameters according to the derivative (gradient) of $\mathbf{f}$
!bt
\begin{equation}
\mathbf{x}_{k+1}=\mathbf{x}_{k}-\gamma\nabla\mathbf{f}.
label{eq:nlin:stpdc}
\end{equation}
!et
$\gamma$ is the learning rate, and a good choice of $\gamma$ is important. $\gamma$ might also change from one iteration to the other, and does not have to be constant.  

===== Exercise: Gradient descent solution of linear regression  =====

A very typical example is if we have a model and we would like to fit some parameters of the model to a data set (e.g. linear regression). Assume that we have observations $(x_i,y_i)$ and model predictions $f(x_i,\mathbf{\beta})$, the model parameters are contained in the vector $\mathbf{\beta}$. The *least square*, $S$, is the square of the sum of all the *residuals*, i.e. the difference between the observations and model predictions 
!bt
\begin{equation}
S=\sum_i(y_i-f(x_i,\mathbf{\beta}))^2.
label{eq:nlin:lsq}
\end{equation}
!et

Specializing to linear regression, we choose the model to be linear
!bt
\begin{equation}
f(x_i,\mathbf{\beta})=b_0+b_1x_i.
label{eq:nlin:lin}
\end{equation}
!et
Equation (ref{eq:nlin:lsq}) now takes the form
!bt
\begin{equation}
S=\sum_i(y_i-b_0+b_1x_i)^2.
label{eq:nlin:lsq2}
\end{equation}
!et
The gradients are:
!bt
\begin{align}
\frac{\partial S}{\partial b_0}&=-2\sum_i(y_i-b_0+b_1x_i),\no\\
\frac{\partial S}{\partial b_1}&=-2\sum_i(y_i-b_0+b_1x_i)x_i,.
label{eq:nlin:dlsq}
\end{align}
!et

* Implement the gradient descent method using a constant learning rate of $10^{-3}$, to minimize the least square function
* Test the linear regression on the data set $x_i=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]$, and $y=[1, 3, 2, 5, 7, 8, 8, 9, 10, 12]$, choose a starting value $(b_0,b_1)=(0,0)$. What happens if you increase the learning rate?

!bsol
Below is an implementation of the gradient descent method with a constant learning rate
@@@CODE src-nlin/gradient_descent_nD.py fromto: def gradient_descent@#end

The linear regression is implemented as below
@@@CODE src-nlin/gradient_descent_nD.py fromto: x_obs_ = np.array@b=gradient_descent
The first four iterations are shown in figure ref{fig:nlin:grsc}. If we choose a learning rate that is too high, we will move past the minimum, and the solution will oscillate. This can be avoided by lowering the learning rate as we iterate, by e.g. replacing `g` with `g/(n+1)` in the implementation above.

FIGURE: [fig-nlin/stdec_comb.png, width=400 frac=1.0] First four iterations of the gradient descent solution of linear regression. label{fig:nlin:grsc}  

!esol

======= Other useful methods =======

In this chapter we have covered the *basic*, but you should now be well equipped to dive into other methods. We highly recommend cite{press2001} as a starting point, although the code examples are written in C++, the theory is presented in a very accurate, but informal way.
* Brents method:  uses root bracketing, bisection, and inverse quadratic interpolation. The 1D method of choice if the function and not its derivative is known

##@@@CODE src-nlin/mandelbrot.py

###The purpose of this section is to introduce a handful of techniques for solving a nonlinear equation. In many cases a combination of methods must be used, and the algorithm must be adopted to your specific problem. 
!split

========= Numerical integration =========
label{ch:numint}
###### Content provided under a Creative Commons Attribution license, CC-BY 4.0; code under MIT License. (c)2018 Aksel Hiorth

======= Algorithmic thinking =======

The only way to improve in coding and algorithmic thinking is practice. The concept of one dimensional numerical integration is easy to understand, i.e. to calculate the area under a curve. In this chapter we will implement several numerical methods, and it will serve as a very simple playground that illustrates the key aspects of numerical modeling

o We start with a mathematical model (in this case an integral)
o The mathematical model is formulated in discrete form 
o Then we design an algorithm to solve the model 
o The numerical solution for a test case is compared with the true solution (could be an analytical solution or data)
o Error analysis: we investigate the accuracy of the algorithm by changing the number of iterations and/or make changes to the implementation or algorithm

The main point of this chapter is not to develop your own integration methods, the built in methods in Scipy will work in most cases. However, the way to break down the main task of calculating an integral into smaller tasks that is understandable by a computer, may work as a template for many different problems you would typically solve using a computer. A second motivation is that by analyzing the origin of numerical errors gives ideas for improving the algorithm, which is transferable to other problems.      

===== The midpoint rule =====
idx{midpoint method}
Numerical integration is encountered in numerous applications in physics and engineering sciences. 
Let us first consider the most simple case, a function $f(x)$, which is a function of one variable, $x$. The most straight forward way of calculating the area $\int_a^bf(x)dx$ is 
simply to divide the area under the function into $N$ equal rectangular slices with size $h=(b-a)/N$, as illustrated in figure ref{fig:numint:mid}. The area of one box is:
!bt
\begin{equation}
M(x_k,x_k+h)=f(x_k+\frac{h}{2}) h,\label{eq:numint:mid0}
\end{equation}
!et
and the area of all the boxes is:
!bt
\begin{align}
I(a,b)&=\int_a^bf(x)dx\simeq\sum_{k=0}^{N-1}M(x_k,x_k+h)\nonumber\\
&=h\sum_{k=0}^{N-1}f(x_k+\frac{h}{2})=h\sum_{k=0}^{N-1}f(a+(k+\frac{1}{2})h).
\label{eq:numint:mid1}
\end{align}
!et
Note that the sum goes from $k=0,1,\ldots,N-1$, a total of $N$ elements. We could have chosen to let the sum go from $k=1,2,\ldots,N$. 
In Python, C, C++ and many other programming languages the arrays start by indexing the elements from $0,1,\ldots$ to $N-1$, 
therefore we choose the convention of having the first element to start at $k=0$.

FIGURE: [fig-numint/func_sq.png, width=800] Integrating a function with the midpoint rule. label{fig:numint:mid}

Below is a Python code, where this algorithm is implemented for $\int_0^\pi\sin (x)dx$
@@@CODE src-numint/midpoint.py fromto: import@#%%

!bnotice
In the implementation above, we have taken advantage of Numpys ability to pass a vector to a function. This greatly enhances the speed and makes clean, readable code. If you were coding in a lower level programming language like Fortran, C or C++, you would probably implement the loop like (in Python syntax):
!bc pycod
for k in range(0,N): # loop over k=0,1,..,N-1
    val = lower_limit+(k+0.5)*h # midpoint value
    area += func(val)
return area*h
!ec
!enotice


% if FORMAT == 'ipynb':
By increasing $N$ the numerical result will get closer to the true answer. How much do you need to increase $N$ in order to reach an accuracy higher than $10^{-8}$.
 What happens when $N$ increases?
% endif

===== The trapezoidal rule =====
idx{trapezoidal method}
The numerical error in the above example is quite low, only about 2$\%$ for $N=5$. 
However, by just looking at the graph above it seems likely that we can develop a better algorithm by using trapezoids instead of rectangles, 
see figure ref{fig:numint:trap}.

FIGURE: [fig-numint/func_tr.png, width=800] Integrating a function with the trapezoidal rule. label{fig:numint:trap}

Earlier we approximated the area using the midpoint value: $f(x_k+h/2)\cdot h$. Now we use $A=A_1+A_2$, where $A_1=f(x_k)\cdot h$ 
and $A_2=(f(x_k+h)-f(x_k))\cdot h/2$, hence the area of one trapezoid is:
!bt
\begin{equation}
A\equiv T(x_k,x_k+h)=(f(x_k+h)+f(x_k))h/2.
\end{equation}
!et
This is the trapezoidal rule, and for the whole interval we get:
!bt
\begin{align}
I(a,b)&=\int_a^bf(x)dx\simeq\frac{1}{2}h\sum_{k=0}^{N-1}\left[f(x_k+h)+f(x_k)\right] \nonumber \\
&=h\left[\frac{1}{2}f(a)+f(a+h) + f(a+2h) +\nonumber\right. \\
&\left.\qquad\cdots + f(a+(N-2)h)+\frac{1}{2}f(b)\right]\nonumber \\
&=h\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=1}^{N-2}f(a+k h)\right].
\end{align}
!et
Note that this formula was bit more involved to derive, but it requires only one more function evaluations compared to the midpoint rule. 
Below is a python implementation:
@@@CODE src-numint/trapez.py fromto: def trap@N=

In the table below, we have calculated the numerical error for various values of $N$.

|--------c-------------------c-------------------c-------------------c----------|
| $N$               | $h$               | Error Midpoint    | Error Trapezoidal |
|--------c-------------------c-------------------c-------------------c----------|
| 1                 | 3.14              | -57\%             | 100\%             |
| 5                 | 0.628             | -1.66\%           | 3.31\%            |
| 10                | 0.314             | -0.412\%          | 0.824\%           |
| 100               | 0.031             | -4.11E-3\%        | 8.22E-3\%         |
|-------------------------------------------------------------------------------|


Note that we get the surprising result that this algorithm performs poorer, a factor of 2 than the midpoint rule.
How can this be explained? By just looking at figure ref{fig:numint:mid}, we see that the midpoint rule actually over predicts the area from $[x_k,x_k+h/2]$ 
 and under predicts in the interval $[x_k+h/2,x_{k+1}]$ or vice versa. The net effect is that for many cases the midpoint rule give a slightly better 
 performance than the trapezoidal rule. In the next section we will investigate this more formally.

===== Numerical errors on integrals =====
idx{numerical integrals, error}
It is important to know the accuracy of the methods we are using, otherwise we do not know if the
computer produce correct results. In the previous examples we were able to estimate the error because we knew the analytical result. However, if we know the 
analytical result there is no reason to use the computer to calculate the result(!). Thus, we need a general method to estimate the error, and let the computer 
run until a desired accuracy is reached. 

In order to analyze the midpoint rule in more detail we approximate the function by a Taylor 
series at the midpoint between $x_k$ and $x_k+h$: 
!bt
\begin{align}
f(x)&=f(x_k+h/2)+f^\prime(x_k+h/2)(x-(x_k+h/2))\nonumber\\ 
&+\frac{1}{2!}f^{\prime\prime}(x_k+h/2)(x-(x_k+h/2))^2+\mathcal{O}(h^3)
\end{align}
!et
Since $f(x_k+h/2)$ and its derivatives are constants it is straight forward to integrate $f(x)$:
!bt
\begin{align}
I(x_k,x_k+h)&=\int_{x_k}^{x_k+h}\left[f(x_k+h/2)+f^\prime(x_k+h/2)(x-(x_k+h/2))\right.\nonumber\\
&\left.+\frac{1}{2!}f^{\prime\prime}(x_k+h/2)(x-(x_k+h/2))^2+\mathcal{O}(h^3)\right]dx
\end{align}
!et
The first term is simply the midpoint rule, to evaluate the two other terms we make the substitution: $u=x-x_k$:
!bt
\begin{align}
I(x_k,x_k+h)&=f(x_k+h/2)\cdot h+f^\prime(x_k+h/2)\int_0^h(u-h/2)du\nonumber\\
&+\frac{1}{2}f^{\prime\prime}(x_k+h/2)\int_0^h(u-h/2)^2du+\mathcal{O}(h^4)\nonumber\\
&=f(x_k+h/2)\cdot h-\frac{h^3}{24}f^{\prime\prime}(x_k+h/2)+\mathcal{O}(h^4).
\end{align}
!et
Note that all the odd terms cancels out, i.e $\int_0^h(u-h/2)^m=0$ for $m=1,3,5\ldots$. Thus the error for the midpoint rule, $E_{M,k}$, on this particular interval is:
!bt
\begin{equation}
E_{M,k}=I(x_k,x_k+h)-f(x_k+h/2)\cdot h=-\frac{h^3}{24}f^{\prime\prime}(x_k+h/2),
\end{equation}
!et
where we have ignored higher order terms. We can easily sum up the error on all the intervals, but clearly $f^{\prime\prime}(x_k+h/2)$ will 
not, in general, have the same value on all intervals. However, an upper bound for the error can be found by replacing $f^{\prime\prime}(x_k+h/2)$ 
with the maximal value on the interval $[a,b]$, $f^{\prime\prime}(\eta)$:
!bt
\begin{align}
E_{M}&=\sum_{k=0}^{N-1}E_{M,k}=-\frac{h^3}{24}\sum_{k=0}^{N-1}f^{\prime\prime}(x_k+h/2)\leq-\frac{Nh^3}{24}f^{\prime\prime}(\eta),\label{eq:numint:em}\\
E_{M}&\leq-\frac{(b-a)^3}{24N^2}f^{\prime\prime}(\eta),
\end{align}
!et
where we have used $h=(b-a)/N$. We can do the exact same analysis for the trapezoidal rule, but then we expand the function around $x_k-h$ instead of the midpoint. 
The error term is then:
!bt
\begin{equation}
E_T=\frac{(b-a)^3}{12N^2}f^{\prime\prime}(\overline{\eta}).
\end{equation}
!et
At the first glance it might look like the midpoint rule always is better than the trapezoidal rule, but note that the second derivative is 
evaluated in different points ($\eta$ and $\overline{\eta}$). Thus it is possible to construct examples where the midpoint rule performs poorer 
than the trapezoidal rule.

Before we end this section we will rewrite the error terms in a more useful form as it is not so easy to evaluate 
$f^{\prime\prime}(\eta)$ (since we do not know which value of $\eta$ to use). By taking a closer look at equation (ref{eq:numint:em}), 
we see that it is closely related to the midpoint rule for $\int_a^bf^{\prime\prime}(x)dx$, hence:
!bt
\begin{align}
E_{M}&=-\frac{h^2}{24}h
\sum_{k=0}^{N-1}f^{\prime\prime}(x_k+h/2)\simeq-\frac{h^2}{24}\int_a^b
f^{\prime\prime}(x)dx\\
E_M&\simeq\frac{h^2}{24}\left[f^\prime(b)-f^\prime(a)\right]=-\frac{(b-a)^2}{24N^2}\left[f^\prime(b)-f^\prime(a)\right]
\end{align}
!et
The corresponding formula for the trapezoid formula is:
!bt
\begin{equation}
E_T\simeq \frac{h^2}{12}\left[f^\prime(b)-f^\prime(a)\right]=\frac{(b-a)^2}{12N^2}\left[f^\prime(b)-f^\prime(a)\right]
\end{equation}
!et
##Now, we can make an algorithm that automatically choose the number of steps to reach (at least) a predefined accuracy:
##@@@CODE src-numint/adaptive_midpoint.py
##!bnotice
##In Python it is sometimes convenient to enter default values for the arguments in a ##function. In the above example, we could also have written the function definition as\\ ##`def int_adaptive_midpoint(func, lower_limit, upper_limit,` \\ `tol=1e-8):`. If the ##`tol` parameter is not given the code will assume an accuracy of $10^{-8}$. 
##!enotice
===== Practical estimation of errors on integrals (Richardson extrapolation) =====
idx{Richardson extrapolation}
label{sec:numint:parct}
From the example above we were able to estimate the number of steps needed to reach (at least) a certain precision. 
In many practical cases we do not deal with functions, but with data and it can be difficult to evaluate the derivative. 
We also saw from the example above that the algorithm gives a higher precision than what we asked for. 
How can we avoid doing too many iterations? A very simple solution to this question is to double the number of intervals until 
a desired accuracy is reached. The following analysis holds for both the trapezoid and midpoint method, because in both cases 
the (global) error scale as $h^2$[^trapez].
[^trapez]: You can do the following analysis by assuming that the local error is $h^3$, but then you need to take into account that you need to take twice as many steps, which will give the same result.

Assume that we have evaluated the integral with a step size $h_1$, and the computed result is $I_1$. 
Then we know that the true integral is $I=I_1+c h_1^2$, where $c$ is a constant that is unknown. If we now half the step size: $h_2=h_1/2$, 
then we get a new (better) estimate of the integral, $I_2$, which is related to the true integral $I$ as: $I=I_2+c h_2^2$. 
Taking the difference between $I_2$ and $I_1$ give us an estimation of the error:
!bt
\begin{equation}
I_2-I_1=I-c h_2^2-(I-ch_1^2)=3c h_2^2,
\end{equation}
!et
where we have used the fact that $h_1=2h_2$, Thus the error term is:
!bt
\begin{equation}
E(a,b)=c h_2^2=\frac{1}{3}(I_2-I_1).
\end{equation}
!et
This might seem like we need to evaluate the integral twice as many times as needed. This is not the case, by choosing to exactly 
half the spacing we only need to evaluate for the values that lies halfway between the original points. We will demonstrate how 
to do this by using the trapezoidal rule, because it operates directly on the $x_k$ values and not the midpoint values. 
The trapezoidal rule can now be written as:
!bt
\begin{align}
I_2(a,b)&=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=1}^{N_2-1}f(a+k h_2)\right],\\
&=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=\text{even values}}^{N_2-1}f(a+k h_2)\right.\nonumber\\
&\left.\qquad+\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2)\right],
\end{align}
!et
in the last equation we have split the sum into odd an even values. The sum over the even values can be rewritten:
!bt
\begin{equation}
\sum_{k=\text{even values}}^{N_2-1}f(a+k h_2)=\sum_{k=0}^{N_1-1}f(a+2k h_2)=\sum_{k=0}^{N_1-1}f(a+k h_1),
\end{equation}
!et
note that $N_2$ is replaced with $N_1=N_2/2$, we can now rewrite $I_2$ as:
!bt
\begin{align}
I_2(a,b)&=h_2\left[\frac{1}{2}f(a)+\frac{1}{2}f(b)+\sum_{k=0}^{N_1-1}f(a+k h_1)\right.\nonumber\\
&\left.+\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2)\right]
\end{align}
!et
Note that the first terms are actually the trapezoidal rule for $I_1$, hence:
!bt
\begin{equation}
I_2(a,b)=\frac{1}{2}I_1(a,b)+h_2\sum_{k=\text{odd values}}^{N_2-1}f(a+k h_2).
\end{equation}
!et
The factor $1/2$ in front of $I_1(a,b)$, appears because $h_2=h_1/2$. 
A possible algorithm is then:
o Choose a low number of steps to evaluate the integral, $I_0$, the first time, e.g. $N_0=1$
o Double the number of steps, $N_1=2N_0$ 
o Calculate the missing values by summing over the odd number of steps $\sum_{k=\text{odd values}}^{N_1-1}f(a+k h_1)$
o Check if $E_1(a,b)=\frac{1}{3}(I_1-I_0)$ is lower than a specific tolerance
o If yes quit, if not, return to 2, and continue until $E_i(a,b)=\frac{1}{3}(I_{i+1}-I_{i})$ is lower than the tolerance  

Below is a Python implementation:
@@@CODE src-numint/adaptive_trapez.py fromto:def int_@prec
% if FORMAT == 'ipynb':
What is a good number to start with, what happens if we choose $N_0$ too large? Compare the adaptive midpoint rule with the adaptive 
trapezoidal rule, is it possible to get the same accuracy with the same number of iterations? Check the expected number of 
iterations with the theoretical value $N=\sqrt{\frac{(b-a)^2}{12E_T}\left[f^\prime(b)-f^\prime(a)\right]}$.
% endif

If you compare the number of terms used in the adaptive trapezoidal rule, which was developed by halving the step size, and the adaptive midpoint rule that was derived on the basis of the theoretical error term, you will find the adaptive midpoint rule is more efficient. So why go through all this trouble? In the next section we will see that the development we did for the adaptive trapezoidal rule is closely related to Romberg integration, which is *much* more effective.


 
======= Romberg integration =======
idx{Romberg integration}
The adaptive algorithm for the trapezoidal rule in the previous section can be easily improved by remembering 
that the true integral was given by[^romerr] : $I=I_i+ch_i^2+\mathcal{O}(h^4)$. The error term was in the previous example only used to 
check if the desired tolerance was achieved, but we could also have added it to our estimate of the integral to reach an accuracy to fourth order:

[^romerr]: Note that all odd powers of $h$ is equal to zero, thus the corrections are always in even powers.  

!bt
\begin{equation}
I=I_{i+1}+ch^2+\mathcal{O}(h^4)=I_{i+1}+\frac{1}{3}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^4).
\end{equation}
!et
As before the error term $\mathcal{O}(h^4)$, can be written as: $ch^4$. Now we can proceed as in the previous section: First we estimate the 
integral by one step size $I_i=I+ch_i^4$, next we half the step size $I_{i+1}=I+ch_{i+1}^4$ and use these two estimates to calculate the error term:
!bt
\begin{align}
I_{i+1}-I_{i}&=I-c h_{i+1}^4-(I-ch_i^4)=-c h_{i+1}^4+c(2h_{i+1})^4=15c h_{i+1}^4,\nonumber\\
ch_{i+1}^4&=\frac{1}{15}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^6).
\end{align}
!et
but now we are in the exact situation as before, we have not only the error term but the correction up to order $h^4$ for this integral:
!bt
\begin{equation}
I=I_{i+1}+\frac{1}{15}\left[I_{i+1}-I_{i}\right]+\mathcal{O}(h^6).\label{eq:numint:rom}
\end{equation}
!et
Each time we half the step size we also gain a higher order accuracy in our numerical algorithm. Thus, there are two iterations going on at the same time; 
one is the iteration that half the step size ($i$), and the other one is the increasing number of higher order terms added (which we will denote $m$). 
We need to improve our notation, and replace the approximation of the integral ($I_i$) with $R_{i,m}$. Equation (ref{eq:numint:rom}), can now 
be written:
!bt
\begin{equation}
I=R_{i+1,2}+\frac{1}{15}\left[R_{i+1,2}-R_{i,2}\right]+\mathcal{O}(h^6).
\end{equation}
!et
A general formula valid for any $m$ can be found by realizing:
!bt
\begin{align}
I&=R_{i+1,m+1}+c_mh_i^{2m+2}+\mathcal{O}(h_i^{2m+4})\label{eq:numint:rom0}\\
I&=R_{i,m+1}+c_mh_{i-1}^{2m+2}+\mathcal{O}(h_{i-1}^{2m+4})\nonumber\\
&=R_{i,m+1}+2^{2m+2}c_mh_{i}^{2m+2}+\mathcal{O}(h_{i-1}^{2m+4}),\label{eq:numint:rom1}
\end{align}
!et
where, as before $h_{i-1}=2h_i$. Subtracting equation (ref{eq:numint:rom0}) and (ref{eq:numint:rom1}), we find an expression for the error term:
!bt
\begin{align}
c_mh_{i}^{2m+2}&=\frac{1}{4^{m+1}-1}(R_{i,m}-R_{i-1,m})\label{eq:numint:rom2}
\end{align}
!et
Then the estimate for the integral in equation (ref{eq:numint:rom1}) is:
!bt
\begin{align}
I&=R_{i,m+1}+\mathcal{O}(h_i^{2m+2})\\
R_{i,m+1}&=R_{i,m}+\frac{1}{4^{m+1}-1}(R_{i+1,m}-R_{i,m}).
\end{align}
!et
A possible algorithm is then:

o Evaluate $R_{0,0}=\frac{1}{2}\left[f(a)+f(b)\right](b-a)$ as the first estimate
o Double the number of steps, $N_{i+1}=2N_i$ or half the step size $h_{i+1}=h_i/2$ 
o Calculate the missing values by summing over the odd number of steps $\sum_{k=\text{odd values}}^{N_1-1}f(a+k h_{i+1})$
o Correct the estimate by adding *all* the higher order error term $R_{i,m+1}=R_{i,m}+\frac{1}{4^m-1}(R_{i+1,m+1}-R_{i,m+1})$
o Check if the error term is lower than a specific tolerance $E_{i,m}(a,b)=\frac{1}{4^{m+1}-1}(R_{i,m}-R_{i-1,m})$, if yes quit, if no goto 2, increase $i$ and $m$ by one
The algorithm is illustrated in figure ref{fig:numint:romberg}.
FIGURE: [fig-numint/romberg.png, width=400 frac=0.5] Illustration of the Romberg algorithm. Note that for each new evaluation of the integral $R_{i,0}$, all the correction terms $R_{i,m}$ (for $m>0$) must be evaluated again. label{fig:numint:romberg}

Note that the tolerance term is not the correct one as it uses the error estimate for the current step, 
which we also use correct the integral in the current step to reach a higher accuracy. 
Thus the error on the integral will always be lower than the user specified tolerance.
Below is a Python implementation:
@@@CODE src-numint/romberg.py fromto: def int_romberg@#end

Note that the Romberg integration only uses 32 function evaluations to reach a precision of $10^{-8}$, whereas the adaptive midpoint and trapezoidal rule in the previous
section uses 20480 and 9069 function evaluations, respectively. 

===== Alternative implementation of adaptive integration =====
Before we proceed, we will consider an alternative implementation of the adaptive method presented in the previous sections, with the following modification
o We will use Simpsons rule (see the exercise at the end), which takes the following form $\int_a^bf(x)dx\simeq\frac{h}{6}\left[f(a)+4f(a+\frac{h}{2})+2f(a+h)+ 4f(a+3\frac{h}{2})+2f(a+2h)+\cdots+f(b)\right]$
o We only divide the intervals needed to reach the desired accuracy.
Simpsons rule is accurate up to $\mathcal{O}(h^4)$, and by following the same arguments as above we can estimate the error as $E_i(a,b)=\frac{1}{15}(I_{i+1}-I_{i})$. The factor 1/15 (as opposed to 1/3) originates from the higher order accuracy. The integration proceeds as follows
* `S` is an empty list
* `S.append([a,b])`
* $I=0$
* `while S not empty do:`
  * `[a,b]=S.pop(-1)`
  * $m=(b+a)/2$
  * $I_1=$ `simpson_step(a,b)`
  * $I_2=$ `simpson_step(a,m)+simpson_step(m,b)`
  * if $|I_1-I_2|<15|b-a|\cdot tol$
    * $I+=I_2$
  * else:
    * `S.append([a,m])`
    * `S.append([m,b])`
  * return $I$
Note the use of the list `S`, we remove the interval $[a,b]$ from the list and calculates the integral. If the integral is not accurate enough we add to new intervals to the list, and continue until we reach the desired accuracy, then we proceed with the next interval. Since we remove (`pop`) the element from the list, we know that we will finish the evaluation once the list is empty. This algorithm allows for different sub interval to have different degrees of subdivisions, contrary to Rombergs algorithm. The full python implementation is shown below
@@@CODE src-numint/adaptive_trapez.py fromto: simpson_step@Area =


======= Gaussian quadrature =======
idx{Gaussian quadrature}
Many of the methods we have looked into are of the type:
!bt
\begin{align}
	\int_a^b f(x) dx = \sum_{k=0}^{N-1} \omega_k f(x_k),\label{eq:numint:qq1}
\end{align}
!et
where the function is evaluated at fixed interval. For the midpoint rule $\omega_k=h$ for all values of $k$, for the trapezoid rule 
$\omega_k=h/2$ for the endpoints and $h$ for all the interior points. 
For the Simpsons rule (see exercise) $\omega_k=h/3, 4h/3,2h/3,4h/3,\ldots,4h/3,h/3$. 
Note that all the methods we have looked at so far samples the function in equal spaced points, $f(a+k h)$, 
for $k=0, 1, 2\ldots, N-1$. If we now allow for the function to be evaluated at unevenly spaced points, we can do a lot better. 
This realization is the basis for Gaussian Quadrature. We will explore this in the following, 
but to make the development easier and less cumbersome, we transform the integral from the domain $[a,b]$ to $[-1,1]$:
!bt
\begin{align}
\int_a^bf(t)dt&=\frac{b-a}{2}\int_{-1}^{1}f(x)dx\text{ , where:}\\
x&=\frac{2}{b-a}t-\frac{b+a}{b-a}.
\end{align}
!et
The factor in front comes from the fact that $dt=(b-a)dx/2$, thus we can develop our algorithms on the domain $[-1,1]$, 
and then do the transformation back using: $t=(b-a)x/2+(b+a)/2$.

!bnotice
The idea we will explore is as follows:
If we can approximate the function to be integrated on the domain $[-1,1]$ (or on $[a,b]$) as a 
polynomial of as *large a degree as possible*, then the numerical integral of this polynomial will be very close to the integral of the 
function we are seeking.
!enotice
This idea is best understood by a couple of examples. Assume that we want to use $N=1$ in equation (ref{eq:numint:qq1}):
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx\simeq\omega_0f(x_0).
\end{equation}
!et
We now choose $f(x)$ to be a polynomial of as large a degree as possible, but with the requirement that the integral is exact. If $f(x)=1$, we get:
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx=\int_{-1}^{1}1\,dx=2=\omega_0,
\end{equation}
!et
hence $\omega_0=2$. If we choose $f(x)=x$, we get:
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx=\int_{-1}^{1}x\,dx=0=\omega_0f(x_0)=2x_0,
\end{equation}
!et
hence $x_0=0$. 
!bnotice The Gaussian integration rule for $N=1$ is:

!bt
\begin{align}
&\int_{-1}^{1}f(x)\,dx\simeq 2f(0)\text{, or: }\nonumber\\
&\int_{a}^{b}f(t)\,dt\simeq\frac{b-a}{2}\,2f(\frac{b+a}{2})=(b-a)f(\frac{b+a}{2}).
\end{align}
!et

!enotice

This equation is equal to the midpoint rule, by choosing $b=a+h$ we reproduce equation (ref{eq:numint:mid0}). If we choose $N=2$:
!bt
\begin{equation}
\int_{-1}^{1}f(x)\,dx\simeq\omega_0f(x_0)+\omega_1f(x_1),
\end{equation}
!et
we can show that now $ f(x)=1,\,x,\,x^2\,x^3$ can be integrated exact:
!bt
\begin{align}
\int_{-1}^{1}1\,dx&=2=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0+\omega_1\,,\\
\int_{-1}^{1}x\,dx&=0=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0+\omega_1x_1\,,\\
\int_{-1}^{1}x^2\,dx&=\frac{2}{3}=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0^2+\omega_1x_1^2\,,\\
\int_{-1}^{1}x^3\,dx&=0=\omega_0f(x_0)+\omega_1f(x_1)=\omega_0x_0^3+\omega_1x_1^3\,,
\end{align}
!et
hence there are four unknowns and four equations. The solution is: $\omega_0=\omega_1=1$ and $x_0=-x_1=1/\sqrt{3}$.

!bnotice The Gaussian integration rule for $N=2$ is:
!bt
\begin{align}
\int_{-1}^{1}f(x)\,dx&\simeq f(-\frac{1}{\sqrt{3}})+f(\frac{1}{\sqrt{3}})\, \text{, or:}\\
\int_{a}^{b}f(x)\,dx&\simeq \frac{b-a}{2}\left[f(-\frac{b-a}{2}\frac{1}{\sqrt{3}}+\frac{b+a}{2})
+f(\frac{b-a}{2}\frac{1}{\sqrt{3}}+\frac{b+a}{2})\right].
\end{align}
!et
!enotice

@@@CODE src-numint/gaussquad2.py fromto: def int_@a=0

=== The case N=3 ===
For the case $N=3$, we find that $f(x)=1,x,x^2,x^3,x^4,x^5$ can be integrated exactly:
!bt
\begin{align}
\int_{-1}^{1}1\,dx&=2=\omega_0+\omega_1+\omega_2\,,\\
\int_{-1}^{1}x\,dx&=0=\omega_0x_0+\omega_1x_1+\omega_2x_2\,,\\
\int_{-1}^{1}x^2\,dx&=\frac{2}{3}=\omega_0x_0^2+\omega_1x_1^2+\omega_2x_2^2\,,\\
\int_{-1}^{1}x^3\,dx&=0=\omega_0x_0^3+\omega_1x_1^3+\omega_2x_2^3\,,\\
\int_{-1}^{1}x^4\,dx&=\frac{2}{5}=\omega_0x_0^4+\omega_1x_1^4+\omega_2x_2^4\,,\\
\int_{-1}^{1}x^5\,dx&=0=\omega_0x_0^5+\omega_1x_1^5+\omega_2x_2^5\,,
\end{align}
!et
the solution to these equations are $\omega_{0,1,2}=5/9, 8/9, 5/9$ and $x_{1,2,3}=-\sqrt{3/5},0,\sqrt{3/5}$. Below is a Python implementation:
@@@CODE src-numint/gaussquad3.py fromto: def int_@a=0

Note that the Gaussian quadrature converges very fast. From $N=2$ to $N=3$ function evaluation we reduce the error (in this specific case) 
from 6.5% to 0.1%. Our standard trapezoidal formula needs more than 20 function evaluations to achieve this, the Romberg method uses 4-5 function
evaluations. How can this be? If we use the standard Taylor formula for the function to be integrated, we know that for $N=2$ the Taylor 
formula must be integrated up to $x^3$, so the error term is proportional to $h^4f^{(4)}(\xi)$ (where $\xi$ is some x-value in $[a,b]$). 
$h$ is the step size, and we can replace it with $h\sim (b-a)/N$, thus the error scale as $c_N/N^4$ (where $c_N$ is a constant). 
Following the same argument, we find for $N=3$ that the error term is $h^6f^{(6)}(\xi)$ or that the error term scale as $c_N/N^6$. 
Each time we increase $N$ by a factor of one, the error term reduces by $N^2$. Thus if we evaluate the integral for $N=10$, 
increasing to $N=11$ will reduce the error by a factor of $11^2=121$.

===== Error term on Gaussian integration =====
idx{Gaussian quadrature, error term}
The Gaussian integration rule of order $N$ integrates exactly a polynomial of order $2N-1$. 
%if book:
From Taylors error formula, see equation (ref{eq:taylor:error}) in Chapter ref{ch:taylor},
%else:
From Taylors error formula, 
%endif
we can easily see that the error term must be of order $2N$, and be proportional to $f^{(2N)}(\eta)$, see cite{stoer2013} for more details on the derivation of error terms. The drawback with an analytical error term derived from series expansion is that it involves the derivative of the function. As we have already explained, this is very unpractical and it is much more practical to use the methods described in section ref{sec:numint:parct}. Let us consider this in more detail, assume that we evaluate the integral using first a Gaussian integration rule with $N$ points, and then $N+1$ points. Our estimates of the "exact" integral, $I$,  would then be:
!bt
\begin{align}
 I&=I_N+ch_{N}^{2N},label{eq:numint:gerr1}\\
 I&=I_{N+1}+ch_{N+1}^{2N+1}.
label{eq:numint:gerr2}
\end{align}
!et
In principle $h_{N+1}\neq h_{N}$, but in the following we will assume that $h_N\simeq h_{N+1}$, and $h\ll 1$. Subtracting equation (ref{eq:numint:gerr1}) and (ref{eq:numint:gerr2}) we can show that a reasonable estimate for the error term $ch^{2N}$ would be:
!bt
\begin{equation}
ch^N= I_{N+1}-I_N.
\end{equation}
!et
If this estimate is lower than a given tolerance we can be quite confident that the higher order estimate $I_{N+1}$ approximate the true integral within our error estimate. This is the method implemented in SciPy, "`integrate.quadrature`":"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.quadrature.html"

===== Common weight functions for classical Gaussian quadratures =====
##https://pomax.github.io/bezierinfo/legendre-gauss.html
======= Integrating functions over an infinite range  =======
idx{numerical integral, infinite}
Integrating a function over an infinite range can be done by the following trick. Assume that we would like to evaluate
!bt
\begin{equation}
\int_a^\infty f(x) dx.
label{eq:numint:inf}
\end{equation}
!et
If we introduce the following substitution
!bt
\begin{equation}
z=\frac{x-a}{1+x-a},
label{eq:numint:infs}
\end{equation}
or equivalently
\begin{equation}
x=a+\frac{z}{1-z},
label{eq:numint:infs2}
\end{equation}
!et
then if $x=a$, $z=0$, and if $x\to\infty$ then $z\to1$, hence:
!bt
\begin{equation}
\int_a^\infty f(x) dx = \int_0^1 f(a+\frac{z}{1-z}) \frac{dz}{(1-z)^2}.
label{eq:numint:infs3}
\end{equation}
!et
##===== Exercises =====

===== Exercise: Numerical Integration =====
!bsubex
Show that for a linear function, $y=a\cdot x+b$ both the trapezoidal rule and the rectangular rule are exact
!esubex
!bsubex
Consider $I(a,b)=\int_a^bf(x)dx$ for $f(x)=x^2$. The analytical result is $I(a,b)=\frac{b^3-a^3}{3}$. Use the Trapezoidal and 
  Midpoint rule to evaluate these integrals and show that the error for the Trapezoidal rule is exactly twice as big as the Midpoint rule.
!esubex
!bsubex
Use the fact that the error term on the trapezoidal rule is twice as big as the midpoint rule to derive Simpsons formula: $I(a,b)=\sum_{k=0}^{N-1}I(x_k,x_k+h)=\frac{h}{6}\left[f(a)+ 4f(a+\frac{h}{2})+2f(a+h)+4f(a+3\frac{h}{2})+2f(a+2h)+\cdots+f(b)\right]$ Hint: $I(x_k,x_k+h)=M(x_k,x_k+h)+E_M$ (midpoint rule) and $I(x_k,x_k+h)=T(x_k,x_k+h)+E_T=T(x_k,x_k+h)-2E_M$ (trapezoidal rule).

!bsol
Simpsons rule is an improvement over the midpoint and trapezoidal rule. It can be derived in different ways, we will make use of 
the results in the previous section. If we assume that the second derivative is reasonably well behaved on the interval $x_k$ 
and $x_k+h$ and fairly constant we can assume that $f^{\prime\prime}(\eta)\simeq f^{\prime\prime}(\overline{\eta})$, hence $E_T=-2E_M$.
!bt
\begin{align}
I(x_k,x_k+h)&=M(x_k,x_k+h)+E_M\text{ (midpoint rule)}\\
I(x_k,x_k+h)&=T(x_k,x_k+h)+E_T\nonumber\\
&=T(x_k,x_k+h)-2E_M\text{ (trapezoidal rule)},
\end{align}
!et
we can now cancel out the error term by multiplying the first equation with 2 and adding the equations:
!bt
\begin{align}
3I(x_k,x_k+h)&=2M(x_k,x_k+h)+T(x_k,x_k+h)\\
&=2f(x_k+\frac{h}{2}) h+\left[f(x_k+h)+f(x_k)\right] \frac{h}{2}\\
I(x_k,x_k+h)&=\frac{h}{6}\left[f(x_k)+4f(x_k+\frac{h}{2})+f(x_k+h)\right].
\end{align}
!et
Now we can do as we did in the case of the trapezoidal rule, sum over all the elements:
!bt
\begin{align}
I(a,b)&=\sum_{k=0}^{N-1}I(x_k,x_k+h)\nonumber\\
&=\frac{h}{6}\left[f(a)+ 4f(a+\frac{h}{2})+2f(a+h)+4f(a+3\frac{h}{2})\right.\nonumber\\
&\left.\qquad+2f(a+2h)+\cdots+f(b)\right]\\
&=\frac{h^\prime}{3}\left[f(a)+ f(b) + 4\sum_{k= \text{odd}}^{N-2}f(a+k h^\prime)+2\sum_{k= \text{even}}^{N-2}f(a+k h^\prime)\right],
\end{align}
!et
note that in the last equation we have changed the step size $h=2h^\prime$.
!esol



!esubex
!bsubex
Show that for $N=2$ ($f(x)=1,x,x^3$), the points and Gaussian quadrature rule for $\int_{0}^{1}x^{1/2}f(x)\,dx$
is $\omega_{0,1}=-\sqrt{70}{150} + 1/3, \sqrt{70}{150} + 1/3$
##=\simeq 0.27755599823106164, 0.38911066843560504$
and $x_{0,1}=-2\sqrt{70}{63} + 5/9, 2\sqrt{70}{63} + 5/9$
##\simeq 0.27755599823106164, 0.38911066843560504$
o Integrate $\int_0^1x^{1/2}\cos x\,dx$ using the rule derived in the exercise above and compare with the standard Gaussian quadrature rule for ($N=2$, and $N=3$).
!esubex
!bsubex
Make a Python program that uses the Midpoint rule to integrate experimental data that are unevenly spaced and given in the form of two arrays.  
!esubex


!split

========= Ordinary differential equations =========
label{ch:ode}
###### Content provided under a Creative Commons Attribution license, CC-BY 4.0; code under MIT License. (c)2018 Aksel Hiorth

% if FORMAT == 'ipynb':
!bc pycod
from IPython.core.display import HTML
css_file = 'style.css'
HTML(open(css_file, "r").read())
!ec
% endif

======= 0D models =======
By 0D models we mean models that are only dependent on one variable, usually time. These models are often called "lumped parameter models", because they ignore spatial dependency and the parameters in the model are adjusted to match experimental data. In many cases it is more important to predict how a system evolves in time, than the spatial dependency. A very recent example is the spread of infectious deceases, like Covid-19, where the evolution of total number of infected people might be the most important and not where the infection happens. 

======= Ordinary differential equations =======

Physical systems evolves in space and time, and very often they are described by a ordinary differential equations (ODE) and/or
partial differential equations (PDE). The difference between an ODE and a PDE is that an ODE only describes 
the changes in one spatial dimension *or* time, whereas a PDE describes a system that evolves in the $x-$, $y-$, $z-$ dimension 
and/or in time. In the following we will spend a significant
amount of time to explore one of the simplest algorithm, Eulers method.
Sometimes this is exactly the algorithm you would like to use, but with very 
little extra effort much more sophisticated algorithms can easily be implemented, such as the Runge-Kutta fourth order method.
However, all these algorithms, will at some point run into the same
kind of troubles if used reckless. Thus we will use the Eulers method as a playground,
investigate when the algorithm run into trouble and
suggests ways to fix it, these approaches can easily be extended to the higher order methods. Most of the other algorithms boils down to the same idea of extrapolating
a function using derivatives multiplied with a small step size.  

======= A simple model for fluid flow =======
idx{continuous stirred tank reactor (CSTR)}
Let us consider a simple example from chemical engineering, a continuous stirred tank reactor (CSTR), see figure ref{fig:ode:cstr}. 
The flow is incompressible ($q_\text{out}=q_\text{in}$), a fluid is entering
on the top and exiting at the bottom, the tank has a fixed volume $V$. Assume that the tank is filled with saltwater, and that freshwater is pumped into it, how much time does it 
take before $90\%$ of the saltwater is replaced with freshwater? The tank is *well mixed*, illustrated with the propeller, this means that at every time the 
concentration is uniform in the tank, i.e. that $C(t)=C_\text{out}(t)$.  

FIGURE: [fig-ode/cstr, width=800] A continuous stirred tank model, $C(t)=C_\text{out}(t)$, and $q_\text{out}=q_\text{in}$. label{fig:ode:cstr}

The concentration $C$ is measured in gram of salt per liter water, and the flow rate $q$ is liter of water per day. The model for the salt balance in this system can be described in words by:
!bt
\begin{align}
[\text{accumulation of salt}] &= [\text{salt into the system}] - [\text{salt out of the system}]\nonumber\\
& + [\text{generation of salt}].\label{eq:ode:mbal}
\end{align}
!et
In our case there are no generation of salt within the system so this term is zero. The flow of salt into the system during a time $\Delta t$ is: 
$q_\text{in}(t)\cdot C_\text{in}(t)\cdot \Delta t=q(t)\cdot C_\text{in}(t)\cdot \Delta t$, 
the flow of salt out of the system is: $q_\text{out}(t)\cdot C_\text{out}(t)\cdot \Delta t=q(t)\cdot C(t)\cdot \Delta t$, and the accumulation during a time step is:
$C(t+\Delta t)\cdot V - C(t)\cdot V$, hence:
!bt
\begin{equation}
C(t+\Delta t)\cdot V - C(t)\cdot V = q(t)\cdot C_\text{in}(t)\cdot \Delta t - q(t)\cdot C(t)\cdot \Delta t.\label{eq:ode:cstr1}
\end{equation}
!et 
Note that it is not a priori apparent, which time the concentrations and flow rates on the right hand side should be evaluated at, 
we could have chosen to evaluate them at $t+\Delta t$, or at any time $t\in [t,t+\Delta t]$. We will return to this point later in this chapter. Dividing by $\Delta t$, and taking the limit
$\Delta t\to 0$, we can write equation (ref{eq:ode:cstr1}) as:
!bt
\begin{equation}
V\frac{dC(t)}{dt} = q(t)\left[C_\text{in}(t) - C(t)\right].\label{eq:ode:cstr2}
\end{equation}
!et 
Seawater contains about 35 gram salt/liter fluid, if we assume that the fresh water contains no salt, we have the boundary conditions
$C_\text{in}(t)=0$, $C(0)=$35gram/l. The equation (ref{eq:ode:cstr2}) the reduces to:
!bt
\begin{equation}
V\frac{dC(t)}{dt} = -qC(t),\label{eq:ode:cstr3}
\end{equation}
!et
this equation can easily be solved, by dividing by $C$, multiplying by $dt$ and integrating:
!bt
\begin{align}
V\int_{C_0}^C\frac{dC}{C} &= -q\int_0^tdt,\nonumber\\
C(t)&=C_0e^{-t/\tau},\text{ where } \tau\equiv \frac{V}{q}.\label{eq:ode:sol}
\end{align}
!et
This equation can be inverted to give $t=-\tau\ln[C(t)/C]$. If we assume that the volume of the tank is 1m$^3$=1000liters, 
and that the flow rate is 1 liter/min, we find that $\tau$=1000min=0.69days and that it takes about $-0.69\ln0.9\simeq1.6$days to reduce the concentration
by 90$\%$ to 3.5 gram/liter.     

!bnotice The CSTR
You might think that the CSTR is a very simple model, and it is, but this type of model is the basic building blocks in chemical engineering.
By putting CSTR tanks in series and/or connecting them with pipes, the efficiency of manufacturing various type of chemicals
can be investigated. Although the CSTR is an idealized model for the part of a chemical factory, it is actually a *very good* model 
for fluid flow in a porous media. By connecting CSTR tanks in series, one can model how chemical tracers propagate in the subsurface. 
The physical reason for this is that dispersion in porous media will play the role of the propellers and mix the concentration
uniformly.      
!enotice

======= Euler's method =======
idx{Eulers method}
If the system gets slightly more complicated, e.g several tanks in series with a varying flow rate or if salt was generated in the tank, there is a
good chance that we have to solve the equations numerically to obtain a solution.
Actually, we have already developed a numerical algorithm to solve equation (ref{eq:ode:cstr2}), 
before we arrived at equation (ref{eq:ode:cstr2}) in equation (ref{eq:ode:cstr1}). This is a special case of Eulers method, which is basically to 
replace the derivative in equation (ref{eq:ode:cstr2}), with $(C(t+\Delta t)-C(t))/\Delta t$. By rewriting equation (ref{eq:ode:cstr1}), so that we
keep everything related to the new time step, $t+\Delta t$, on one side, we get:
!bt
\begin{align}
VC(t+\Delta t) &= VC(t) + qC_\text{in}(t) - qC(t),\label{eq:ode:eu0}\\
C(t+\Delta t) &= C(t) + \frac{\Delta t}{\tau}\left[C_\text{in}(t) - C(t)\right]\label{eq:ode:eu1},
\end{align}
!et
we introduce the short hand notation: $C(t)=C_n$, and $C(t+\Delta t)=C_{n+1}$, hence the algorithm can be written more compact as:
!bt
\begin{equation}
C_{n+1} = \left(1-\frac{\Delta t}{\tau}\right)C_n + \frac{\Delta t}{\tau}C_{\text{in},n}\label{eq:ode:eu2},
\end{equation}
!et
In the script below, we have implemented equation (ref{eq:ode:eu2}).
@@@CODE src-ode/euler.py fromto: def analytical@# rest of
FIGURE: [fig-ode/euler, width=800] The concentration in the tank for different step size $\Delta t$. label{fig:ode:euler}

In figure ref{fig:ode:euler} the result of the implementation is shown for different values of $\Delta t$.
Clearly we see that the results are dependent on the step size, as the step increases the numerical solution deviates from the analytical solution. At some point the 
numerical algorithm fails completely, and produces results that have no meaning. 

===== Error analysis - Euler's method =====
idx{Eulers method, error analysis}
There are two obvious questions:
o When does the algorithm produce unphysical results?  
o What is an appropriate step size? 
Let us consider the first question, clearly when the concentrations gets negative the solution is unphysical. From equation (ref{eq:ode:eu2}), 
we see that when $\Delta t/\tau > 1$, the concentration 
become negative. For this specific case (the CSTR), there is a clear physical interpretation of this condition. Inserting $\tau=V/q$, we can rewrite
the condition $\Delta t/\tau <1$ as $q\Delta t < V$. The volume into the tank during one time step is: $q\Delta t$, which means that
whenever we *flush more than one tank volume through the tank during one time step, the algorithm fails*.
When this happens the new concentration in the tank cannot be predicted from the old one. This makes sense, because we could have switched to a
new solution (e.g. seawater) during that time step, then the new solution does not have any relation to the old solution. 

The second question, "what is an appropriate step size?",  is a bit more difficult to answer.
One strategy could be to simply use the results from chapter [Taylor], where we showed that the truncation error had a minimum value
with a step size of $10^{-8}$  (when using a first order Taylor approximation).
How does the value $10^{-8}$ relate to the step sizes in minutes used in our Euler implementation?
In order to see the connection, we need to rewrite equation (ref{eq:ode:cstr2}) in a dimensionless form,
by making the following substitution:
 $t\to t/\tau$:
!bt
\begin{equation}
\frac{dC(\tau)}{d\tau} = \left[C_\text{in}(\tau) - C(\tau)\right].\label{eq:ode:cstr2dim}
\end{equation}
!et
As we found earlier $\tau = 1000$min, thus a step size of e.g. 1 min would correspond to a dimensionless time step of 
$\Delta t\to$1min/1000min$=10^{-3}$. This number can be directly compared to the value $10^{-8}$, which is the lowest value we can
choose without getting into trouble with round off errors on the machine. 
!bnotice Dimensionless variables
It is a  good idea to formulate our equations in terms of dimensionless variables.
The algorithms we develop can then be used in the same form regardless of changes in the system size and flow rates.
Thus we do not need to rewrite the algorithm each time the physical system changes. This also means that if you use
an algorithm developed by someone else (e.g. in Matlab or Python), you should always formulate the ODE system in dimensionless form before using the algorithm.

A second reason is that from a pure modeling point of view, dimensionless variables is a way of getting some
understanding of what kind of combination of the physical parameters that describes the behavior of the system.
For the case of the CSTR, there is a time scale $\tau=V/q$, which 
is an intrinsic measure of time in the system. No matter what the flow rate through the tank or the volume of the tank is,
it will always take  0.1$\tau$ before
the concentration in the tank is reduced by 90%.
!enotice
As already mentioned a step size of $10^{-8}$, is probably the smallest we can choose with respect to round off errors, 
but it is smaller than necessary and would lead to large simulation times. 
If it takes 1 second to run the simulation with a step size of $10^{-3}$, it would take $10^5$ seconds or 1 day
with a step size of $10^{-8}$. 
To continue the error analyses, we write our ODE for a general system as:
!bt
\begin{align}
\frac{dy}{dt}=f(y,t),label{eq:ode:ode}
\end{align}
!et
or in discrete form:
!bt
\begin{align}
\frac{y_{n+1}-y_n}{h}-\frac{h}{2}y^{\prime\prime}(\eta_n)&=f(y,t).\nonumber\\
y_{n+1}&=y_n+hf(y,t)+\frac{h^2}{2}y^{\prime\prime}(\eta_n).
\end{align}
!et
$h$ is now the (dimensionless) step size, equal to $\Delta t$ if the derivative is with respect to $t$ or $\Delta x$ if the derivative is respect to $x$ etc. Note that we
have also included the error term related to the numerical derivative, $\eta_n\in[t_n,t_n+h]$. At each step we get an error term,
and the distance between the true solution and our estimate, the *local error*, after $N$ steps is:
!bt
\begin{align}
\epsilon=\sum_{n=0}^{N-1}\frac{h^2}{2}y^{\prime\prime}(\eta_n)&=\frac{h^2}{2}\sum_{n=0}^{N-1}f^\prime(y_n,\eta_n)\simeq\frac{h}{2}\int_{t_0}^{t_f}f^\prime(y,\eta)d\eta\nonumber\\
&=\frac{h}{2}\left[f(y(t_f),t_f)-f(y(t_0),t_0)\right].\label{eq:ode:eu3}
\end{align}
!et
Note that when we replace the sum with an integral in the equation above, this is only correct if the step size is not too large.
From equation (ref{eq:ode:eu3})
we see that even if the error term on the numerical derivative is $h^2$, the local error is proportional to $h$
(one order lower). This is because we accumulate errors for each step.

In the following we specialize to the CSTR, to see if we can gain some additional insight. First we change variables in 
equation (ref{eq:ode:cstr3}): $y=C(t)/C_0$, and $x=t/\tau$, hence:
!bt
\begin{equation}
\frac{dy}{dx}=-y.\label{eq:ode:simple}
\end{equation}
!et
The solution to this equation is $y(x)=e^{-x}$, substituting back for the new variables $y$ and $x$, we reproduce the result in equation (ref{eq:ode:sol}). 
The local error, equation (ref{eq:ode:eu3}), reduces to:
!bt
\begin{align}
\epsilon=\frac{h}{2}\left[-y(x_f)+y(x_0)\right]=\frac{h}{2}\left[1-e^{-x_f}\right],\label{eq:ode:eu4}
\end{align}
!et
we have assumed that $x_0=t_0/\tau=0$. This gives the estimated local error at time $x_f$. For $x_f=0$, the 
numerical error is zero, this makes sense because at $x=0$ we know the exact solution because of the initial conditions. When we move further away from the initial conditions, the
numerical error increases, but equation (ref{eq:ode:eu4}) ensures us that as long as the step size is low enough we can get as
close as possible to the true solution, since the error scales as $h$ (at some point we might run into trouble with round off error in the computer).

Can we prove directly that we get the analytical result? In this 
case it is fairly simple, if we use Eulers method on equation (ref{eq:ode:simple}), we get:
!bt
\begin{align}
\frac{y_{n+1}-y_n}{h}&=-y_nf.\nonumber\\
y_{n+1}&=(1-h)y_n,
\end{align}
!et
or alternatively:
!bt
\begin{align}
y_1&=(1-h)y_0,\nonumber\\
y_2&=(1-h)y_1=(1-h)^2y_0,\nonumber\\
\vdots\nonumber\\
y_{N+1}&=(1-h)^{N}y_0=(1-h)^{x_f/h}y_0.
\end{align}
!et
In the last equation, we have used the the fact the number of steps, $N$, is equal to the simulation time divided by the step size, hence: $N=x_f/h$. From calculus,
the equation above is one of the well known limits for the exponential function: $\lim_{x\to\infty}(1+k/x)^{mx}=e^{mk}$, hence:
!bt
\begin{align}
y_n&=(1-h)^{x_f/h}y_0\to e^{-x_f},
\end{align}
!et   
when $h\to0$. Below is an implementation of the Euler algorithm in this simple case, we also estimate the local error, and global error after $N$ steps. 
@@@CODE src-ode/euler_simple.py
By changing the step size $h$, you can easily verify that the local error systematically increases or decreases proportional to $h$.
Something curious happens with the global error when the 
step size is changed, it does not change very much. The global error involves a second sum over the local error for each step,
which can be approximated as a second integration in equation (ref{eq:ode:eu4}):
!bt
\begin{align}
\epsilon_\text{global}=\frac{1}{2}\int_{0}^{x_f}\left[-y(x)+y(0)\right]dx=\frac{1}{2}\left[x_f+e^{-x_f}-1\right].\label{eq:ode:eu5}
\end{align}
!et
Note that the global error does not go to zero when the step size decreases, which can easily be verified by changing the step size. This is strange, but can be understood
by the following argument: when the step size decreases the local error scales as $\sim h$, but the number of steps scales as $1/h$, so the global error must scale as $h\times 1/h$
or some constant value. Usually it is much easier to control the local error than the global error, this should be kept in mind if you ever encounter a problem where it is 
important control the global error. For the higher order methods that we will discuss later in this chapter, the global error will go to zero when $h$ decreases.   

The answer to our original question, ''What is an appropriate step size?'', will depend on what you want to achieve in terms of local or global error.
In most practical situations you would
specify a local error that is acceptable for the problem under investigation and then choose a step size where the local error always is lower than this value. In the 
next subsection we will investigate how to achieve this in practice.

===== Adaptive step size - Euler's method =====
idx{Eulers method, adaptive step size}
We want to be sure that we use a step size that achieves a certain accuracy in our numerical solution, but at
the same time that we do not waste simulation time using a too low step size. The following approach is similar to the one we derived for the Romberg integration, and
a special case of what is known as Richardson Extrapolation. The method is easily extended to higher order methods. 

We know that Eulers algorithm is accurate to second order. Our estimate of the new value, $y_1^*$  
(where we have used a$\,{}^*$ to indicate that we have used a step size of size $h$), should then be related to the true solution $y(t_1)$ in the following way:
!bt
\begin{align}
y^*_1=y(t_1)+ch^2.\label{eq:ode:aeb0}
\end{align}
!et
The constant $c$ is unknown, but it can be found by taking two smaller steps of size $h/2$. If the steps are not too large, our new estimate
of the value $y_1$ will be related to the true solution as:
!bt
\begin{align}
y_1=y(t_1)+2c\left(\frac{h}{2}\right)^2.\label{eq:ode:aeb1}
\end{align}
!et
The factor 2 in front of $c$ is because we now need to take two steps, and we accumulate a total error of $2c(h/2)^2=ch^2/2$. It might not be completely 
obvious that the constant $c$ should be the same in equation (ref{eq:ode:aeb0}) and (ref{eq:ode:aeb1}). If you are not convinced, there is an exercise at the end 
of the chapter.  
We define:
!bt
\begin{align}
\Delta\equiv y^*_1-y_1=c\frac{h^2}{2}.\label{eq:ode:ae5}
\end{align}
!et
The truncation error in equation (ref{eq:ode:aeb1}) is:
!bt
\begin{align}
\epsilon=y(t_1)-y_1=2c\left(\frac{h}{2}\right)^2=\Delta.\label{eq:ode:ae5b}
\end{align}
!et
Now we have everything we need: We want the local error to be smaller than some predefined
tolerance, $\epsilon^\prime$, or equivalently 
that $\epsilon\le\epsilon^\prime$. 
To achieve this we need to use an optimal step size, $h^\prime$,  that gives us exactly the desired error:
!bt
\begin{align}
\epsilon^\prime=c\frac{{h^\prime}^2}{2}.\label{eq:ode:ae6}
\end{align}
!et
##Note that we use the error term in (ref{eq:ode:ae4}), because we want to use the small step size as our estimate for the next time step. The large
##step size is only used to estimate the error term. 
Dividing equation (ref{eq:ode:ae6}) by equation (ref{eq:ode:ae5b}), we can estimate the optimal step size:
!bt
\begin{align}
h^\prime=h\sqrt{\left|\frac{\epsilon^\prime}{\epsilon}\right|},\label{eq:ode:ae7}
\end{align}
!et
where the estimated error, $\epsilon$, is calculated from equation (ref{eq:ode:ae5b}).
Equation (ref{eq:ode:ae7}) serves two purposes, if the estimated error $\epsilon$ is higher than the tolerance, $\epsilon^\prime$, we have specified it will 
give us an estimate for the step size we should choose in order to achieve a higher accuracy, if on the other hand $\epsilon^\prime > \epsilon$, then we 
get an estimate for the next, larger step. Before the implementation we note, as we did for the Romberg integration, that equation (ref{eq:ode:ae5b}) 
also gives us an estimate for the error term in equation (ref{eq:ode:aeb1}) as an improved estimate of $y_1$. This we get for
free and will make our Euler algorithm accurate to $h^3$, hence the improved Euler step, $\hat{y_1}$, is to *subtract* the error
term from our previous estimate:
!bt
\begin{align}
\hat{y_1}=y_1-\epsilon=2y_1-y_1^*.
\end{align}
!et
Below is an implementation of the adaptive Euler algorithm:
% if FORMAT == 'ipynb':
@@@CODE src-ode/adaptive_euler.py
% endif
% if FORMAT != 'ipynb':
@@@CODE src-ode/adaptive_euler.py fromto: def one_step@# rest of

FIGURE: [fig-ode/adaptive_euler, width=800] The concentration in the tank using adaptive Euler. Number of Euler steps are: 3006, 117, 48 and 36 for the different step sizes. label{fig:ode:adapt_euler}

In figure ref{fig:ode:adapt_euler} the result of the implementation is shown. 
% endif
Note that the number of steps for an accuracy of $10^{-6}$ is only about 3000. Without knowing anything about the accuracy, we would have to assume
that we needed a step size of the order of $h$ in order to reach a local accuracy of $h$ because of equation (ref{eq:ode:eu3}). In the current case,
we would have needed $10^7$ steps, which would lead to unnecessary long simulation times.
!bnotice Local error and bounds
In the previous example we set an absolute tolerance, and required that our estimate $y_n$ always is within a certain bound
of the true  solution $y(t_n)$, i.e. $|y(t_n)-y_n|\le\epsilon^\prime$. This is a very strong demand, and sometimes it makes more 
sense to require that we also accept a relative tolerance proportional to function value. In some areas the solution might have a very large
value, and then another possibility would be to have an $\epsilon^\prime$ that varied with the function value: $\epsilon^\prime = atol +|y|rtol$, where 'atol' is the absolute tolerance and 'rtol' is the relative tolerance. A sensible choice would be to set 'atol=rtol' (e.g. = $10^{-4}$). 

##!bt
##\begin{equation}
##\epsilon^\prime = atol +|y|rtol,
##\end{equation}
##!et  
!enotice

======= Runge-Kutta methods =======
idx{Runge-Kutta}
FIGURE: [fig-ode/rk_fig, width=800] Illustration of the Euler algorithm, and a motivation for using the slope a distance from the $t_n$.label{fig:ode:rk}

The Euler method only have an accuracy of order $h$, and a global error that do not go to zero as the step size decrease. 
The Runge-Kutta methods may be motivated by inspecting the Euler method in figure ref{fig:ode:rk}. The Euler method uses information from
the previous time step to estimate the value at the new time step. The Runge Kutta methods uses the information about the slope between the
points $t_n$ and $t_n+h$. By inspecting figure ref{fig:ode:rk}, we clearly see that by using the slope at $t_n+h/2$ would give us a
significant improvement. The 2. order Runge-Kutta method can be derived by Taylor expanding the solution around $t_n+h/2$, we do this by
setting $t_n+h=t_n+h/2+h/2$:
!bt
\begin{align}
y(t_n+h)=y(t_n+\frac{h}{2})+\frac{h}{2}\left.\frac{dy}{dt}\right|_{t=t_n+h/2}+\frac{h^2}{4}\left.\frac{d^2y}{dt^2}\right|_{t=t_n+h/2}
+\mathcal{O}(h^3).\label{eq:ode:rk1}
\end{align}
!et
Similarly we can expand the solution in $y(t_n)$ about $t_n+h/2$, by setting $t_n=t_n+h/2-h/2$:
!bt
\begin{align}
y(t_n)=y(t_n+\frac{h}{2})-\frac{h}{2}\left.\frac{dy}{dt}\right|_{t=t_n+h/2}+\frac{h^2}{4}\left.\frac{d^2y}{dt^2}\right|_{t=t_n+h/2}
-\mathcal{O}(h^3).\label{eq:ode:rk2}
\end{align}
!et
Subtracting these two equations the term $y(t_n+\frac{h}{2})$, and all even powers in the derivative cancels out:
!bt
\begin{align}
y(t_n+h)&=y(t_n)+h\left.\frac{dy}{dt}\right|_{t=t_n+h/2}+\mathcal{O}(h^3),\nonumber\\
y(t_n+h)&=y(t_n)+hf(y_{n+h/2},t_n+h/2)+\mathcal{O}(h^3).\label{eq:ode:rk3}
\end{align}
!et
In the last equation, we have used equation (ref{eq:ode:ode}). Note that we now have an expression that is very similar to Eulers algorithm,
but it is accurate to order $h^3$. There is one problem, and that is that the function $f$ is to be evaluated at the point $y_{n+1/2}=y(t_n+h/2)$
which we do not know. This can be fixed by using Eulers algorithm: $y_{n+1/2}=y_n+h/2f(y_n,t_n)$. We can do this even if Eulers algorithm has an error term of order $h^2$, because the $f$ in equation (ref{eq:ode:rk3}) is multiplied by $h$, and thus our algorithm is still has an error term of order $h^3$. 
!bnotice The 2. order Runge-Kutta:
!bt
\begin{align}
k_1&=hf(y_n,t_n)\nonumber\\
k_2&=hf(y_n+\frac{1}{2}k_1,t_n+h/2)\nonumber\\
y_{n+1}&=y_n+k_2\label{eq:ode:rk4}
\end{align}
!et
!enotice
Below is a Python implementation of equation (ref{eq:ode:rk4}):
@@@CODE src-ode/rk2.py  fromto: def fm@# rest
FIGURE: [fig-ode/rk2, width=800] The concentration in the tank for different step size $\Delta t$. label{fig:ode:rk2}

In figure ref{fig:ode:rk2} the result of the implementation is shown. 
Note that when comparing Runge-Kutta 2. order with Eulers method,
see figure ref{fig:ode:rk2} and ref{fig:ode:euler},
we of course have 
the obvious result that a larger step size can be taken, without loosing numerical accuracy. It is also worth noting that we can take steps that
is larger than the tank volume. Eulers method failed whenever the time step was larger than one tank volume ($h=t/\tau>1$), whereas the Runge-Kutta 
method finds a physical solution for step sizes lower than twice the tank volume. If the step size is larger, we see that the concentration in the tank
increases, which is clearly unphysical. 
 
The Runge-Kutta fourth order method is one of he most used methods, it is accurate to order $h^4$, and has an error of order $h^5$. The development of the 
algorithm itself is similar to the 2. order method, but of course more involved. We just quote the result:
!bnotice The 4. order Runge-Kutta:
!bt
\begin{align}
k_1&=hf(y_n,t_n)\nonumber\\
k_2&=hf(y_n+\frac{1}{2}k_1,t_n+h/2)\nonumber\\
k_3&=hf(y_n+\frac{1}{2}k_2,t_n+h/2)\nonumber\\
k_4&=hf(y_n+k_3,t_n+h)\nonumber\\
y_{n+1}&=y_n+\frac{1}{6}(k_1+2k_2+2k_3+k_4)\label{eq:ode:rk5}
\end{align}
!et
!enotice
In figure ref{fig:ode:rk4} the result of the Runge-Kutta fourth order is shown, by comparing it to figure ref{fig:ode:rk2} it is easy to see that a larger step size can be chosen.     
#Below is a Python implementation of equation (ref{eq:ode:rk5}):
#@@@CODE src-ode/rk4.py  fromto: def fm@# rest
FIGURE: [fig-ode/rk4, width=800] The concentration in the tank for different step size $\Delta t$. label{fig:ode:rk4}


# % endif

===== Adaptive step size - Runge-Kutta method =====
idx{Runge-Kutta, adaptive step size}
Just as we did with Eulers method, we can implement an adaptive method. The derivation is exactly the same, but this time our method is accurate to
fourth order, hence the error term is of order $h^5$. We start by taking one large step of size $h$, our estimate, $y_1^*$ is related to the true 
solution, $y(t_1)$, in the following way:
!bt
\begin{align}
y^*_1&=y(t_1)+ch^5,\label{eq:ode:rka0}
\end{align}
!et
Next, we take two steps of half the size, $h/2$, hence:
!bt
\begin{align}
y_1&=y(t)+2c\left(\frac{h}{2}\right)^5.\label{eq:ode:rka1}
\end{align}
!et
Subtracting equation (ref{eq:ode:rka0}) and (ref{eq:ode:rka1}), we find an expression similar to equation (ref{eq:ode:ae5}):
!bt
\begin{align}
\Delta\equiv& y_1^*-y_1=c\frac{15}{16}h^5,\label{eq:ode:rka2}
\end{align}
!et
or $c=16\Delta/(15h^5)$. For the Euler scheme, $\Delta$ also happened to be equal to the truncation error, but in this case it is:
!bt
\begin{align}
\epsilon=2c\left(\frac{h}{2}\right)^5=\frac{\Delta}{15}\label{eq:ode:rka5}
\end{align}
!et
we want the local error, $\epsilon$, to be smaller than some tolerance, $\epsilon^\prime$.  
The optimal step size, $h^\prime$,  that gives us exactly the desired error is then:
!bt
\begin{align}
\epsilon^\prime=2c\left(\frac{{h^\prime}}{2}\right)^5.\label{eq:ode:rka3}
\end{align}
!et
Dividing equation (ref{eq:ode:rka3}) by equation (ref{eq:ode:rka5}), we can estimate the optimal step size:
!bt
\begin{align}
h^\prime=h\left|\frac{\epsilon}{\epsilon}\right|^{1/5},\label{eq:ode:rka4}
\end{align}
!et
$\epsilon$ can be calculated from equation (ref{eq:ode:rka5}). In figure ref{fig:ode:adaptive_rk4} the result of an  implementation is shown (see the exercises). 
#Below is an implementation
#% if FORMAT == 'ipynb':
#Run the script below and inspect the results.
#@@@CODE src-ode/rk4.py  
#% endif
#% if FORMAT != 'ipynb':
#@@@CODE src-ode/adaptive_rk4.py  fromto: def fm@# rest
FIGURE: [fig-ode/adaptive_rk4, width=800] The concentration in the tank for different step size $\Delta t$. Number of rk4 steps are: 138, 99, 72 and 66 for the different step sizes and 'rtol=0', for 'rtol=tol' the number of rk4 steps are 81, 72, 63, 63.label{fig:ode:adaptive_rk4}


#% endif 


In general we can use the same procedure any method accurate to order $h^p$, and you can easily verify that:
!bnotice Error term and step size for a $h^p$ method:
idx{adaptive step size}
!bt
\begin{align}
|\epsilon|&=\frac{|\Delta|}{2^p-1}=\frac{|y_1^*-y_1|}{2^p-1},label{eq:eode:1}\\
h^\prime&=\beta h\left|\frac{\epsilon}{\epsilon_0}\right|^{\frac{1}{p+1}},label{eq:eode:2}\\
\hat{y_1}&=y_1-\epsilon=\frac{2^p y_1-y_1^*}{2^{p}-1}label{eq:eode:3},
\end{align}
!et
where $\beta$ is a safety factor $\beta\simeq0.8,0.9$, and you should always be careful that the step size do not become too large so that
the method breaks down. This can happens when $\epsilon$ is very low, which may happen if $y_1^*\simeq y_1$ and/or if $y_1^*\simeq y_1\simeq 0$.  
!enotice
 
===== Conservation of mass =====
A mathematical model of a physical system should always be formulated in such a way that it is
consistent with the laws of nature. In practical situations this statement is usually equivalent to state that
the mathematical model should respect conservation laws. The conservation laws can be conservation of mass, energy, momentum, 
electrical charge, etc. In our
example with the mixing tank, we were able to derive an expression for the concentration of salt out of
the tank, equation (ref{eq:ode:sol}), by *demanding* conservation of mass (see equation (ref{eq:ode:cstr1})).

A natural question to ask is then: If our mathematical model respect conservation of mass, are we sure that our 
solution method respect conservation of mass? We of course expect that
when the grid spacing approaches zero our numerical solution will get closer and closer to the analytical
solution. Clearly when $\Delta x\to 0$, the mass is conserved. So what is the problem? The problem is that in many practical problems
we cannot always have a step size that is small enough to ensure that our solution always is close enough to the analytical 
solution. The physical system we consider might be very complicated (e.g. a model for the earth climate), and our ODE system could
be a very small part of a very big system. A very good test of any code is to investigate if the code respect
the conservation laws. If we know that our implementation respect e.g. mass conservation at the discrete level, we can easily
test mass conservation by summing up all the mass entering, and subtracting the mass out of and present in our system.
If the mass is not conserved exactly, there is a good chance that there is a bug in our implementation.

If we now turn to our system, we know that the total amount of salt in the system when we start is $C(0)V$.
The amount entering is zero, and the amount leaving each time step is $q(t)C(t)\Delta t$. Thus we should
expect that if we add the amount of salt in the tank to the amount that has left the system
we should always get an amount that is equal to the original amount. Alternatively, we expect
$\int_{t_0}^t qC(t)dt + C(t)V -C(0)V=0$. Adding the following code in the `while(ti <= t_final):` loop:
!bc pycod
mout += 0.5*(c_old+c_new)*q*dt
mbal  = (c_new*vol+mout-vol*c_init)/(vol*c_init)
!ec
it is possible to calculate the amount of mass lost (note that we have used the
trapezoidal formula to calculate the integral). In the table below the fraction of mass lost relative to the original
amount is shown for the various numerical methods.

|-----c-------------c-------------c-------------c-------------c-------|
| ﻿$\Delta t$ | $h$         | Euler       | RK 2. order | RK 4. order |
|-----c-------------c-------------c-------------c-------------c-------|
| 900         | 0.9         | -0.4500     | 0.3682      | 0.0776      |
| 500         | 0.5         | -0.2500     | 0.0833      | 0.0215      |
| 100         | 0.1         | -0.0500     | 0.0026      | 0.0008      |
| 10          | 0.01        | -0.0050     | 2.5E-05     | 8.3E-06     |
|---------------------------------------------------------------------|

We clearly see from the table that the Runge-Kutta methods performs better than Eulers method, but
*all of the methods violates mass balance*. 

This might not be a surprise as we know that our numerical solution is always an approximation to the analytical solution. How can 
we then formulate an algorithm that will respect conservation laws at the discrete level? It turns out that for Eulers method it is not
so difficult. Eulers algorithm at the discrete level (see equation (ref{eq:ode:eu0})) is actually a two-step process: first we inject the fresh water while we remove the ``old`` fluid *and then we mix*. By thinking about the
problem this way, it makes more sense to calculate the mass out of the tank as $\sum_kq_kC_k\Delta t_k$. If we in our implementation calculates the mass out of the tank as:
!bc pycod
mout += c_old*q*dt
mbal  = (c_new*vol+mout-vol*c_init)/(vol*c_init)
!ec
We easily find that the mass is exactly conserved at every time for Eulers method. The concentration in the tank will of course not be any closer to the 
analytical solution, but if our mixing tank was part of a much bigger system we could make sure that the mass would always be conserved if we make
sure that the mass out of the tank and into the next part of the system was equal to $qC(t)\Delta t$. 

======= Solving a set of ODE equations =======
What happens if we have more than one equation that needs to be solved? If we continue with our current example, we might be interested in what would happen 
if we had multiple tanks in series. This could be a very simple model to describe the cleaning  of a salty lake by injecting fresh water into it, but at 
the same time this lake was connected to two nearby fresh water lakes, as illustrated in figure ref{fig:ode:cstr3}. The weakest part of the model is the assumption about 
complete mixing, in a practical situation we could enforce complete mixing with the salty water in the first tank by injecting fresh water at multiple point in the 
lake. For the two next lakes, the degree of mixing is not obvious, but salt water is heavier than fresh water and therefore it would sink and mix with the fresh water. Thus
if the flow rate was slow, one might imaging that a more or less complete mixing could occur. Our model then could answer questions like, how long time would it take before most
of the salt water is removed from the first lake, and how much time would it take before most of the salt water was cleared from the whole system? The answer to 
these questions would give practical input on how much and how fast one should inject the fresh water to clean up the system. If we had 
data from an actual system, we could compare our model predictions with data from the physical system, and investigate if our model description was correct. 

FIGURE: [fig-ode/cstr3, width=800] A simple model for cleaning a salty lake that is connected to two lakes down stream. label{fig:ode:cstr3}

For simplicity we will assume that all the lakes have the same volume, $V$. The governing equations follows
as before, by assuming mass balance (equation (ref{eq:ode:mbal})):
!bt
\begin{align}
C_0(t+\Delta t)\cdot V - C_0(t)\cdot V &= q(t)\cdot C_\text{in}(t)\cdot \Delta t - q(t)\cdot C_0(t)\cdot \Delta t,\nonumber\\
C_1(t+\Delta t)\cdot V - C_1(t)\cdot V &= q(t)\cdot C_0(t)\cdot \Delta t - q(t)\cdot C_1(t)\cdot \Delta t,\nonumber\\
C_2(t+\Delta t)\cdot V - C_2(t)\cdot V &= q(t)\cdot C_1(t)\cdot \Delta t - q(t)\cdot C_2(t)\cdot \Delta t.\label{eq:ode:cstr3a}
\end{align}
!et
Taking the limit $\Delta t\to 0$, we can write equation (ref{eq:ode:cstr3a}) as:
!bt
\begin{align}
V\frac{dC_0(t)}{dt} &= q(t)\left[C_\text{in}(t) - C_0(t)\right],\label{eq:ode:cstr3b}\\
V\frac{dC_1(t)}{dt} &= q(t)\left[C_0(t) - C_1(t)\right],\label{eq:ode:cstr3c}\\
V\frac{dC_2(t)}{dt} &= q(t)\left[C_1(t) - C_2(t)\right].\label{eq:ode:cstr3d}
\end{align}
!et 
Let us first derive the analytical solution: Only the first tank is filled with salt water $C_0(0)=C_{0,0}$, $C_1(0)=C_2(0)=0$, and $C_\text{in}=0$. 
The solution to equation (ref{eq:ode:cstr3b}) is, as before $C_0(t)=C_{0,0}e^{-t/\tau}$, inserting this equation into equation (ref{eq:ode:cstr3c}) we find:
!bt
\begin{align}
V\frac{dC_1(t)}{dt} &= q(t)\left[C_{0,0}e^{-t/\tau} - C_1(t)\right]\label{eq:ode:cstr3e},\\
\frac{d}{dt}\left[e^{t/\tau}C_1\right]&= \frac{C_{0,0}}{\tau}\label{eq:ode:cstr3f},\\
C_1(t)&=\frac{C_{0,0}t}{\tau}e^{-t/\tau}\label{eq:ode:cstr3g}.
\end{align}
!et
where we have use the technique of "integrating factors":"https://en.wikipedia.org/wiki/Integrating_factor" when going from equation (ref{eq:ode:cstr3e}) to (ref{eq:ode:cstr3f}). 
Inserting equation (ref{eq:ode:cstr3g}) into equation (ref{eq:ode:cstr3d}), solving the equation in a similar way as for $C_1$ we find:
!bt
\begin{align}
V\frac{dC_2(t)}{dt} &= q(t)\left[\frac{C_{0,0}t}{\tau}e^{-t/\tau} - C_2(t)\right],\label{eq:ode:cstr3h}\\
\frac{d}{dt}\left[e^{t/\tau}C_2\right]&= \frac{C_{0,0}t}{\tau},\label{eq:ode:cstr3i}\\
C_2(t)&=\frac{C_{0,0}t^2}{2\tau^2}e^{-t/\tau}.\label{eq:ode:cstr3j}
\end{align}
!et
The numerical solution follows the exact same pattern as before if we introduce a vector notation. Before doing that, we rescale the time $t\to t/\tau$ and the concentrations,
 $\hat{C_i}=C_i/C_{0,0}$ for $i=0,1,2$, hence:
!bt
\begin{align}
\frac{d}{dt}
\left(
\begin{array}{c} 
 \hat{C_0}(t)\\
 \hat{C_1}(t)\\
 \hat{C_2}(t)
 \end{array}
 \right)
&=\left(
\begin{array}{c} 
 \hat{C_\text{in}}(t) - \hat{C_0}(t)\\
 \hat{C_0}(t) - \hat{C_1}(t)\\
 \hat{C_1}(t) - \hat{C_2}(t)
 \end{array}
 \right),\nonumber
 \\
 \frac{d\mathbf{\hat{C}}(t)}{dt}&=\mathbf{f}(\mathbf{\hat{C}},t).
\end{align}

!et
In figure ref{fig:ode:rk4_2} results of an implementation using Runge-Kutta 4. order is shown (see exercises for more details).
#Below is an implementation using the Runge Kutta 4. order method:
# % if FORMAT == 'ipynb':
#Run the script below and inspect the results.
#@@@CODE src-ode/rk4_2.py  
#% endif
#% if FORMAT != 'ipynb':
#@@@CODE src-ode/rk4_2.py  fromto: def fm@# rest
FIGURE: [fig-ode/rk4_2, width=800] The concentration in the tanks. label{fig:ode:rk4_2}


#% endif 

======= Stiff sets of ODE  and implicit methods =======
idx{stiff equations}
idx{implicit method}
As already mentioned a couple of times, our system could be part of a much larger system. To illustrate this, let us now assume that we have two 
tanks in series. The first tank is similar to our original tank, but the second tank is a sampling tank, 1000 times smaller.   

FIGURE: [fig-ode/cstr2, width=800] A continuous stirred tank model with a sampling vessel. label{fig:ode:cstr2}

The governing equations can be found by requiring mass balance for each of the tanks (see equation (ref{eq:ode:mbal}):
!bt
\begin{align}
C_0(t+\Delta t)\cdot V_0 - C_0(t)\cdot V_0 &= q(t)\cdot C_\text{in}(t)\cdot \Delta t - q(t)\cdot C_0(t)\cdot \Delta t.\nonumber\\
C_1(t+\Delta t)\cdot V_1 - C_1(t)\cdot V_1 &= q(t)\cdot C_0(t)\cdot \Delta t - q(t)\cdot C_1(t)\cdot \Delta t.
\label{eq:ode:cstr2a}
\end{align}
!et 
Taking the limit $\Delta t\to 0$, we can write equation (ref{eq:ode:cstr2a}) as:
!bt
\begin{align}
V_0\frac{dC_0(t)}{dt} &= q(t)\left[C_\text{in}(t) - C_0(t)\right].\label{eq:ode:cstr2bb}\\
V_1\frac{dC_1(t)}{dt} &= q(t)\left[C_0(t) - C_1(t)\right].\label{eq:ode:cstr2b}
\end{align}
!et
Assume that the first tank is filled with seawater, $C_0(0)=C_{0,0}$, and fresh water is flooded into the tank, i.e. $C_\text{in}=0$. Before we start to consider a numerical
solution, let us first find the analytical solution: As before the solution for the first tank (equation (ref{eq:ode:cstr2bb})) is:
!bt
\begin{equation}
C_0(t)=C_{0,0}e^{-t/\tau_0},
\end{equation}
!et
where $\tau_0\equiv V_0/q$. Inserting this equation into equation (ref{eq:ode:cstr2b}), we get:
!bt
\begin{align}
\frac{dC_1(t)}{dt} &= \frac{1}{\tau_1}\left[C_{0,0}e^{-t/\tau_0} - C_1(t)\right],\nonumber\\
\frac{d}{dt}\left[e^{t/\tau_2}C_1\right]&= \frac{C_{0,0}}{\tau_1}e^{-t(1/\tau_0-1/\tau_1)}\label{eq:ode:cstr2c},\\
C_1(t)&=\frac{C_{0,0}}{1-\frac{\tau_1}{\tau_0}}\left[e^{-t/\tau_0}-e^{-t/\tau_1}\right],\label{eq:ode:cstr2d}
\end{align}
!et
where $\tau_1\equiv V_1/q$.

Next, we will consider the numerical solution. You might think that these equations are more simple to solve numerically than the equations with three tanks
in series discussed in the previous section. Actually, this system is much harder to solve with the methods we have discussed so far.
The reason is that there are now *two time scales* in the system, $\tau_1$ and $\tau_2$. The smaller tank sets a strong limitation on the step size
we can use, because we should never use step sizes larger than a tank volume. Thus if you use the code in the previous section to solve equation
(ref{eq:ode:cstr2bb}) and (ref{eq:ode:cstr2b}), it will not find the correct solution, unless the step size is lower than $10^{-3}$. Equations of this type
are known as *stiff*. 
!bnotice Stiff equations
There is no precise definition of ''stiff'', but it is used to describe a system of differential equations, where the numerical solution becomes unstable unless
a very small step size is chosen. Such systems occurs because there are several (length, time) scales in the system, and the numerical solution is constrained
by the shortest length scale. You should always be careful on how you scale your variables in order to make the system dimensionless, which is of 
particular importance when you use adaptive methods. 
!enotice

These types of equations are often encountered in practical applications. If our sampling tank was extremely small, maybe $10^6$ smaller than the chemical
reactor, then we would need a step size of the order of $10^{-8}$ or lower to solve the system. This step size is so low that we easily run into trouble
with round off errors in the computer. In addition the simulation time is extremely long.  How do we deal with this problem? The solution is actually
quite simple. The reason we run into trouble is that we require that the concentration leaving the tank must be a small perturbation of the old one.
This is not necessary, and it is best illustrated with Eulers method. As explained earlier Eulers method can be viewed as a two step process:
first we inject a volume (and remove an equal amount: $qC(t)\Delta t$), and then we mix. Clearly when we try to remove more than what is left, we run into
trouble. What we want to do is to remove or flood much more than one tank volume through the tank during one time step, this can be achieved by
$q(t)C(t)\Delta t\to q(t+\Delta t)C(t+\Delta t)\Delta t$. The term $q(t+\Delta t)C(t+\Delta t)\Delta t$ now represents
*the mass out of the system during the time step $\Delta t$*.

The methods we have considered so far are known as *explicit*, whenever we replace the solution in the right hand side of our algorithm with $y(t+\Delta t)$
or ($y_{n+1}$),
the method is known as *implicit*. Implicit methods are always stable, meaning that we can take as large a time step that we would like, without
getting oscillating solution. It does not mean that we will get a more accurate solution, actually explicit methods are usually more accurate.

!bnotice Explicit and Implicit methods
Explicit methods are often called *forward* methods, as they use only information from the previous step to estimate the next value. The explicit
methods are easy to implement, but get into trouble if the step size is too large. Implicit methods are often called *backward* methods as the next 
step cannot be calculated directly from the previous solution, usually a non-linear equation has to be solved. Implicit methods are generally much
more stable, but the price is often lower accuracy. Many commercial simulators uses implicit methods extensively because they are stable, and stability is often viewed
as a much more important criterion than numerical accuracy.   
!enotice
Let us consider our example further, and for simplicity use the implicit Eulers method:
!bt
\begin{align}
{C_0}_{n+1}V_0 - {C_0}_nV_0 &= q(t+\Delta t){C_\text{in}}_{n+1}\Delta t -
q(t+\Delta t){C_0}_{n+1}\Delta t.\nonumber\\
{C_1}_{n+1}V_1 - {C_1}_nV_1 &= q(t+\Delta t){C_0}_{n+1}\Delta t - q(t+\Delta t){C_1}_{n+1}\Delta t.
\label{eq:ode:cstr2ai}
\end{align}
!et 
This equation is equal to equation (ref{eq:ode:cstr2a}), but the concentrations on the right hand side are now evaluated at the next time step.
The immediate problem is now that we have to find an expression for $C_{n+1}$ that is given in terms of known variables. In most cases one needs
to use a root finding method, like Newtons method, in order to solve equation (ref{eq:ode:cstr2ai}). In this case it is straight forward to show:
!bt
\begin{align}
{C_0}_{n+1}&=\frac{{C_0}_n + \frac{\Delta t}{\tau_0}{C_\text{in}}_{n+1}}{1+\frac{\Delta t}{\tau_0}},\nonumber\\
{C_2}_{n+1}&=\frac{{C_1}_n + \frac{\Delta t}{\tau_1}{C_0}_{n+1}}{1+\frac{\Delta t}{\tau_1}}.\label{eq:ode:cstri1}
\end{align}
!et
In figure ref{fig:ode:euler_imp} the result of the implementation is shown, note that quite large step sizes can be used without inducing non physical results.
#Below is an implementation
#% if FORMAT == 'ipynb':
#Run the script below and inspect the results.
#@@@CODE src-ode/euler_imp_2.py  
#% endif
#% if FORMAT != 'ipynb':
#@@@CODE src-ode/euler_imp_2.py  fromto: def fm@# rest
FIGURE: [fig-ode/euler_imp, width=800] The concentration in the tanks for $h=0.01$.label{fig:ode:euler_imp}


#% endif 

##======= Bibliography =======
===== Exercise: Truncation error in Euler's method =====
In the following we will take a closer look at the adaptive Eulers algorithm and show that the 
constant $c$ is indeed the same in equation (ref{eq:ode:aeb0}) and (ref{eq:ode:aeb1}). 
The true solution $y(t)$, obeys the following equation:
!bt
\begin{align}
\frac{dy}{dt}&=f(y,t),\label{eq:ode:ay}
\end{align}
!et
and Eulers method to get from $y_0$ to $y_1$ by taking one (large) step, $h$ is:
!bt
\begin{align}
y^*_1&=y_0+hf(y_0,t_0),\label{eq:ode:ae0}
\end{align}
!et
We will also assume (for simplicity) that in our starting point $t=t_0$, the numerical solution, $y_0$, is equal to the true solution, $y(t_0)$, hence $y(t_0)=y_0$.
!bsubex
Show that when we take one step of size $h$ from $t_0$ to $t_1=t_0+h$, $c=y^{\prime\prime}(t_0)/2$ in equation (ref{eq:ode:aeb0}).
!bans 
The local error, is the difference between the numerical solution and the true solution:
!bt
\begin{align}
\epsilon^*&=y(t_0+h)-y_{1}^*=y(t_0)+y^{\prime}(t_0)h+\frac{1}{2}y^{\prime\prime}(t_0)h^2+\mathcal{O}(h^3)\nonumber\\
&-\left[y_0+hf(y_0,t_0+h)\right],
\end{align}
!et
where we have used Taylor expansion to expand the true solution around $t_0$, and equation (ref{eq:ode:ae0}).
Using equation (ref{eq:ode:ay}) to replace $y^\prime(t_0)$ with $f(y_0,t_0)$, we find:
!bt
\begin{align}
\epsilon^*=&y(t_0+h)-y_{1}^*=\frac{1}{2}y^{\prime\prime}(t_0)h^2\equiv ch^2,
\end{align}
!et
hence $c=y^{\prime\prime}(t_0)/2$.
!eans
!esubex
!bsubex
Show that when we take two steps of size $h/2$ from $t_0$ to $t_1=t_0+h$, Eulers algorithm is:
!bt
\begin{align}
y_{1}&=y_{0}+\frac{h}{2}f(y_0,t_0)+\frac{h}{2}f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2).
\end{align}
!et
!bans
!bt
\begin{align}
y_{1/2}&=y_0+\frac{h}{2}f(y_0,t_0),\label{eq:ode:ae1b}\\
y_{1}&=y_{1/2}+\frac{h}{2}f(y_{1/2},t_0+h/2),\label{eq:ode:ae2b}\\
y_{1}&=y_{0}+\frac{h}{2}f(y_0,t_0)+\frac{h}{2}f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2).\label{eq:ode:ae3b}
\end{align}
!et
Note that we have inserted
equation (ref{eq:ode:ae1b}) into equation (ref{eq:ode:ae2b}) to arrive at equation (ref{eq:ode:ae3b}). 
!eans
!esubex
!bsubex
Find an expression for the local error when using two steps of size $h/2$, and show that the local error is: $\frac{1}{2}ch^2$
!bans
!bt
\begin{align}
\epsilon&=y(t_0+h)-y_{1}=y(t_0)+y^{\prime}(t_0)h+\frac{1}{2}y^{\prime\prime}(t_0)h^2+\mathcal{O}(h^3)\nonumber\\
&-\left[y_{0}+\frac{h}{2}f(y_0,t_0)+\frac{h}{2}f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2)\right].\label{eq:ode:ay5b}
\end{align}
!et
This equation is slightly more complicated, due to the term involving $f$ inside the last parenthesis, we can use Taylor expansion to expand it about $(y_0,t_0)$:
!bt
\begin{align}
&f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2)=f(y_0,t_0)\nonumber\\
&+\frac{h}{2}\left[f(y_0,t_0)\left.\frac{\partial f}{\partial y}\right|_{y=y_0,t=t_0}
+\frac{h}{2}\left.\frac{\partial f}{\partial t}\right|_{y=y_0,t=t_0}\right]+\mathcal{O}(h^2).\label{eq:ode:ay2b}
\end{align}
!et
It turns out that this equation is related to $y^{\prime\prime}(t_0,y_0)$, which can be seen by differentiating equation (ref{eq:ode:ay}):
!bt
\begin{align}
\frac{d^2y}{dt^2}&=\frac{df(y,t)}{dt}=\frac{\partial f(y,t)}{\partial y}\frac{dy}{dt}+\frac{\partial f(y,t)}{\partial t}
=\frac{\partial f(y,t)}{\partial y}f(y,t)+\frac{\partial f(y,t)}{\partial t}.\label{eq:ode:ay3b}
\end{align}
!et
Hence, equation (ref{eq:ode:ay2b}) can be written:
!bt
\begin{align}
f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2)=f(y_0,t_0)+\frac{h}{2}y^{\prime\prime}(t_0,y_0),\label{eq:ode:ay4b}
\end{align}
!et
hence the truncation error in equation (ref{eq:ode:ay5b}) can finally be written:
!bt
\begin{align}
\epsilon=&y(t_1)-y_{1}=\frac{h^2}{4} y^{\prime\prime}(y_0,t_0)=\frac{1}{2}ch^2,\label{eq:ode:ae4b}
\end{align}
!et
!eans
!esubex
!bsol
The local error, is the difference between the numerical solution and the true solution:
!bt
\begin{align}
\epsilon^*&=y(t_0+h)-y_{1}^*=y(t_0)+y^{\prime}(t_0)h+\frac{1}{2}y^{\prime\prime}(t_0)h^2+\mathcal{O}(h^3)\nonumber\\
&-\left[y_0+hf(y_0,t_0+h)\right],
\end{align}
!et
where we have used Taylor expansion to expand the true solution around $t_0$, and equation (ref{eq:ode:ae0}).
Using equation (ref{eq:ode:ay}) to replace $y^\prime(t_0)$ with $f(y_0,t_0)$, we find:
!bt
\begin{align}
\epsilon^*=&y(t_0+h)-y_{1}^*=\frac{1}{2}y^{\prime\prime}(t_0)h^2\equiv ch^2,
\end{align}
!et
where we have ignored terms of higher order than $h^2$, and defined $c$ as $c=y^{\prime\prime}(t_0)/2$. Next we take two steps of size $h/2$ to
reach $y_1$:  
!bt
\begin{align}
y_{1/2}&=y_0+\frac{h}{2}f(y_0,t_0),\label{eq:ode:ae1}\\
y_{1}&=y_{1/2}+\frac{h}{2}f(y_{1/2},t_0+h/2),\label{eq:ode:ae2}\\
y_{1}&=y_{0}+\frac{h}{2}f(y_0,t_0)+\frac{h}{2}f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2).\label{eq:ode:ae3}
\end{align}
!et
Note that we have inserted
equation (ref{eq:ode:ae1}) into equation (ref{eq:ode:ae2}) to arrive at equation (ref{eq:ode:ae3}). The truncation error in this case is, as before:
!bt
\begin{align}
\epsilon&=y(t_0+h)-y_{1}=y(t_0)+y^{\prime}(t_0)h+\frac{1}{2}y^{\prime\prime}(t_0)h^2+\mathcal{O}(h^3)\nonumber\\
&-\left[y_{0}+\frac{h}{2}f(y_0,t_0)+\frac{h}{2}f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2)\right].\label{eq:ode:ay5}
\end{align}
!et
This equation is slightly more complicated, due to the term involving $f$ inside the last parenthesis, we can use Taylor expansion to expand it about $(y_0,t_0)$:
!bt
\begin{align}
&f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2)=f(y_0,t_0)\nonumber\\
&+\frac{h}{2}\left[f(y_0,t_0)\left.\frac{\partial f}{\partial y}\right|_{y=y_0,t=t_0}
+\left.\frac{\partial f}{\partial t}\right|_{y=y_0,t=t_0}\right]+\mathcal{O}(h^2).\label{eq:ode:ay2}
\end{align}
!et
It turns out that this equation is related to $y^{\prime\prime}(t_0,y_0)$, which can be seen by differentiating equation (ref{eq:ode:ay}):
!bt
\begin{align}
\frac{d^2y}{dt^2}&=\frac{df(y,t)}{dt}=\frac{\partial f(y,t)}{\partial y}\frac{dy}{dt}+\frac{\partial f(y,t)}{\partial t}
=\frac{\partial f(y,t)}{\partial y}f(y,t)+\frac{\partial f(y,t)}{\partial t}.\label{eq:ode:ay3}
\end{align}
!et
Hence, equation (ref{eq:ode:ay2}) can be written:
!bt
\begin{align}
f(y_0+\frac{h}{2}f(y_0,t_0),t_0+h/2)=f(y_0,t_0)+\frac{h}{2}y^{\prime\prime}(t_0,y_0),\label{eq:ode:ay4}
\end{align}
!et
hence the truncation error in equation (ref{eq:ode:ay5}) can finally be written:
!bt
\begin{align}
\epsilon=&y(t_1)-y_{1}=\frac{h^2}{4} y^{\prime\prime}(y_0,t_0)=\frac{1}{2}ch^2,\label{eq:ode:ae4}
\end{align}
!et
!esol
##!bhint
##"Wolframalpha": "http://wolframalpha.com" can perhaps
##compute the integral.
##!ehint

##!bsubex
##Subexercises are numbered a), b), etc.
##!esubex
##!bans 
##Short answer to subexercise a).
##!eans

##!bremarks
##At the very end of the exercise it may be appropriate to summarize
##and give some perspectives. The text inside the `!bremarks` and `!eremarks`
##directives is always typeset at the end of the exercise.
##!eremarks


## By default, answers, solutions, and hints are typeset as paragraphs. The command-line arguments --without_answers and 
## --without_solutions turn off output of answers and solutions, respectively, except for examples.
## Publish (https://bitbucket.org/logg/publish is used to
## handle references. The line below specifies the name of
## the Publish database file (see the doconce manual for details).

##BIBFILE: ../papers.pub


!split
========= Monte Carlo Methods =========
label{ch:mc}
###### Content provided under a Creative Commons Attribution license, CC-BY 4.0; code under MIT License. (c)2018 Aksel Hiorth

======= Monte Carlo methods =======
Monte Carlo methods are named after the Monte Carlo Casino in Monaco,
this is because at its core it uses random numbers to solve problems.
Monte Carlo methods are quite easy to program, and they are
usually much more intuitive than a theoretical approach. 
# If we would
# like to find the probability to get at least 5 on three dices after 5 throws
# there are methods from statistics that could tell us the probability.
# Using the Monte Carlo method, we would get the computer to pick a
#random integer between 1 and 6, three times, to represent one throw of
#the dices bla bla.
# later in this chapter 
# Usually  Usually we use differential equations to describe physical systems, the solution to these equations are continuous functions. In order for these solutions 
# to be useful, they require that the differential equation describes our physical sufficiently. In many practical cases we have no control over many

# of the parameters entering the differential equation, or stated differently *our system is not deterministic*. This means that there could be some random
# fluctuations, occurring at different times and points in space, that we have no control over. In a practical situation we might would like to investigate how these fluctuations would
# affect the behavior of our system. A 

# o "ray  tracing":"https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-rendering-practical-example"

======= Monte Carlo integration  ''hit and miss'' =======
idx{Monte Carlo integration}
Let us start with a simple illustration of the Monte Carlo Method (MCM), Monte Carlo integration. To the left
in figure ref{fig:mc:mci} there is a shape of a pond. Imagine that we wanted to estimate the area of the pond, how could
we do it? Assume further that you did not have your phone or any other electronic devices to help you. 

FIGURE: [fig-mc/mci, width=400 frac=1.0] Two ponds to illustrate the MCM. label{fig:mc:mci}

One possible approach is: First to walk around it, and put up some bands (illustrated by the black dotted line).
Then estimate the area inside the bands (e.g. 4$\times$3 meters). Then
we would know that the area was less than e.g. 12m$^2$. Finally,
and this is the difficult part, throw rocks *randomly* inside the
bands. The number of rocks hitting the pond divided by the total
number rocks thrown should be equal to the area of the pond divided by
the total area inside the bands, i.e. the area of the pond should be
equal to:
!bt
\begin{equation}
A\simeq\text{Area of rectangle}\times\frac{\text{Number of rocks hitting the pond}}{\text{Number of rocks thrown}}.
label{eq:mc:mci}
\end{equation}
!et
It is important that we throw the rocks randomly, otherwise  equation (ref{eq:mc:mci}) is not correct. Now, let us
investigate this in more detail, and use the idea of throwing rocks to estimate $\pi$. To the right in figure ref{fig:mc:mci},
there is a well known shape, a circle. The area of the circle is $\pi d^2/4$, and the shape is given by $x^2+y^2=d^2/4$. Assume that
the circle is inscribed in a square with sides of $d$. To throw rocks randomly inside the square, is equivalent pick random numbers
with coordinates $(x,y)$, where $x\in[0,d]$ and $y\in[0,d]$. We want all the $x-$ and $y-$values to be chosen with equal probability,
which is equivalent to pick random numbers from a *uniform* distribution. Below is a Python implementation:
@@@CODE src-mc/pi.py fromto:import@NN

In the table below, we have run the code for $d=1$ and different values of $N$. 

|------c--------------c--------------c--------------c-------|
| MC estimate  | Error        | $N$          | $1/\sqrt{N}$ |
|------c--------------c--------------c--------------c-------|
| 3.04         | -0.10159     | 10$^2$       | 0.100        |
| 3.176        | $\,$0.03441  | 10$^3$       | 0.032        |
| 3.1584       | $\,$0.01681  | 10$^4$       | 0.010        |
| 3.14072      | -0.00087     | 10$^5$       | 0.003        |
|-----------------------------------------------------------|

We clearly see that a fair amount of rocks or numbers needs to be used in order to get a good estimate. If you run this code several
times you will see that the results changes from time to time. This
makes sense as the coordinates $x$ and $y$ are chosen at random.

!bnotice A note on performance
The code above is not efficient and for MC simulations we usually have to use many random numbers. Instead of
!bc pypro
for k in range(0,N):
        x=np.random.uniform(0,d)
        y=np.random.uniform(0,d)
!ec
one should avoid loops, and take advantage of build in functions in Numpy
!bc pypro
x=np.random.uniform(0,d,size=N)
y=np.random.uniform(0,d,size=N)
!ec
!enotice
===== Random number generators =====
idx{random number generators}
idx{Mersenne Twister}
There are much to be said about random number generators. The MCM depends on a good random number generator, otherwise we cannot use the results from
statistics to develop our algorithms. Below, we briefly summarize some important points that you should be aware of:

o Random number generators are generally of two types: *hardware random number generator* (HRNG) or *pseudo random number generator* (PRNG).
o HRNG uses a physical process to generate random numbers, this could atmospheric noise, radioactive decay, microscopic fluctuations, which is translated to an electrical signal. The electrical signal is converted to a digital number (1 or 0), by sampling the random signal random numbers can be generated. The HRNG are often named *true random number generators*, and their main use are in *cryptography*.
o PRNG uses a mathematical algorithm to generate an (apparent) random sequence. The algorithm uses an initial number, or a *seed*,  to start the sequence of random number. The sequence is deterministic, and it will generate the same sequence of numbers if the same seed is used. At some point the algorithm will reproduce itself, i.e. it will have certain period. For some seeds the period may be much shorter.
o Many of the PRNG are not considered to be cryptographically secure, because if a sufficiently long sequence of random numbers are generated from them, the rest of the sequence can be predicted. 
o Python uses the "Mersenne Twister":"https://en.wikipedia.org/wiki/Mersenne_Twister" algorithm to generate random numbers, and has a period of $2^{19937}−1\simeq4.3\cdot10^{6001}$. It is not considered to be cryptographically secure.

In Pythons `random.uniform` function, a random seed is chosen each time the code is run, but
if we set e.g. `random.seed(2)`, the code will generate the same sequence of numbers each time it is called. 

===== Encryption  =====
idx{encryption}
This section can be skipped as it is not relevant for development of
the numerical algorithms, but it is a good place to explain the basic
idea behind encryption of messages. A very simple, but not a very good
encryption, is to replace all the letters in the alphabet with a
number, e.g. A=1, B=2, C=3, etc. This is what is know as a
*substitution cipher*, it does not need to be a number it could be a
letter, a sequence of letters, letters and numbers etc. The receiver
can solve the code by doing the reverse operation.

The
weakness of this approach is that it can fairly easily be cracked, by
the following approach: First we analyze the encrypted message and find the frequency of each of the symbols.
Assume that we know that the message is written in English, then the
frequency of symbols can be compared with the frequency of
letters from a known English text (the most common is `E` (12$\%$), then `T`
(9$\%$), etc.). We would then guess that the most occurring symbol
probably is an `E` or `T`. When some of the letters are in place, we
can compare with the frequency of words, and so on. By the help of
computers this process can easily be automated.

A much better algorithm is *to not replace a letter with the same
symbol*. To make it more clear, consider our simple example where A=1, B=2,
C=3, $\ldots$. If we know say that A=1 but we add a *random number*,
then our code would be much harder to crack. Then the letter A
could be several places in the message but represented as a complete different
number. Thus we could not use the frequency of the various symbols to
crack the message.

How can the receiver decrypt the message? Obviously, it can be done if
both the sender and receiver have the same sequence of random numbers (or the *key*).
This can be achieved quite simple with random number generators, if we
know the seed  used we can generate the same sequence of
random numbers. If Alice where to send a message to Bob without Eve
knowing what it is, Alice and Bob could agree to send a message that
was scrambled using Pythons Mersenne-Twister algorithm with seed=2.

The weakness of this approach is of course that Eve could convince
Alice or Bob to give her the seed or the key. Another possibility is
that Eve could write a program that tested
different random number generators and seeds to decipher the message.
How to avoid this?

Let us assume that Alice and Bob each had their own
hardware random generator. This generator generated random numbers that was truly
random, and the sequence could not be guessed by any outsider. Alice
do not want to share her key (sequence of random numbers) with Bob,
and Bob would not share his key with Alice. How can they send a
message without sharing the key? One possible way of doing it is as
follows: Alice write a message and encrypt it with her key, she send
the message to Bob. Bob then encrypt the message with his key, he
sends it back to Alice. Alice then decrypt the message with her key
and send it back to Bob. Now, Bob can decrypt it with his own key and
read the message. The whole process can be visualized by thinking of
the message as box with the message. Alice but her padlock on the box
(keeps her key for her self), she sends the message to Bob. Bob locks
the box with his padlock, now there are two padlocks on the box. He
sends the box back to Alice, Alice unlocks her padlock with her key,
and sends it back to Bob. The box now only has Bob's key, he can
unlock the box and read the message. The important point is that the
box was never unlocked throughout the transaction, and Alice and Bob
never had to share the key with anyone. 

===== Errors on Monte Carlo integration and the binomial distribution  =====
idx{Monte Carlo Integration, error}
idx{binomial distribution}
How many rocks do we need to throw in order to reach a certain accuracy? To answer this question we need some results from statistics. Our problem of calculating the integral is closely related to the *binomial distribution*. When we throw a rock one of two things can happen i) the rock falls into the water, or ii) it falls outside the pond. If we denote the probability that the rock falls into the pond as $p$, then the probability that it falls outside the pond, $q$, has to be $q=1-p$.
This is simply because there are no other possibilities and the sum of the two probabilities has to be one: $p+q=p+(1-p)=1$. The binomial distribution is given by:
!bt
\begin{equation}
p(k)=\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}.
label{eq:mc:bin}
\end{equation}
!et
$p(k)$ is the probability that an event happens $k$ times after $n$ trials. The mean, $\mu$, and the variance, $\sigma^2$, of the binomial distribution is:
!bt
\begin{align}
\mu&=\sum_{k=0}^{n-1}kp(k)=np, label{eq:mc:binm}\\
\sigma^2&=\sum_{k=0}^{n-1}(k-\mu)^2p(k)=np(1-p). label{eq:mc:bins}
\end{align}
!et
!bnotice Mean and variance
idx{mean}
idx{variance}
The mean of a distribution is simply the *sum* divided by the *count*,
the symbol $\mu$ or $\overline{x}$ is usually used. For $N$ observations, $x_i$,
$\mu=\sum_i x_i/N$. The mean is just an average, it could e.g. be the sum of all the heights of
students in the class divided by the number of students. The mean
would then be the average height of all the students in the class.

The variance is calculated by taking the difference between each of
the data points and the mean, square it, and sum over all data points.
Usually the symbol $\sigma^2$ is used, $\sigma^2=\sum_i(\mu-x_i)^2/N$.
The variance measures the spread in the data. Furthermore, it squares the
distance between the mean and the individual observations, meaning
that the points lying far a way from $\mu$ contributes more to the
variance. 
!enotice
Before we proceed, we should take a moment and look a little more into
the meaning of equation (ref{eq:mc:bin}) to appreciate its usefulness.  A classical example of the use of the binomial formula is to toss a coin, if the coin is fair it will have an equal probability of giving us a head or tail, hence $p=1/2$. Equation (ref{eq:mc:bin}), can answer questions like: ''What is the probability to get only heads after 4 tosses?''. Let us calculate this answer using equation (ref{eq:mc:bin}), the number of tosses is 4, the number of success is 4 (only heads each time)
!bt
\begin{equation}
p(k=4)=\frac{4!}{4!(4-4)!}\frac{1}{2}^4(1-\frac{1}{2})^{4-4}=\frac{1}{2^4}=\frac{1}{16}.
label{eq:mc:coin}
\end{equation}
!et
''What is the probability to get three heads in four tosses?'', using
the same equation, we find:
!bt
\begin{equation}
p(k=3)=\frac{4!}{3!(4-3)!}\frac{1}{2}^3(1-\frac{1}{2})^{4-3}=\frac{4}{2^4}=\frac{1}{4}.
label{eq:mc:coin2}
\end{equation}
!et
In figure ref{fig:mc:coin}, all the possibilities are shown. The
number of possibilities are 16, and there are only one possibility
that we get only heads, i.e. the probability is 1/16 as calculated in
equation (ref{eq:mc:coin}). In the figure we also see that there are 4
possible ways we can get three heads, hence the probability is
4/16=1/4 as calculated in equation (ref{eq:mc:coin2}).

FIGURE: [fig-mc/coin, width=400 frac=1.0] The famous Norwegian Moose coin, and possible outcomes of four coin flips in a row. label{fig:mc:coin}

Now, let us return to our original question, ''What is the error on our
estimate of the integral, when using the MCM?''. Before we continue we
should also clean up our notation, let $I$ be the value of the true
integral, $A$ is our *estimate* of the integral, and $I_N$ is the area
of the rectangle. First, let us show
that the mean or expectation value of the binomial distribution is
related to our estimate of the area of the pond or the circle, $A$. In our case we draw $n=N$
random numbers, and $k$ times the coordinate falls inside the circle,
equation (ref{eq:mc:binm}) tells us that the mean value is $np$. $p$
is the probability that the coordinate is within the area to be
integrated, hence as before $p$ is equal to the area to be integrated
divided by the area of the total domain, thus:
!bt
\begin{equation}
\mu=np=N\frac{A}{I_N},
\end{equation}
!et
or
!bt
\begin{equation}
A=I_N\frac{\mu}{N}.
\end{equation}
!et
Equation (ref{eq:mc:bins}), gives us an estimate of the variance of
the mean value. Assume for simplicity that we can replace $1-p\simeq
p$, this is of course only correct if the area of the rectangle is
twice as big as our pond, but we are only interested in an
estimate of the error, hence $\sigma^2\simeq np^2$. We can now use the
standard deviation as an estimate of the error of our integral:
!bt
\begin{align}
I&\simeq I_N\frac{\mu\pm\sigma}{n}=I_N\frac{Np\pm \sqrt{N}p}{N}\nonumber\\
&\simeq I_N(p\pm \frac{p}{\sqrt{N}})=A\pm \frac{A}{\sqrt{N}}.
label{eq:mc:mcmf}
\end{align}
!et
In the last equation we have replaced $p$ with $A/I_N$. 
Hence, the error of our integral is inversely proportional to the
square root of the number of points. 

===== The mean value method =====
idx{Monte Carlo integration, mean value}
How does our previous method compare with some of our standard methods,
like the midpoint rule? The error for the MC method scales as
$1/\sqrt{N}$, in our previous error estimates we used the step length,
$h$, as an indicator of the accuracy, and not $N$. The s$N$ is
related to the number of points as $h=(b-a)/n$, where $b$ and $a$ are
the integration limit. Thus our MCM scales as $1/\sqrt{n}\sim
h^{1/2}$, this is actually worse than the midpoint or trapezoidal
rule, which scaled as $h$.

The MCM can be improved. We will first describe the mean value method.
In the last section we calculated the area of
a circle by picking random numbers inside a square and estimated the
fraction of points inside the circle. This is equivalent to calculate
the area of a half circle, and multiply with 2:
!bt
\begin{equation}
I=2\int_{-d/2}^{d/2}\sqrt{(d/2)^2-x^2}dx=\frac{\pi d^2}{4}.
label{eq:mc:Is}
\end{equation}
!et
The half-circle is now centered at the origin. Before we proceed we
write our integral in a general form as:
!bt
\begin{equation}
I=\int_a^bf(x)dx.
label{eq:mc:I1}
\end{equation}
!et
Instead of counting the number of points inside the curve given by
$f(x)$, we could instead use the mean of
the function, which we will define as $\overline{f}=\sum_k f(x_k)/N$:
!bt
\begin{equation}
I=\int_a^bf(x)dx\simeq\overline{f}\int_a^bdx=(b-a)\overline{f}
=\frac{(b-a)}{N}\sum_{k=0}^{N-1}f(x_k).
label{eq:mc:I2}
\end{equation}
!et
Note that this formula is similar to the midpoint rule, but now the
function is not evaluated at the midpoint, but at several points and
we use the average value. 

FIGURE: [fig-mc/mcint, width=400 frac=1.0] Illustration of MC integration for $N=4$. label{fig:mc:int}

Below is an implementation:
@@@CODE src-mc/pi2.py fromto: import@N=

In the table below we have compared the mean value method with the
''hit and miss'' method. We see
that the mean value method performs somewhat better, but there are
some random fluctuations and in some cases it performs poorer. 

|---c---------c---------c---------c---------c-----|
| MC-mean | Error   | MC      | Error   | $N$     |
|---c---------c---------c---------c---------c-----|
| 3.1706  | 0.0290  | 3.1600  | 0.0184  | 10$^2$  |
| 3.1375  | -0.0041 | 3.1580  | 0.0164  | 10$^3$  |
| 3.1499  | 0.0083  | 3.1422  | 0.0006  | 10$^4$  |
| 3.1424  | 0.0008  | 3.1457  | 0.0041  | 10$^5$  |
| 3.1414  | -0.0002 | 3.1422  | 0.0006  | 10$^6$  |
|-------------------------------------------------|

We also see that in this case the error scales as $1/\sqrt{N}$.
!bnotice 
At first sight it might be a little counter intuitive that if we
multiply the average value of the function with the size of the
integration domain we get an estimate for the integral, as illustrated
in the top figure in figure ref{fig:mc:int}. A different, but
equivalent way, of viewing the mean value method is the lower figure
in figure ref{fig:mc:int}. For each random point we choose, we
multiply with the area $(b-a)/N$, as $N$ increases the area decreases
and the mean value method approaches the midpoint algorithm. The
reason the mean value method performs poorer is that we do not sample
the function at regular intervals. The "law of large
numbers":"https://en.wikipedia.org/wiki/Law_of_large_numbers", ensures
that our estimate approach the true value of the integral.
!enotice

===== Basic properties of probability distributions =====
The MCM is closely tied to statistics, and it is important to have a
basic understanding of probability density functions (PDF). In
the previous section, we used a random number generator to give us
random numbers in an interval. All the numbers are picked with an
equal probability. Another way to state this is to say that: we *draw*
random numbers from an *uniform* distribution. Thus all the numbers
are drawn with an equal probability $p$. What is the value of $p$?
That value is given from another property of PDF's, all PDF's must
be *normalized* to 1. This is equivalent to state that the sum of all
probabilities must be equal to one. Thus for a general PDF, $p(x)$, we
must have:
!bt
\begin{equation}
\int_{-\infty}^{\infty}p(x)dx=1.
label{eq:mc:pdf1}
\end{equation}
!et
A uniform distribution, $p(x)=U(x)$, is given by:
!bt
\begin{equation}
U(x)=\begin{cases} \frac{1}{b-a}, \text{ for }x\in[a,b]\\
0, \text{ for } x<a \text{ or }x>b,
\end{cases}
label{eq:mc:pdfu}
\end{equation}
!et
you can easily verify that $\int_{-\infty}^{\infty}U(x)=1$. In the MCM
we typically evaluate *expectation values*. The expectation
value, $E[f]$, for a function is defined:
!bt
\begin{equation}
E[f]\equiv\int_{-\infty}^{\infty}f(x)p(x)dx\simeq\frac{1}{N}\sum_{k=0}^{N-1}f(x_k),
label{eq:mc:ef}
\end{equation}
!et
specializing to a uniform distribution, $p(x)=U(x)$, we get:
!bt
\begin{equation}
E[f]=\int_{-\infty}^{\infty}f(x)U(x)dx=\frac{1}{b-a}\int_a^bf(x)dx.
label{eq:mc:efu}
\end{equation}
!et
Rearranging this equation, we see that we can write the above equation
as:
!bt
\begin{equation}
\int_a^bf(x)dx=(b-a)E[f]\simeq(b-a)\frac{1}{N}\sum_{k=0}^{N-1}f(x_k).
label{eq:mc:efu2}
\end{equation}
!et
This equation is the same as equation (ref{eq:mc:I2}), but in the
previous section we never explained why the expectation value of
$f(x_k)$ was equal to the integral. The derivation above shows that
$\int_a^bf(x)dx$ is equal to the expectation value of $f(x)$ 
only under the condition that *we pick numbers from a uniform distribution*.

To make this a bit more clearer, let us specialize to $f(x)=x$. In
this case the expectation value is equal to the mean:
!bt
\begin{equation}
E[x]=\mu=\int_{-\infty}^\infty xp(x)=\frac{1}{N}\sum_kx_k.
label{eq:mc:mean}
\end{equation}
!et
In this case we also find that the numerical error scales as $N^{-1/2}$, from the definition of the variance
!bt
\begin{equation}
\sigma=\sqrt{\frac{1}{N}\sum_k(f(x_i)-\langle f\rangle)^2}\sim\frac{1}{\sqrt{N}},
label{eq:mc:varf}
\end{equation}
!et
!bnotice Why would we or anyone use MC integration?
Monte Carlo integration performs much poorer than any of our previous
methods. So why should we use it, or when should we use it? The
strength of MC integration is only apparent when there is a large
number of dimensions, as we will see in the next section. 
!enotice

===== Example: Monte Carlo integration of a hyper sphere =====
The volume of a hyper sphere is known:
!bt
\begin{equation}
V(R)=\frac{\pi^{D/2}}{\Gamma(D/2+1)}R^D,
label{eq:mc:hyp}
\end{equation}
!et
where $D$ is the number of dimensions $\Gamma(D/2+1)$ is the gamma
function, if $n$ is an integer then $\Gamma(n)=(n-1)!$ and
$\Gamma(n+1/2)=(2n)!/(4^nn!)\sqrt{\pi}$. You can easily verify that
for $D=2,3$, $V(R)=\pi R^2, 4/3\pi R^3$, respectively.

In the case of MC integration, we simply place the sphere inside a cube, and then count
the number of points that hits inside the hyper sphere:

@@@CODE src-mc/hypersphere3.py fromto: def mc_nballII@def mc_nball_sampling

Notice how easy it is to do MC integration in any number of dimensions. If you run the code above for $D=3,\ldots,9$ you will see that the error is the same regardless of dimensions i.e. it only scales proportional to $N^{-1/2}$ and *not the number of dimensions*.  

How can we extend the traditional methods to more dimensions? One way of doing it is
to call a one dimensional integration routine several times. 
The volume of a hypersphere can be written
!bt
\begin{equation}
V(R)=\int_{-R}^{R}\int_{-\sqrt{R^2-x_0^2}}^{\sqrt{R^2-x_0^2}}
\int_{-\sqrt{R^2-x_0^2-x_1^2}}^{\sqrt{R^2-x_0^2-x_1^2}}\cdots
\int_{-\sqrt{R^2-x_0^2-x_1^2-\cdots -x_{n-2}^2}}^{\sqrt{R^2-x_0^2-x_1^2-\cdots -x_{n-2}^2}}dx_0dx_1dx_2\cdots dx_{n-1}.
label{eq:mc:hypV}
\end{equation}
!et
To simplify the notation lets look a little bit closer at $D=3$ ($n=2$). We can always do the last integration
regardless of the number of dimensions
!bt
\begin{equation}
V(R)=2\int_{-R}^{R}\int_{-\sqrt{R^2-x_0^2}}^{\sqrt{R^2-x_0^2}}
\sqrt{R^2-x_0^2-x_1^2}dx_0dx_1.
label{eq:mc:hypV2}
\end{equation}
!et
This equation can be rewritten as
!bt
\begin{align}
V(R)&=2\int_{-R}^{R}F(x_0)dx_0,\nonumber\\
F(x_0)&\equiv\int_{-\sqrt{R^2-x_0^2}}^{\sqrt{R^2-x_0^2}}\sqrt{R^2-x_0^2-x_1^2}dx_1,
label{eq:mc:hypV3}
\end{align}
!et
when integrating $F(x_0)$, we do it by dividing the x-axis from $-R$
to $R$ into $N$ equal slices as before. We also need to evaluate
$F(x_0)$ for each value of $x_0$, which is slightly more tricky, see
figure ref{fig:mc:2Dint} for an illustration. 

FIGURE: [fig-mc/2Dint, width=400 frac=1.0] Illustration of a 2D integration to evaluate the volume of a sphere.label{fig:mc:2Dint}

The multi dimensional
integral is done by placing a box around the sphere, and divide this
box into $N\times N$ equal boxes. If start the integration
at $x=-R$, $F(-R)=0$, because the integrand is zero. If we move one
step to the left, we need to integrate from $y=-R$ to $y=R$. We see
from the figure to the right in figure ref{fig:mc:2Dint} that the
function is not defined for two first points. Thus we need to make
sure that if we are outside the integration bounds the function is
zero. Below is an implementation that uses recursive function calls

@@@CODE src-mc/hypersphere4.py fromto: def dd_trapez@def mc_n

!bwarning Recursive functions
idx{recursive functions}
Recursive implementation is very elegant, and more transparent, but it comes with a price. The reason is that when a function is called additional memory is allocated to store the local variables. If we where to calculate $100!$, 100 copies of the variable $n$ are created, whereas using a loop only one variable is created. Each time a function is called more memory is allocated, and if the recursive calls are too many it might cause memory overflow. If you try to call `fact_rec(1000)`, Python will give an error, because the maximum number of recursions are reached, it can be changed by:
!bc
import sys
sys.setrecursionlimit(1500)
!ec
!ewarning

!bnotice Error Analysis in higher dimensions
In the chapter about numerical integration, we did an error analysis
on the trapezoidal rule and found that it scaled as $h^2$. As we see
from the example above, a higher order integration is simply to do a
series of 1D integrations in all the dimensions, thus the error term
should be $h_{x_0}^2+h_{x_1}^2+\cdots+h_{x_{d-1}}^2$. If we use the
same spatial resolution in all dimensions, then the overall error
scale as $h^2$. If we let $n$ denote the number of points in each
directions, $h\sim 1/n$, the total number of points used is $N=n\times
n\cdots n=n^d$. Thus, the error term scales as $h^2\sim N^{-2/d}$, and
we see that if $d\geq 4$, the MC integration is expected to perform
better. 
!enotice

===== Exercise: The central limit theorem =====
idx{central limit theorem}
label{ex:mc:norm}
##file=solution.pdf
The central limit theorem is a corner stone in statistics, and it is
the reason for why the normal distribution is so widely used. The
central limit theorem states that if we calculate the average of an
independent random variable, the *average will be distributed
according to a normal distribution*. Not that the central limit
theorem does not state anything about the distribution of the original
variable. We will not prove the central limit theorem, but illustrate
it with two examples. 

# !bsol
# !esol

!bsubex
First we will investigate a random variable that follows a *uniform
distribution*. Write a Python function that returns the average of
$N$ uniformly distributed numbers in $[0,1]$.
!bsol
@@@CODE src-mc/clt.py fromto: def@def hist
!esol
!esubex

!bsubex
Calculate the average $M$ times and make a histogram of the values.
!bsol
@@@CODE src-mc/clt.py fromto: def hist@average3
!esol
!esubex

!bsubex
Repeat the above exercise for a Poisson distribution.
!bsol
@@@CODE src-mc/clt.py fromto: def average3@hist2
!esol
!esubex

!bremarks
It is quite remarkable that the distribution of the average values
from both a uniform and Poisson distribution follows a normal
distribution. The general
"proof":"https://en.wikipedia.org/wiki/Central_limit_theorem"
is not that complicated, but the ramifications are large. The central
limit theorem explains why it makes sense to use the standard
deviation as a measure of confidence for the mean value.
!eremarks
===== Exercise: Birthday Paradox =====
idx{birthday paradox}
label{ex:mc:BP}
##file=solution.pdf
The human mind is not good at logical thinking, and if we use our
intuition we often get into trouble. A well known example is the
''Birthday Paradox'', it is simply to answer the following question:
''How many randomly selected people do we need in order that there is
a 50\% chance that two of them have birthday on the same date?'' 

# !bsol
# !esol

!bsubex
Write a Python function that pick a random date
# subexercise...

!bsol
Below are two examples, the first one picks a date, while the second
one just picks a random day at year.  
@@@CODE src-mc/rd.py fromto: from@def No
!esol
!esubex

!bsubex
Write a function that takes as argument, number of persons in a group,
and returns 1 if two of them has birthday on the same date and 0
otherwise.

!bsol
. 
@@@CODE src-mc/rd.py fromto: def No@def BP

!esol

!esubex

!bsubex

Write a function that returns the probability that two people in a
group of $p$ persons have birthday on the same day, and determine how
many people we need to have a probability of 50\%.

!bsol
In order to get some statistics, we need to sample $N$ groups and
return the fraction of groups that had two persons with the same birthday.
@@@CODE src-mc/rd.py fromto: def BP@def /N
By trial an error, we find that 23 persons is needed in order to have
a probability of 

!esol

!esubex
##https://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb
##https://nbviewer.jupyter.org/url/norvig.com/ipython/ProbabilityParadox.ipynb
## standard deviation, the variance, the confidence interval, 95% confidence
## how to compute confidence interval - emperical rule - assume mean
## estimate is zero, no bias, distribution of errors are normal

!split
======= References =======

BIBFILE: ../chapters/papers.pub
