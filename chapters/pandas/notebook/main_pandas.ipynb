{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1b9c6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- dom:TITLE: Organizing data: pandas -->\n",
    "# Organizing data: pandas\n",
    "**Aksel Hiorth**\n",
    "University of Stavanger\n",
    "\n",
    "Date: **Apr 4, 2023**\n",
    "\n",
    "<!-- Common Mako variables and functions -->\n",
    "\n",
    "# What is Pandas?\n",
    "Pandas is a Python package that among many things are used to handle data, and perform operations on groups of data. It is built on top of Numpy, which makes it easy to perform vectorized operations. Pandas is written by Wes McKinney, and one of it objectives is according to the official website [ '' providing fast, flexible, and expressive data structures designed to make working with ''relational'' or ''labeled'' data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python''](https://pandas.pydata.org/). Pandas also has excellent functions for reading and writing excel and csv files.  An excel file is read directly into memory in what is called a `DataFrame` in Pandas. A DataFrame is a two dimensional object where data are typically stored in column or row format. Pandas has a lot of functions that can be used to calculate statistical properties of the data frame as a whole. In this chapter we will focus on basic data manipulation, stuff you might do in excel, but can be done much faster in Python and Pandas.\n",
    "\n",
    "# Creating a data frame\n",
    "In the following we will assume that you have imported pandas, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d38cf7b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dc54a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From empty DataFrame\n",
    "This is perhaps the most basic way of creating a DataFrame, first we create an empty DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15464db5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd661c",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Variable name.**\n",
    "\n",
    "Note that we often use `df` as a variable name for a DataFrame, this is a choice, but it is a usually a good choice as someone else reading the code could infer from a name that `df` is a DataFrame. If you need more than one DataFrame variable you could use `df1`, `df2`, etc. or even better to use a descriptive name, `df_sales_data`.\n",
    "\n",
    "\n",
    "Next, we can add columns to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643965b6",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['ints']=[0,1,2,3]\n",
    "df['floats']=[4.,5.,6.,7.]\n",
    "df['tools']=['hammer','saw','rock','nail']\n",
    "print(df) # to view data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638f341",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that all columns needs to have the same size.\n",
    "\n",
    "**`pd.Series()`.**\n",
    "\n",
    "Even if we initialize the DataFrame column with a list, the command `type(df['a'])` will tell you that the column in the DataFrame are of type `pd.Series()`. Thus the fundamental objects in Pandas are of type `Series`. Series are more flexible, and it is possible to calculate `df['a']/df['b']`, whereas `[0,1,2,3]/[4,5,6,7]` is not possible.\n",
    "\n",
    "\n",
    "\n",
    "## Create DataFrame from dictionary\n",
    "A DataFrame can be quite easily be generated from a dictionary. A dictionary is a special data structure, where an unique key is associated with a data type (key:value pair). In this case, the key would be the title of the column, and the value would be the data in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e268cb",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_dict={'ints':[0,1,2,3], 'floats':[4.,5.,6.,7.],\n",
    "'tools':['hammer','saw','rock','nail']\n",
    "}\n",
    "df=pd.DataFrame(my_dict)\n",
    "print(df) # to view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff490c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From file\n",
    "Assume you have some data organized in excel or in a csv file. The csv file could just be a file with column data, they could be separated by a comma or tab\n",
    "\n",
    "<!-- dom:FIGURE: [fig-pandas/covid_comb.png, width=800 frac=1.0] Official Covid-19 data, and example of files (left) tab separated (right) excel file. <div id=\"fig:file\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:file\"></div>\n",
    "\n",
    "<img src=\"fig-pandas/covid_comb.png\" width=800><p style=\"font-size: 0.9em\"><i>Figure 1: Official Covid-19 data, and example of files (left) tab separated (right) excel file.</i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83de9f12",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('../data/corona_data.xlsx') # excel file\n",
    "df2=pd.read_csv('../data/corona_data.dat',sep='\\t') # csv tab separated file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6da1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "If the excel file has several sheets, you can give the sheet name directly, e.g. `df=pd.read_excel('file.xlsx',sheet_name=\"Sheet1\")`, for more information see the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html). \n",
    "\n",
    "**Accessing files.**\n",
    "\n",
    "Accessing files from python can be painful. If excel files are open in excel, Windows will not allow a different program to access it - always remember to close the file before opening it. Sometimes we are not in the right directory, to check which directory you are in, you can always do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cddfe66f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd()) # prints current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d901875",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can easily save the data frame to excel format and open it in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "033c0065",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_excel('covid19.xlsx', index=False) # what happens if you put index=True?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a46e2",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Index column.**\n",
    "\n",
    "Whenever you create a DataFrame Pandas by default create an index column, it contains an integer for each row starting at zero. It can be accessed by `df.index`, and it is also possible to define another column as index column.\n",
    "\n",
    "\n",
    "\n",
    "## Accessing data in  DataFrames\n",
    "\n",
    "### Selecting columns\n",
    "\n",
    "If we want to pick out a specific column we can access it in the following ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60c2668",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# following two are equivalent\n",
    "time=df['TIME'] # by the name, alternatively\n",
    "time=df[df.columns[1]]\n",
    "# following two are equivalent\n",
    "time=df.loc[:,['TIME']] # by loc[] if we use name\n",
    "time=df.iloc[:,1] # by iloc, pick column number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b2847",
   "metadata": {
    "editable": true
   },
   "source": [
    "The `loc[]` and `iloc[]` functions also allows for list slicing, one can then pick e.g. every second element in the column by `time=df.iloc[::2,1]` etc. The difference is that `loc[]` uses the name, and `iloc[]` the index (usually an integer). \n",
    "\n",
    "Why several ways of doing the same operation? It turns out that although we are able to extract what we want with these operations, they are of different type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54eda073",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(type(df['TIME']))\n",
    "print(type(df.loc[:,['TIME']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d793c",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Selecting rows\n",
    "\n",
    "When selecting rows in a DataFrame, we can use the `loc[]` and `iloc[]` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3395e436",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# pick column number 0 and 1\n",
    "time=df.loc[0:1,:] # by loc[] \n",
    "time=df.iloc[0:2,:] # by iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5721b",
   "metadata": {
    "editable": true
   },
   "source": [
    "**`pandas.DataFrame.loc` vs `pandas.DataFrame.iloc`.**\n",
    "\n",
    "When selecting rows `loc` and `iloc` they behave differently, `loc` includes the endpoints (in the example above both row 0 and 1), whereas `iloc` includes the starting point and up to 1 minus the endpoint.\n",
    "\n",
    "\n",
    "\n",
    "### Challenges when accessing columns or rows\n",
    "\n",
    "**Special characters.**\n",
    "\n",
    "Sometimes when reading files from excel, headers may contains invisible characters like newline `\\n` or tab `\\t` or maybe Norwegian special letters that have not been read in properly. If you have problem accessing a column by name do `print(df.columns)` and check if the name matches what you would expect.\n",
    "\n",
    "\n",
    "\n",
    "If the header names has unwanted white space, one can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db6634b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '') # all white spaces\n",
    "df.columns = df.columns.str.lstrip() # the beginning of string\n",
    "df.columns = df.columns.str.rstrip() # end of string\n",
    "df.columns = df.columns.str.strip()  # both ends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8c25e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Similarly for unwanted tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f661c1fc",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace('\\t', '') # remove tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6acac91",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you want to make sure that the columns does not contain any white spaces, one can use [`pandas.Series.str.strip()`](https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.Series.str.strip.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb91e5b2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['LOCATION']=df['LOCATION'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176000f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Time columns not parsed properly\n",
    "\n",
    "If you have dates in the file (as in our case for the `TIME` column), you should check if they are in the `datetime` format and not read as `str`.\n",
    "\n",
    "**`datetime`.**\n",
    "\n",
    "The `datetime` library is very useful for working with dates. Data types of the type `datetime` (or equivalently `timestamp` used by Pandas) contains both date and time in the format `YYYY-MM-DD hh:mm:ss`. We can initialize a variable, `a`, by `a=datetime.datetime(2022,8,30,10,14,1)`, to access the hour we do `a.hour`, the year by `a.year` etc. It also easy to increase e.g. the day by one by doing `a+datetime.timedelta(days=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54f4fe9",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "time=df['TIME']\n",
    "# what happens if you set\n",
    "# time=df2['TIME'] #i.e df2 is from pd.read_csv ?\n",
    "print(time[0])\n",
    "print(time[0]+dt.timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9932ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "The code above might work fine or in some cases a date is parsed as a string by Pandas, then we need to convert that column to the correct format. If not, we get into problems if you want to plot data vs the time column.\n",
    "\n",
    "Below are two ways of converting the `TIME` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6470dd7",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2['TIME']=pd.to_datetime(df2['TIME'])\n",
    "# just for testing that everything went ok\n",
    "time=df2['TIME']\n",
    "print(time[0])\n",
    "print(time[0]+dt.timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163cc6f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Another possibility is to do the conversion when reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5efa0851",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2=pd.read_csv('../data/corona_data.dat',sep='\\t',parse_dates=['TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e48fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you have a need to specify all data types, to avoid potential problems down the line this can also be done. First create a dictionary, with column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da6907a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "types_dict={\"LOCATION\":str,\"TIME\":str,\"ELAPSED_TIME_SINCE_OUTBREAK\":int,\"CONFIRMED\":int,\"DEATHS\":int,\"RECOVERED\":int}\n",
    "df2=pd.read_csv('../data/corona_data.dat',sep='\\t',dtype=types_dict,parse_dates=['TIME']) # set data types explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9ee89",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the time data type is `str`, but we explicitly tell Pandas to convert those to `datetime`.\n",
    "\n",
    "## Filtering and visualizing data\n",
    "### Boolean masking\n",
    "\n",
    "Typically you would select rows based on a criterion, the syntax in Pandas is that you enter a series containing `True` and `False` for the rows you want to pick out, e.g. to pick out all entries with Afghanistan we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f1deab",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[df['LOCATION'] == 'Afghanistan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e36a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "The innermost statement `df['LOCATION'] == 'Afghanistan'` gives a logical vector with the value `True` for the five last elements and `False` for the rest. Then we pass this to the DataFrame, and in one go the unwanted elements are removed. It is also possible to use several criteria, e.g. only extracting data after a specific time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c35b1121",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[(df['LOCATION'] == 'Afghanistan') & (df['ELAPSED_TIME_SINCE_OUTBREAK'] > 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e4bfc",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the parenthesis are necessary, otherwise the logical operation would fail.\n",
    "\n",
    "### Plotting a DataFrame\n",
    "\n",
    "Pandas has built in plotting, by calling [`pandas.DataFrame.plot`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b25db853",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2=df[(df['LOCATION'] == 'Afghanistan')]\n",
    "df2.plot()\n",
    "#try \n",
    "#df2=df2.set_index('TIME')\n",
    "#df2.plot() # what is the difference?\n",
    "#df2.plot(y=['CONFIRMED','DEATHS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea197c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Performing mathematical operations on DataFrames\n",
    "When performing mathematical operations on DataFrames there are at least two strategies\n",
    "* Extract columns from the DataFrame and perform mathematical operations on the columns using Numpy, leaving the original DataFrame intact\n",
    "\n",
    "* To operate directly on the data in the DataFrame using the Pandas library\n",
    "\n",
    "**Speed and performance.**\n",
    "\n",
    "Using Pandas or Numpy should in principle be equally fast. The advice is to not worry about performance before it is necessary. Use the methods you are confident with, and try to be consistent. By consistent, we mean that if you have found one way of doing a certain operation stick to that one and try not to implement many different ways of doing the same thing.\n",
    "\n",
    "\n",
    "\n",
    "We can always access the individual columns in a DataFrame by the syntax `df['column_name']`. \n",
    "### Example: mathematical operations on DataFrames\n",
    "\n",
    "1. Create a DataFrame with one column (`a`) containing ten thousand random uniformly distributed numbers between 0 and 1 (checkout [`np.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html))\n",
    "\n",
    "2. Add two new columns: one which all elements of `a` is squared and one where the sine function is applied to column `a`\n",
    "\n",
    "3. Calculate the inverse of all the numbers in the DataFrame\n",
    "\n",
    "4. Make a plot of the results (i.e. `a` vs `a*a`, and `a` vs `sin(a)`)\n",
    "\n",
    "### Solution\n",
    "\n",
    "1. First we make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a14cf699",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "N=10000\n",
    "a=np.random.uniform(0,1,size=N)\n",
    "df=pd.DataFrame() # empty DataFrame\n",
    "df['a']=a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b15fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you like you could also try to use a dictionary. Next, we add the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c2a354f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['b']=df['a']*df['a'] # alternatively np.square(df['a'])\n",
    "df['c']=np.sin(df['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b1edc",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. The inverse of all the numbers in the DataFrame can be calculated by simply doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9695f33d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "1/df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2ed8a",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note: you can also do `df+df` and many other operations on the whole DataFrame.\n",
    "\n",
    "1. To make plots there are several possibilities. Personally, I tend most of the time to use the  [`matplotlib`](https://matplotlib.org/) library, simply because I know it quite well, but Pandas has a great deal of very simple methods you can use to generate nice plots with very few commands.\n",
    "\n",
    "**Matplotlib:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1276f33",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df['a'],df['b'], '*', label='$a^2$')\n",
    "plt.plot(df['a'],df['c'], '^', label='$\\sin(a)$')\n",
    "plt.legend() \n",
    "plt.grid() # make small grid lines\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff25f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Pandas plotting:**\n",
    "First, let us try the built in plot command in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d1ceb5b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f032ba3",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you compare this plot with the previous plot, you will see that Pandas plots all columns versus the index columns, which is not what we want. But, we can set `a` to be the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f08148d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=df.set_index('a')\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f223d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can also make separate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c8c00d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11babda7",
   "metadata": {
    "editable": true
   },
   "source": [
    "or scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66313618",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=df.reset_index()\n",
    "df.plot.scatter(x='a',y='b')\n",
    "df.plot.scatter(x='a',y='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86780c",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that we have to reset the index, otherwise there are no column named `a`. \n",
    "\n",
    "## Grouping, filtering and aggregating data\n",
    "Whenever you have a data set, you would like to do some exploratory analysis. That typically means that you would like to group, filter or aggregate data. Perhaps, we would like to plot the covid data not per country, but the data as a function of dates. Then you first must sort the data according to date, and then sum all the occurrences on that particular date. For all of these purposes we can use the [`pd.DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html ) function. To sort our DataFrame on dates and sum the occurrences we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ae9383a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.groupby('TIME').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c31e5a",
   "metadata": {
    "editable": true
   },
   "source": [
    "Another case could be that we wanted to find the total number of confirmed, deaths and recovered cases in the full database. As always in Python it can be done in different ways, by e.g. splitting the database into individual countries and do  `df[['CONFIRMED','DEATHS','RECOVERED']].sum()` or accessing each column individually and sum each of them e.g. `np.sum(df['CONFIRMED'])`.  However, with the `groupby()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce4338e7",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.groupby('LOCATION').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb9c56",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here Pandas sum all columns with the same location, and drop columns that cannot be summed. By doing `df.groupby('LOCATION').mean()` or `df.groupby('LOCATION').std()` we can find the mean or standard deviation (per day).\n",
    "\n",
    "## Simple statistics in Pandas\n",
    "At the end it is worth mentioning the built in methods `pd.DataFrame.mean`, `pd.DataFrame.median`, `pd.DataFrame.std` which calculates the mean, median and standard deviation on the columns in the DataFrame where it make sense (i.e. avoid strings and dates). To get all these values in one go (and a few more) on can also use `pd.DataFrame.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "256dacae",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d85533",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Joining two DataFrames\n",
    "### Appending DataFrames\n",
    "\n",
    "The DataFrame with the Covid-19 data in the previous section could have been created from two separate DataFrames, using [`concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html). First, create two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d598b86",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "a=dt.datetime(2020,2,24,23,59)\n",
    "b=dt.datetime(2020,2,7,23,59)\n",
    "my_dict1={'LOCATION':7*['Afghanistan'], \n",
    "'TIME':[a+dt.timedelta(days=i) for i in range(7)],\n",
    "'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1, 2, 3, 4, 5, 6],\n",
    "'CONFIRMED':7*[1],\n",
    "'DEATHS':7*[0],\n",
    "'RECOVERED': 7*[0]}\n",
    "my_dict2={'LOCATION':6*['Diamond Princess'], \n",
    "'TIME':[b+dt.timedelta(days=i) for i in range(6)],\n",
    "'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1, 2, 3, 4, 5],\n",
    "'CONFIRMED':[61, 61, 64, 135, 135, 175],\n",
    "'DEATHS':6*[0],\n",
    "'RECOVERED': 6*[0]}\n",
    "df1=pd.DataFrame(my_dict1)\n",
    "df2=pd.DataFrame(my_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c988b93f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2])\n",
    "print(df) # to view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebdfc8",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you compare this DataFrame with the previous one, you will see that the index column is different. This is because when joining two DataFrames Pandas does not reset the index by default, doing `df=pd.concat([df1,df2],ignore_index=True)` resets the index. It is also possible to join DataFrames column vise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b865b4ed",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209c45a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Merging DataFrames\n",
    "\n",
    "In the previous example we had two non overlapping DataFrames (separate countries and times). It could also be the case that some of the data was overlapping e.g. continuing with the Covid-19 data, one could assume that there was one data set from one region and one from another region in the same country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59dea203",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_dict1={'LOCATION':7*['Diamond Princess'], \n",
    "'TIME':[b+dt.timedelta(days=i) for i in range(7)],\n",
    "'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1, 2, 3, 4, 5, 6],\n",
    "'CONFIRMED':7*[1],\n",
    "'DEATHS':7*[0],\n",
    "'RECOVERED': 7*[0]}\n",
    "my_dict2={'LOCATION':2*['Diamond Princess'], \n",
    "'TIME':[b+dt.timedelta(days=i) for i in range(2)],\n",
    "'ELAPSED_TIME_SINCE_OUTBREAK':[0, 1],\n",
    "'CONFIRMED':[60, 60],\n",
    "'DEATHS':2*[0],\n",
    "'RECOVERED': 2*[0]}\n",
    "df1=pd.DataFrame(my_dict1)\n",
    "df2=pd.DataFrame(my_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e018a",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we do `pd.concat([df1,df2])` we will simply add all values after each other. What we want to do is to sum the number of confirmed, recovered and deaths for the same date. This can be done in several ways, but one way is to use [`pd.DataFrame.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html).You can specify the columns to merge on, and choose `outer` which is union (all data from both frames) or `inner` which means the intersect (only data which you merge on that exists in both frames), see [figure 2](#fig:pandas:join) for a visual image.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-pandas/fig_join.png, width=800 frac=1.0] The result of using `how=outer, inner, left`, or `right` in `pd.DataFrame.merge()`. <div id=\"fig:pandas:join\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:pandas:join\"></div>\n",
    "\n",
    "<img src=\"fig-pandas/fig_join.png\" width=800><p style=\"font-size: 0.9em\"><i>Figure 2: The result of using <code>how=outer, inner, left</code>, or <code>right</code> in <code>pd.DataFrame.merge()</code>.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "To be even more specific, after performing the commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e356a320",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df1.merge(df2,on=['LOCATION','TIME'],how='outer')\n",
    "df1.merge(df2,on=['LOCATION','TIME'],how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd666e5",
   "metadata": {
    "editable": true
   },
   "source": [
    "Clearly in this case we need to choose `outer`. In the merge process pandas adds an extra subscript `_x` and `_y` on columns that contains the same header name. We also need to sum those, which can be done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6533ef4",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=df1.merge(df2,on=['LOCATION','TIME'],how='outer')\n",
    "cols=['CONFIRMED','DEATHS', 'RECOVERED']\n",
    "for col in cols:\n",
    "    df[col]=df[[col+'_x',col+'_y']].sum(axis=1) # sum row elements\n",
    "    df=df.drop(columns=[col+'_x',col+'_y']) # remove obsolete columns\n",
    "# final clean up\n",
    "df['ELAPSED_TIME_SINCE_OUTBREAK']=df['ELAPSED_TIME_SINCE_OUTBREAK_x']\t\t\n",
    "df=df.drop(columns=['ELAPSED_TIME_SINCE_OUTBREAK_y','ELAPSED_TIME_SINCE_OUTBREAK_x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cedb9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Working with folders and files\n",
    "When working with big data sets you might want to split data into smaller sets, and also write them to different folders (or files) to view each individually in excel. Working with files and folders in a way that will work on any kind of platform has always been a challenge, but it is greatly simplified by the [Pathlib library](https://docs.python.org/3/library/pathlib.html).\n",
    "\n",
    "### Basic use of Pathlib\n",
    "\n",
    "**List all sub directories and files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e47af1c5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p=Path('.') # the directory where your python file is located\n",
    "for x in p.iterdir():\n",
    "    if x.is_dir():\n",
    "        print('Found dir: ', x)\n",
    "    elif x.is_file():\n",
    "        print('Found file: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f20a1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "**List all files of a type:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba78836b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p=Path('.')\n",
    "for p in p.rglob(\"*.png\"):# rglob means recursively, searches sub directories\n",
    "    print(p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7f9bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you want to print the full path do `print(p.absolute())`.\n",
    "\n",
    "**Create a directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "432f9b87",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Path('tmp_dir').mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df7a0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you run the code twice it will produce an error, because the directory exists, then we can simply do `Path('tmp_dir').mkdir(exist_ok=True)`.\n",
    "\n",
    "**Print current directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46b20ec",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994d864",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Joining paths:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a58c9430",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p=Path('.')\n",
    "new_path = p / 'tmp_dir' / 'my_file.txt'\n",
    "print(new_path.absolute())\n",
    "new_path.touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe9eec",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Basic use of `os`\n",
    "\n",
    "We have already encountered the use of `os` when printing the working directory, i.e. `print(os.getcwd())`. If you want to create a directory named `tmp`, one can do\n",
    "\n",
    "**Creating a directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95c30b7f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e849cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Moving into a directory:**\n",
    "To move into that directory do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19e08f9e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.chdir('tmp')\n",
    "os.chdir('..') # move back up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3b10e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Splitting data into different folders and files\n",
    "\n",
    "**Using the Pathlib library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cde94bfc",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('../data/corona_data.xlsx')\n",
    "countries = df['LOCATION'].unique() #skip duplicates\n",
    "data_folder=Path('../covid-data')\n",
    "data_folder.mkdir()\n",
    "for country in countries:\n",
    "    new_path=data_folder / country\n",
    "    new_path.mkdir()\n",
    "    excel_file=country+'.xlsx'\n",
    "    df2=df[df['LOCATION']==country]\n",
    "    df2.to_excel(new_path/excel_file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59457240",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you run the code twice, it will fail, but that can be resolved by e.g. `data_folder.mkdir(exist_ok=True)`.  \n",
    "\n",
    "**Using the `os` library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1e9fff1",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# first get all the countries:\n",
    "df=pd.read_excel('../data/corona_data.xlsx')\n",
    "countries = df['LOCATION'].unique() #skip duplicates\n",
    "os.mkdir('../covid-data')\n",
    "os.chdir('../covid-data')\n",
    "for country in countries:\n",
    "    os.mkdir(country)\n",
    "    os.chdir(country)\n",
    "    df2=df[df['LOCATION']==country]\n",
    "    df2.to_excel(country+'.xlsx',index=False)\n",
    "    os.chdir('..') # move up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2efc3c",
   "metadata": {
    "editable": true
   },
   "source": [
    "More robust way of creating a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19b3182e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_mkdir(name):\n",
    "    if os.path.isdir(name):\n",
    "        print('Directory ', name,' already exists')\n",
    "    else:\n",
    "        os.mkdir(name)\n",
    "        print('creating directory ',name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc593f",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you want to collect all data, you can do as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6033936",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_new=pd.DataFrame()\n",
    "data_folder=Path('../covid-data')\n",
    "for dir in data_folder.iterdir():\n",
    "    if dir.is_dir():      \n",
    "        file=dir.name+'.xlsx'\n",
    "        df=pd.read_excel(dir/file)\n",
    "        print('Reading file ', file)\n",
    "        df_new=pd.concat([df_new,df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fd522",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Writing more robust code\n",
    "Most likely in the last sections you have encountered quite long error messages from Python. Errors could be\n",
    "* syntax errors, grammatically incorrect code e.g. calling functions that does not exists, using variables that are not defined or writing lines of codes with missing instructions, indentation errors \n",
    "\n",
    "* exceptions e.g. open a file that does not exists, accessing a Pandas header with the wrong name, performing wrong mathematical operations (1/0)\n",
    "\n",
    "* logical errors (bugs), code that runs but produces wrong results. These errors are of course some of the most difficult errors to fix and can only be discovered by comparing the output of the code to known answers. In many cases errors are introduced when extending the code, and unit tests can be extremely helpful.\n",
    "\n",
    "In the rest of this section we will discuss how to avoid or to handle exceptions. The goal is to write a code that catch all the exceptions before they happen, tries to do something with them, if not prints out a reasonable error message of what went wrong.\n",
    "\n",
    "Let us look at the code that we have written so far, starting from the top of the notebook.\n",
    "\n",
    "**Accessing columns in Pandas:**\n",
    "So far we have just accessed the columns directly, but it is very quick to write a wrong name, thus instead of doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e3bbed0",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "time=df['TIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15157fec",
   "metadata": {
    "editable": true
   },
   "source": [
    "we should try to check if the column exist before accessing it from the DataFrame. There are many ways of achieving this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d20d2856",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_column_from_dataframe(name,df):\n",
    "    '''\n",
    "    name: name of column\n",
    "    df: Pandas DataFrame\n",
    "    returns: column if found, and empty otherwise \n",
    "    '''\n",
    "    if name in df.columns:\n",
    "        return df[name]\n",
    "    else:\n",
    "        print('Column not found')\n",
    "        print('Possible column names are : ', df.columns)\n",
    "        return pd.Series(dtype=object)\n",
    "# run the following code with df containing covid data\n",
    "get_column_from_dataframe('TIME2',df)\n",
    "get_column_from_dataframe('TIME',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e4dce",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note the use of doc string in the beginning, the doc string will be printed in advanced editors once you write the name of the function. It also helps you to remember what the function does. It is a good practice to return something of the same type, because then the rest of the code can execute. If it is critical that you find the name of the column you can always test from the outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "627a0c1c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s=get_column_from_dataframe('TIME2',df)\n",
    "if s.empty:\n",
    "    print('Exiting ...')\n",
    "    exit() # note this shuts down the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1b67f",
   "metadata": {
    "editable": true
   },
   "source": [
    "In the function `get_column_from_dataframe` many more things could go wrong, the user could pass a variable that is not a DataFrame, to catch all exceptions one can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f81c195",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_column_from_dataframe_v2(name,df):\n",
    "    '''\n",
    "    name: name of column\n",
    "    df: Pandas DataFrame\n",
    "    returns: column if found, and empty otherwise \n",
    "    '''\n",
    "    try:\n",
    "        return df[name]\n",
    "    except:\n",
    "        print('Something went wrong ...')\n",
    "        print('Maybe wrong column name?')\n",
    "        return pd.Series(dtype=object)\n",
    "# run the following code with df containing covid data\n",
    "get_column_from_dataframe_v2('TIME2',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6d2cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "The `try` and `except` handling is very elegant in Python, and a very easy way of making the code more robust. Python first tries `df[name]` if that is not successful (e.g. wrong column name, wrong DataFrame, maybe Pandas is not even imported) it jumps to the exception.\n",
    "\n",
    "Another thing to consider is to use case insensitive search, we should be able to access a country or a header using e.g. `Afghanistan` or `afghanistan`. A possible solution could be to make sure that when you read in the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e74b1eee",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_column_from_dataframe_v3(name,df):\n",
    "    '''\n",
    "    name: name of column (case insensitive)\n",
    "    df: Pandas DataFrame\n",
    "    returns: column if found, and empty otherwise \n",
    "    '''\n",
    "    COL=df.columns.str.upper()\n",
    "    NAME=name.upper()\n",
    "    try:\n",
    "        idx=COL.get_loc(NAME)\n",
    "        return df.iloc[:,idx]\n",
    "    except:\n",
    "        print('Column not found')\n",
    "        print('Possible column names are : ', df.columns)\n",
    "        return pd.Series(dtype=object)\n",
    "        \n",
    "get_column_from_dataframe_v3('time',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28164d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, we might want to make our code more robust when collecting data for e.g. a specific country `df[df['LOCATION'] == 'Afghanistan']`. This operation assumes 1) that the column `LOCATION` exists and 2) that the country is spelled correctly. However, we have already written code to get a column and check that it exists, but it is written inside a function with a different purpose. Thus, it is better to split the code above in two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f28c8fac",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_col_index(name,df):\n",
    "    '''\n",
    "    name: name of column (case insensitive)\n",
    "    df: Pandas DataFrame\n",
    "    returns: pos if exists, and -1 otherwise \n",
    "    '''\n",
    "    COL=df.columns.str.upper()\n",
    "    NAME=name.upper()\n",
    "    try:\n",
    "        return COL.get_loc(NAME)\n",
    "    except:\n",
    "        print('Column not found')\n",
    "        print('Possible column names are : ', df.columns)\n",
    "        return -1\n",
    "\n",
    "def get_column_from_dataframe_v4(name,df):\n",
    "    '''\n",
    "    name: name of column (case insensitive)\n",
    "    df: Pandas DataFrame\n",
    "    returns: column if found, and empty otherwise \n",
    "    '''\n",
    "    idx=get_col_index(name,df)\n",
    "    if idx>-1:\n",
    "        return df.iloc[:,idx]\n",
    "    else:\n",
    "        return pd.Series(dtype=object)\n",
    "        \n",
    "def get_rows_from_dataframe(name,df,col='LOCATION'):\n",
    "    '''\n",
    "    name: name of rows (case insensitive)\n",
    "    df: Pandas DataFrame\n",
    "    col: name of column to use as logical test\n",
    "    returns: DataFrame, and empty otherwise \n",
    "    '''\n",
    "    idx=get_col_index(col,df)\n",
    "    if idx>-1:\n",
    "        NAME=name.upper()\n",
    "        return df[df.iloc[:,idx].str.upper() == NAME]\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "get_rows_from_dataframe('afGhaniStan',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8377d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "**To summarize:**\n",
    "\n",
    "1. We want to catch errors before they occur, this is most efficiently done by wrapping simple operations in functions\n",
    "\n",
    "2. Functions should be small, i.e. only to as little as possible, that would increase their reusability\n",
    "\n",
    "3. Almost all exceptions can be caught by using `try` and `except` functionality in Python\n",
    "\n",
    "4. Write doc strings in functions to increase user friendliness\n",
    "\n",
    "5. Write meaningful error messages, if possible also print out some additional information to help the user \n",
    "\n",
    "# Bibliography"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
