{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a791501",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- dom:TITLE: Solving non linear systems  -->\n",
    "# Solving non linear systems \n",
    "**Aksel Hiorth**\n",
    "University of Stavanger\n",
    "\n",
    "Date: **Aug 19, 2022**\n",
    "\n",
    "<!-- Common Mako variables and functions -->\n",
    "\n",
    "<!-- Contrary to linear equations, you will most likely find that the functions available in -->\n",
    "<!-- various Python library will *not* cover your needs and in many cases fail to give you -->\n",
    "<!-- the correct solution. The reason for this is that the solution of a nonlinear equation # is greatly -->\n",
    "<!-- dependent on the starting point, and a combination of various techniques  must be used. -->\n",
    "\n",
    "In this chapter we will cover some theory related to the solution of nonlinear equations, and introduce the most used methods. A nonlinear problem is represented as a single equation or a system of equations, where the response is not changing proportionally to the input.  Almost all physical systems are nonlinear, and one frequent use of the methods presented in this chapter is to determine model parameters by matching a nonlinear model to data. \n",
    "\n",
    "Numerical methods that is guaranteed to find a solution (if it exists) are called *closed methods*, and *open* other vise. In many cases the closed methods requires more iterations for well behaved functions than the open methods. For one dimensional problems we will cover: fixed point iteration, bisection, Newton's method, and the secant method.\n",
    "For  multidimensional problems we will cover Newton-Rapson method, which is a direct extension of Newton's method in one\n",
    "dimension, and the steepest decent. The main challenge is that there are (usually) more than one solution, the solution that\n",
    "*you* want for a specific problem is usually dictated by the underlying physics. If computational speed is not an issue, the\n",
    " method of choice is usually the bisection method. It is guaranteed to give an answer, but it might be slow. If speed is an issue, usually Newton's or the secant method will be the fastest (but it depends on the starting point). The secant method is sometimes preferred if the derivative of the function is costly to evaluate. Brents method is a method that combine the secant and bisection method (not covered), and is guaranteed to find a solution if the root is bracketed. \n",
    "\n",
    "In many practical, engineering, applications one usually implements some of the methods described below directly inside functions. This is because it is usually faster than calling a separate all purpose nonlinear solver, and that one usually has a very good idea of what a good starting point for the nonlinear solver is. \n",
    "\n",
    "# Nonlinear equations\n",
    "A nonlinear equation is simply an equation that is not linear. That means that when the variables changes the response is not changing proportional to the values of the variables. Solving a nonlinear equation always proceeds by *iterations*, we start with one or several initial guesses and then search for the solution. In many cases we do not know beforehand if the equation actually has a solution, or multiple solutions. An example of a nonlinear problem is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ef825",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:exp\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "e^{-x}=x^2.\n",
    "\\label{eq:nlin:exp} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17b7a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Traditionally one collect all the terms on one side, to solve an equation of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d08d20",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fx\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x)=x^2-e^{-x}=0.\n",
    "\\label{eq:nlin:fx} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc30520",
   "metadata": {
    "editable": true
   },
   "source": [
    "In [figure 1](#fig:nlin:fx), the solution is shown graphically. Note that in one case the solution is when the graph of $e^{-x}$, and $x^2$ intersect, whereas in the other case the root is located when $x^2-e^{-x}$ intersect the $x-$axis. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/fx.png, width=400 frac=.5] Notice that the root is located at the same place ($x=0.703467417$) <div id=\"fig:nlin:fx\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:fx\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/fx.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 1: Notice that the root is located at the same place ($x=0.703467417$)</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "In the case of more than one unknown, or a set of equations that must be satisfied simultaneously, equation ([2](#eq:nlin:fx)) is replaced with a vector equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604864b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fvec\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{f}(\\mathbf{x})=\\mathbf{0}.\n",
    "\\label{eq:nlin:fvec} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7012dae",
   "metadata": {
    "editable": true
   },
   "source": [
    "Although this equation looks quite similar to equation ([2](#eq:nlin:fx)), this equation is *much* harder to solve. The only methods we will cover is the Newton Rapson method, which is a very good method if a good starting point is given. If you have a multidimensional problem, the advice is to try Newton-Raphson, if this method fails you need to try more advanced method, see e.g. [[press2001]](#press2001).\n",
    "\n",
    "# Example: van der Waals equation of state\n",
    "Before we begin with the numerical algorithms, let us consider an example: the van der Waals equation of state. The purpose is to illustrate some of the typical challenges. You are probably familiar with the ideal gas law:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92266feb",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:pvt\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P\\nu=R_gT,\n",
    "\\label{eq:nlin:pvt} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f4177",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\nu=V/n$ is the molar volume of the gas, $P$ is the pressure, $V$ is the volume, $T$ is the temperature, $n$ is the number of moles of the gas, and $R_g$ is the ideal gas constant.  This equation is an example of an *equation of state* (EOS), it relates $P$, $T$, and $\\nu$. Thus if we know the pressure and temperature of the gas, we can calculate $\\nu$. Equation ([4](#eq:nlin:pvt)) assumes that there are no interactions between the molecules in the gas. Clearly, this is too simplistic, and because of this one normally uses an EOS that better reflect the physical properties of the substance. A very famous EOS is the van der Waal EOS, which is a slight modification of equation ([4](#eq:nlin:pvt)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990db13",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:vdw\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(P+\\frac{a}{\\nu^2}\\right)\\left(\\nu-b\\right)=R_gT.\n",
    "\\label{eq:nlin:vdw} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc24ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$a$ and $b$ are material constants that needs to be determined experimentally. This equation is *not* used in industrial design, but most equations used in practice are based on equation ([5](#eq:nlin:vdw)). Multiplying equation ([5](#eq:nlin:vdw)) with $\\nu^2$, we get a non linear equation that is cubic in the molar volume. It turns out that cubic EOS are a class of equations that are quite successful in modeling the behavior of real systems [[peng1976new]](#peng1976new). However equation ([5](#eq:nlin:vdw)) is a good starting point for more complex and realistic equations.\n",
    "\n",
    "It is common practice to rescale EOS with respect to the critical point. At the critical point we have [ref]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8c8dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:crit1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left.\\frac{\\partial P}{\\partial \\nu}\\right|_{T_c,P_c} =0\n",
    "\\label{eq:nlin:crit1} \\tag{6} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d353e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:crit2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\left.\\frac{\\partial^2 P}{\\partial \\nu^2}\\right|_{T_c,P_c} =0\n",
    "\\label{eq:nlin:crit2} \\tag{7} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdff1f",
   "metadata": {
    "editable": true
   },
   "source": [
    "From equation ([6](#eq:nlin:crit1)),  ([7](#eq:nlin:crit2)), and ([5](#eq:nlin:vdw)), it follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef0fde",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:crit3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nu_c=3b\\quad,P_c=\\frac{a}{27b^2}\\quad,R_gT_c=\\frac{8a}{27b^2}.\n",
    "\\label{eq:nlin:crit3} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f55d01",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting these equations into equation ([5](#eq:nlin:vdw)), and defining the *reduced* quantities $\\hat{P}=P/P_c$, $\\hat{T}=T/T_c$, $\\hat{\\nu}=\\nu/\\nu_c$, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90ad1f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:vdwr\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\\hat{P}+\\frac{3}{\\hat{\\nu}^2}\\right)\\left(3\\hat{\\nu}-1\\right)=8\\hat{T}.\n",
    "\\label{eq:nlin:vdwr} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac905c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- dom:FIGURE: [fig-nlin/vdw.png, width=400 frac=1.0] van der Waal isotherms. <div id=\"fig:nlin:vdw\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:vdw\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/vdw.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 2: van der Waal isotherms.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "In [figure 2](#fig:nlin:vdw), we have plotted the isotherms. Note that if $\\hat{T}<1$ ($T<T_c$), there might be more than one solution for the molar volume. This is clearly unphysical and additional constraints are needed. For the curve $\\hat{T}=0.9$, the dashed lined shows that for $\\hat{P}=0.7$, there are three solutions. This is a typical behavior of the cubic EOS, and physically it corresponds to the saturated case, where the vapor and liquid phase co-exist. The left root is the liquid state and the right root is the vapor state. The root in the middle represents a meta stable state.\n",
    "\n",
    "**It never hurts to look at your function.**\n",
    "\n",
    "The example in [figure 2](#fig:nlin:vdw) illustrates some important points. Solving a nonlinear problem might be very easy in part of the parameter space (e.g. when $T>T_c$ there are only one solution), but extremely hard in other part of the parameter space (e.g. when $T<T_c$, where there are multiple solutions). However, much of the trick to find a solution is to choose a good starting point. When there are multiple solutions we need to start close to the physical solution.\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 1: van der Waal EOS and CO$_2$\n",
    "\n",
    "Use equation ([5](#eq:nlin:vdw)), and the parameters for CO$_2$: a=3.640 L$^2$bar/mol, and b=0.04267 L/mol, to test the van der Waal EOS in equation ([5](#eq:nlin:vdw)). Use that at 2 MPa and 100 $^\\circ$C, CO$_2$ has a specific volume of 0.033586 m$^3$/kg.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "The calculation is straight forward, but it is easy to get an error due to units. We will use SI units: a=0.3640 m$^6$Pa/mol, b=4.267$\\cdot10^{-5}$ m$^3$/mol, $R$=8.314J/mol K.  The molar volume is obtained by multiplying by the molar weight of CO$_2$: $M_w$ = 44 g/mol, hence $\\nu=1.478\\cdot10^{-3}$m$^3$/mol. Using $P=RT/(\\nu-b)-a/\\nu^2=1.993$ MPa, or an error of $0.3\\%$.\n",
    "\n",
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "# Fixed-point iteration\n",
    "A simple (but not always possible) way of solving a nonlinear equation is to reformulate the problem $f(x)=0$ to a problem of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1e01f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:g\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x=g(x).\n",
    "\\label{eq:nlin:g} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca095d3",
   "metadata": {
    "editable": true
   },
   "source": [
    "The algorithm for solving this equation is to guess at a starting point, $x_0$, evaluate $x_1=g(x_0)$, $x_2=g(x_1)$, and so on. In some circumstances we might end up at a stable point, where $x$ does not change. This point is termed a *fixed point*.\n",
    "\n",
    "Note that the form of $g(x)$ is not uniquely determined. For our function defined in equation ([1](#eq:nlin:exp)), we can solve for $x$ directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d277f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:g2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x=e^{-x/2},\n",
    "\\label{eq:nlin:g2} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd633c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "or we could write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc70f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:g3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x=x-x^2+e^{-x}.\n",
    "\\label{eq:nlin:g3} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ed05f",
   "metadata": {
    "editable": true
   },
   "source": [
    "These functions are illustrated in [figure 3](#fig:nlin:fg), by visual inspection they look very similar, but as we will show in the next exercise the convergence is quite different. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/f_g_comb.png, width=400 frac=1] Two examples of iterative functions, that will give the same solution. <div id=\"fig:nlin:fg\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:fg\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/f_g_comb.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 3: Two examples of iterative functions, that will give the same solution.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 2: Implement the fixed point iteration\n",
    "\n",
    "Write a Python function that utilizes the fixed point algorithm in the previous section, find the root of $f(x)=x^2-e^{-x}$. In one case use $g(x)=e^{-x/2}$, and in the other case use $g(x)=x-x^2+e^{-x}$. How many iterations does it take in each case?\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "Below is a straight forward (vanilla) implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218a2473",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def iterative(x,g,prec=1e-8, MAXIT=1000):\n",
    "    '''Approximate solution of x=g(x) by fixed point iterations.\n",
    "    x : starting point for iterations \n",
    "    eps : desired precision\n",
    "    Returns x when x does not change more than prec\n",
    "    and number of iterations MAXIT are not exceeded\n",
    "    '''\n",
    "    eps = 1\n",
    "    n=0\n",
    "    while eps>prec and n < MAXIT:\n",
    "        x_next = g(x)\n",
    "        eps = np.abs(x-x_next)\n",
    "        x = x_next\n",
    "        n += 1\n",
    "        if(np.isinf(x)):\n",
    "            print('Quitting .. maybe bad starting point?')\n",
    "            return x\n",
    "    if (n<MAXIT):\n",
    "        print('Found solution: ', x, ' After ', n, 'iterations')\n",
    "    else:\n",
    "        print('Max number of iterations exceeded')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c257af4",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we start at $x=0$, it will take 174 iterations using $x-x^2+e^{-x}$ ($g(x)$) and only 19 for $e^{-x/2}$ ($h(x)$), the root is $x$=0.70346742.\n",
    "\n",
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 3: Finding the molar volume from the van der Waal EOS by fixed point iteration\n",
    "\n",
    "Extend the code above to take as argument the van der Waal EOS. For simplicity we will use the rescaled EOS in equation ([9](#eq:nlin:vdwr)). Show that for the reduced temperature, $\\hat{T}$=1.2, and pressure, $\\hat{P}$=1.5, the reduced molar volume $\\hat{nu}$ is 1.3522091.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "First we rewrite equation ([9](#eq:nlin:vdwr)) in a more useful form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8786b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:sp\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\nu}=\\frac{1}{3}(1+\\frac{8\\hat{T}}{\\hat{P}+3/\\hat{\\nu}^2})\n",
    "\\label{eq:nlin:sp} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff173ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "The right hand side will play the same role as $g(x)$ above, where $x$ now is the reduced molar volume, and can be implemented in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8005027f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dvdwEOS(nu,t,p):\n",
    "    return (1+8*t/(p+3/nu**2))/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1a661",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that this function requires the values of $\\hat{P}$ and $\\hat{T}$, in addition to $\\hat{\\nu}$ to return a value. Thus in order to use the fixed point iteration method implemented above, we need to pass arguments to our function. This can easily be achieved by taking advantage of Pythons `*args` functionality. By simply rewriting our implementation slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21615c6b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def iterative(x,g,*args,prec=1e-8):\n",
    "    MAX_ITER=1000\n",
    "    eps = 1\n",
    "    n=0\n",
    "    while eps>prec and n < MAX_ITER:\n",
    "        x_next = g(x,*args)\n",
    "        eps = np.abs(x-x_next)\n",
    "        x = x_next\n",
    "        n += 1\n",
    "    print('Number of iterations: ', n)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f7c3d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can find the root by calling the function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25fd347",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iterative(1,dvdwEOS,1.2,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebf3c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "The program returns the correct solution after 71 iterations.\n",
    "\n",
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "## When do the fixed point method fail?\n",
    "<div id=\"sec:nlin:fp\"></div>\n",
    "If we replace $e^{-x}$ with $e^{1-x^2}$ in equation ([12](#eq:nlin:g3)), our method will not give a solution. You can easily verify that the $x=1$ is a solution, so why does our method fail? To investigate this in a bit more detail, we turn to Taylors formula (once again). Assume that the root is located at $x^*$, and our guess is $x_k$, then the next $x$-value will be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8387a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:t1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{k+1}=g(x_0)=g(x^*)+g^\\prime(x^*)(x_k-x^*)+\\cdots\n",
    "\\label{eq:nlin:t1} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189385a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "The true solution is $x^*$, hence $x^*=f(x^*)$, and we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7516855",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:t2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{k+1}-x^*=g^\\prime(x^*)(x_k-x^*),\n",
    "\\label{eq:nlin:t2} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a077fac",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have neglected higher order terms. The point is: at each iteration we want the distance $x_1-x^*$ to decrease, i.e. to be smaller than $x_0-x^*$. This can only be achieved if"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7630b00",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fpi\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "|g^\\prime(x^*)|<1. \n",
    "\\label{eq:nlin:fpi} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f1311",
   "metadata": {
    "editable": true
   },
   "source": [
    "In our example above we saw that if $g(x)=x-x^2+e^{-x}$, we used 172 iterations and only 19 iterations if we replaced $g(x)$ with $h(x)=e^{-x/2}$ to converge to the *same* root $x$=0.70346742. We can now understand this, because $g^\\prime(x)=1-2x-e^{-x}$ and $g(x^*)\\simeq-0.90$, whereas $h^\\prime(x)=-e^{-x/2}/2$, and $h^\\prime(x^*)\\simeq0.35$. We expect the number of iterations, $n$, needed to reach a certain precision, $\\varepsilon$, to scale as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460db6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:scale\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "|g^\\prime(x^*)|^n=\\varepsilon.\n",
    "\\label{eq:nlin:scale} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a3cbb",
   "metadata": {
    "editable": true
   },
   "source": [
    "We expect to use $\\log|h^\\prime(x^*)|/\\log|g^\\prime(x^*)|\\simeq10$ more iterations using $g(x)$ compared to $h(x)$, which is close to the observed value of 172/19$\\simeq 9$.\n",
    "## What to do when the fixed point method fail\n",
    "As discussed in [[newman2013]](#newman2013), there might be an elegant solution whenever $|g^\\prime(x^*)|>1$. If it is possible to invert the $g(x)$, we can show that the derivative of the inverse function\n",
    "$ { g^\\prime }^{-1} (x^*)  = 1/g^\\prime (x^*) $. Why is this useful? Because if $x^*=g(x^*)$ is the solution we are searching for, then this is equivalent to $x^*={g}^{-1}(x^*)$ *if and only if* we can invert $g(x)$. Note that in many cases it is not possible to invert $g(x)$. Let us first show that $ { g^\\prime }^{-1} (x^*)  = 1/g^\\prime (x^*) $. For simplicity write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f4d0e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = g(x)\\Leftarrow x=g^{-1}(y),\n",
    "\\label{_auto1} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb77ca",
   "metadata": {
    "editable": true
   },
   "source": [
    "taking the derivative with respect to x gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ea841",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fpi1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d}{dx}g^{-1}(y)=\\frac{dx}{dx}=1,\\label{eq:nlin:fpi1} \\tag{19}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434a78a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fpi2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{dg^{-1}(y)}{dy}\\frac{dy}{dx}=\\frac{dx}{dx}=1,\\label{eq:nlin:fpi2} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ad40c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fpi3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{dg^{-1}(y)}{dy}=\\frac{1}{\\frac{dy}{dx}}=\\frac{1}{g^{\\prime}(x)}\n",
    "=\\frac{1}{g^{\\prime}(g^{-1}(y))}.\\label{eq:nlin:fpi3} \\tag{21}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26ae1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Going from equation ([19](#eq:nlin:fpi1)) to ([20](#eq:nlin:fpi2)), we have used the chain rule. Equation ([21](#eq:nlin:fpi3)) is general, let us now specify to our fixed point iteration. Then we can use $x^*=g(x^*)=y^*$, and $x^*=g^{-1}(y^*)=g^{-1}(x^*)$ hence we can write the last equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f409a9e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:fpif\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d}{dx}g^{-1}(x^*)=\\frac{1}{g^{\\prime}(x^*)}.\n",
    "\\label{eq:nlin:fpif} \\tag{22}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6b9ec",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 4: Solve $x=e^{1-x^2}$ using fixed point iteration\n",
    "\n",
    "The solution to $x=e^{1-x^2}$ is clearly $x=1$.\n",
    "\n",
    "* First try the fixed point method using $g(x)=e^{1-x^2}$ to find the root $x=1$. Try to start very close to the true solution $x=1$. What is the value of $g^\\prime(x^*)$?\n",
    "\n",
    "* Next, invert $g(x)$, what is the derivative of $g^{-1}(x^*)$? Try the fixed point method using $g^{-1}(x^*)$\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "First, we calculate the derivative of $g(x)$, $g^\\prime(x)=-2xe^{1-x^2}$, hence $g^\\prime(x^*)=-2$ and $|g^\\prime(x^*)|>1$. This is an unstable fixed point, and if we start a little bit off from this point we will spiral away from it.\n",
    "\n",
    "Inverting $y=g(x)$ gives us $ g^{-1} (y)=\\sqrt{1-\\ln y}$. Note that $y^*=x^*=1$ is a solution to this equation as it should be. The derivative is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398ad6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{g^{-1}}^\\prime(y)=-\\frac{1}{2\\sqrt{1-\\ln y}},\n",
    "\\label{_auto2} \\tag{23}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12817b3",
   "metadata": {
    "editable": true
   },
   "source": [
    "and $ {g^{-1}}^\\prime(y^*)=-1/2 $.\n",
    "It takes about 30 iterations to reach the correct solution $y^*=1$, when the starting point is $y=0$.\n",
    "\n",
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "# Rate of convergence\n",
    "The rate of convergence is the speed at which a *convergent* sequence approach the limit. Assume that our sequence $x_{k}$ converges to the number $x^*$, the sequence is said to *converge linearly* to $x^*$ if there exists a number $\\mu\\in<0,1>$, such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd9320",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:linconv\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lim_{k\\to\\infty}=\\frac{|x_{k+1}-x^*|}{|x_k-x^*|}=\\mu\n",
    "\\label{eq:nlin:linconv} \\tag{24}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c56451",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting equation ([15](#eq:nlin:t2)) in equation ([24](#eq:nlin:linconv)), we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fc91f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:ling\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lim_{k\\to\\infty}=\\frac{|x_{k+1}-x_k|}{x_k-x^*}\n",
    "=\\frac{|g^\\prime(x^*)(x_k-x^*)|}{|x_k-x^*|}=|g^\\prime(x^*)|.\n",
    "\\label{eq:nlin:ling} \\tag{25}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337a140",
   "metadata": {
    "editable": true
   },
   "source": [
    "Hence the fixed point iteration is expected to converge *linearly* to the correct solution. The definition in equation ([24](#eq:nlin:linconv)), can be extended to include the definition of quadratic, cubic, etc. convergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52d991",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:qconv\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lim_{k\\to\\infty}=\\frac{|x_{k+1}-x^*|}{|x_k-x^*|^q}=\\mu.\n",
    "\\label{eq:nlin:qconv} \\tag{26}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4dd6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "If $q=2$ the convergence is said to be quadratic and so on.\n",
    "\n",
    "# The bisection method\n",
    "The idea behind bisection is that the root is bracketed, i.e. that there exists two points $a$ and $b$, such that $f(a)\\cdot f(b)<0$. In practice it might be a challenge to find these two points. However, if you know that the function has a only root between two values, and that speed is not a big issue this method guarantees that the root will be found within a finite number of steps. The basic idea behind the method is to divide the interval into two (i.e. bisecting the interval). The method only works if the function is continuous on the interval. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/bisection.png, width=400 frac=1] Illustration of the bisection method for the van der Waal EOS. <div id=\"fig:nlin:bisection\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:bisection\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/bisection.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 4: Illustration of the bisection method for the van der Waal EOS.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "The algorithm is as follows:\n",
    "* Test if $f(a)\\cdot f(b)<0$, if not return an error message\n",
    "\n",
    "* Calculate the midpoint $c=(a+b)/2$. If $f(a)\\cdot f(c)<0$ the root is in the interval $[a,c]$, else the root is in the interval $[c,b]$\n",
    "\n",
    "* Half the interval, and test in which interval the root lies, and continue until a convergence criterion.\n",
    "\n",
    "In [figure 4](#fig:nlin:bisection), there is a graphical illustration.\n",
    "Below is an implementation of the bisection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66cda97",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bisection(f,a,b,prec=1e-8,MAXIT=100):\n",
    "    '''Approximate solution of f(x)=0 on interval [a,b] by bisection.\n",
    "\n",
    "    f   : f(x)=0.\n",
    "    a,b : brackets the root f(a)*f(b) has to be negative \n",
    "    eps : desired precision\n",
    "    \n",
    "    Returns the midpoint when it is closer than eps to the root, \n",
    "    unless MAXIT are not exceeded\n",
    "    '''\n",
    "    if f(a)*f(b) >= 0:\n",
    "        print('You need to bracket the root, f(a)*f(b) >= 0')\n",
    "        return None\n",
    "    an = a\n",
    "    bn = b\n",
    "    cn = 0.5*(an + bn)\n",
    "    c_old = cn - 10*prec\n",
    "    n=0\n",
    "    while np.abs(cn-c_old)>=prec and n<MAXIT:\n",
    "        c_old = cn \n",
    "        f_cn = f(cn)\n",
    "        if f(an)*f_cn < 0:\n",
    "            bn = cn\n",
    "        elif f(bn)*f_cn < 0:\n",
    "            an = cn\n",
    "        elif f_cn == 0:\n",
    "            print('Found exact solution ', cn, \n",
    "                  ' after ', n, 'iterations' )\n",
    "            return cn\n",
    "        else:\n",
    "            print('Bisection method fails.')\n",
    "            return None\n",
    "        cn = 0.5*(an+bn)\n",
    "        n += 1\n",
    "    if n<MAXIT-1:\n",
    "        print('Found solution ', cn,' after ', n, 'iterations' )\n",
    "        return cn\n",
    "    else:\n",
    "        print('Max number of iterations: ', MAXIT, ' reached.') \n",
    "        print('Try to increase MAXIT or decrease prec')\n",
    "        print('Returning best guess, value of function is: ', f_cn)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebd250",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Warnings.**\n",
    "\n",
    "Note that the implementation of the bisection algorithm is only a few lines of code, and most of the code is to give warnings to the user. In this case it is important to do additional checking, and give the user warnings. If $f(c)$=0, then we must stop and return the exact solution. If we only test if $f(a)\\cdot f(c)$ is greater or lower than zero the algorithm would fail.\n",
    "\n",
    "\n",
    "\n",
    "## Rate of convergence\n",
    "If $c_n$ is the midpoint after $n$ steps, the difference between the solution $x^*$ and $c_n$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc258c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:bisec\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "|c_n-x^*| \\le \\frac{|b-a|}{2^n}\n",
    "\\label{eq:nlin:bisec} \\tag{27}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc87380",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using our previous definition in equation ([26](#eq:nlin:qconv)), we find that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a409b01",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:bsc1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lim_{k\\to\\infty}=\\frac{|c_{k+1}-x^*|}{|c_k-x^*|}\\le\\frac{|b-a|/2^{n+1}}{|b-a|/2^n}=\\frac{1}{2},\n",
    "\\label{eq:nlin:bsc1} \\tag{28}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73ec67",
   "metadata": {
    "editable": true
   },
   "source": [
    "hence the bisection method converges linearly.\n",
    "# Newtons method\n",
    "Newtons method is one of the most used methods. If it converges, it converges quadratically to the correct solution. The drawback is that contrary to the bisection method it may fail if a bad starting point is given. Newtons method for finding the root of a function $f(x)=0$ is illustrated in [figure 5](#fig:nlin:newton). The main idea is to use more information about the function in the search of the root. In this case we want to find the point where the tangent of the function in $x_k$ intersect the $x-$axis, and take that as our next point, $x_{k+1}$. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/newton_comb.png, width=400 frac=1.0] Illustration of Newtons method for the van der Waals EOS.<div id=\"fig:nlin:newton\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:newton\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/newton_comb.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 5: Illustration of Newtons method for the van der Waals EOS.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "We can easily derive the algorithm by finding the formula for the tangent line. Using $y=ax+b$ for the tangent line, we immediately know that $a=f^\\prime(x_k)$. $b$ can be found as we know that the line intersects $(x_k,f(x_k))$: $f(x_k)=f^\\prime(x_k)x_k+b$, hence the equation for the tangent line is $y=f^\\prime(x_k)x+f(x_k)-f^\\prime(x_k)x_k$. The next point is located where $y$ crosses the $x$-axis, hence $0=f^\\prime(x_k)x_{k+1}+f(x_k)-f^\\prime(x_k)x_k$. Rearranging this equation, we can write Newtons method in the standard form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e3a0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:newton\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{k+1}=x_k-\\frac{f(x_k)}{f^\\prime(x_k)}.\n",
    "\\label{eq:nlin:newton} \\tag{29}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d0cf2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the derivative of $f(x)$ enters in equation ([29](#eq:nlin:newton)), which means that if our function has a extremal value in our search domain, Newtons method most likely will fail. In particular $x_1$, and $x_4$ in the figure to the right in [figure 6](#fig:nlin:newton2) are bad starting point for Newtons method.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/newton2.png, width=400 frac=1.0]  Illustration of some of the possible challenges with Newtons method. Note that if the derivative is zero somewhere in the search interval, Newtons method will fail. <div id=\"fig:nlin:newton2\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:newton2\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/newton2.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 6: Illustration of some of the possible challenges with Newtons method. Note that if the derivative is zero somewhere in the search interval, Newtons method will fail.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "An implementation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551c4d7a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def newton(f,x, prec=1e-8,MAXIT=500):\n",
    "    '''Approximate solution of f(x)=0 by Newtons method.\n",
    "    The derivative of the function is calculated numerically\n",
    "    f   : f(x)=0.\n",
    "    x   : starting point  \n",
    "    eps : desired precision\n",
    "    \n",
    "    Returns x when it is closer than eps to the root, \n",
    "    unless MAX_ITERATIONS are not exceeded\n",
    "    '''\n",
    "    MAX_ITERATIONS=MAXIT\n",
    "    x_old = x\n",
    "    h     = 1e-4\n",
    "    for n in range(MAX_ITERATIONS):\n",
    "        x_new = x_old - 2*h*f(x_old)/(f(x_old+h)-f(x_old-h))\n",
    "        if(abs(x_new-x_old)<prec):\n",
    "            print('Found solution:', x_new, \n",
    "                  ', after:', n, 'iterations.' )\n",
    "            return x_new\n",
    "        x_old=x_new\n",
    "    print('Max number of iterations: ', MAXIT, ' reached.') \n",
    "    print('Try to increase MAXIT or decrease prec')\n",
    "    print('Returning best guess, value of function is: ', f(x_new))\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df5c6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Comparing [figure 4](#fig:nlin:bisection) and [fig:nlin:newton](#fig:nlin:newton), you immediately get the sense that Newtons method converges faster, and indeed it does. \n",
    "\n",
    "## Rate of convergence\n",
    " Newtons method is similar to the fixed point method, but where we do not use $g(x)=x-f(x)$, but $g(x)=x-\\frac{f(x)}{f^\\prime(x)}$. We will now analyze Newtons method, using the same approach as in the section [When do the fixed point method fail?](#sec:nlin:fp). First we expand $g(x)$ around the root $x^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582d72c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:nsec\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{k+1}=g(x_k)=g(x^*)+g^\\prime(x^*)(x_k-x^*)+\\frac{1}{2}g^{\\prime\\prime}(x^*)(x_k-x^*)^2,\n",
    "\\label{eq:nlin:nsec} \\tag{30}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44280e",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have skipped all higher order terms. You can easily verify that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a69c27",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:gn2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g^\\prime(x) =\\frac{f^{\\prime\\prime}(x)f(x)}{f^\\prime(x)^2}\n",
    "\\label{eq:nlin:gn2} \\tag{31} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a5a3e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:gn3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "g^{\\prime\\prime}(x) =\\frac{(f^{\\prime\\prime\\prime}(x)f^\\prime(x)-2f^{\\prime\\prime}(x)^2f^\\prime(x))f(x)\n",
    "+f^{\\prime\\prime}(x)f^\\prime(x)^2}{f^\\prime(x)^4}.\n",
    "\\label{eq:nlin:gn3} \\tag{32}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118cf85",
   "metadata": {
    "editable": true
   },
   "source": [
    "$x^*$ is a solution, hence $f(x^*)=0$, we then find from equation ([31](#eq:nlin:gn2)) and ([32](#eq:nlin:gn3)) that $g^\\prime(x^*)=0$, and $g^{\\prime\\prime}(x^*)=f^{\\prime\\prime}(x^*)/f^{\\prime}(x^*)^2$. Thus from equation ([30](#eq:nlin:nsec)) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a7207",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:nsecn\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{k+1}=x^*+\\frac{1}{2}\\frac{f^{\\prime\\prime}(x^*)}{f^{\\prime}(x^*)^2}(x_k-x^*)^2,\n",
    "\\label{eq:nlin:nsecn} \\tag{33}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54166a",
   "metadata": {
    "editable": true
   },
   "source": [
    "or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12976251",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:nsecn2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{x_{k+1}-x^*}{(x-x^*)^2}=\\frac{1}{2}\\frac{f^{\\prime\\prime}(x^*)}{f^{\\prime}(x^*)^2}.\n",
    "\\label{eq:nlin:nsecn2} \\tag{34}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0329347",
   "metadata": {
    "editable": true
   },
   "source": [
    "The denominator has a power of two, and hence Newtons method is *quadratic* convergent (assuming that the sequence $x_{k+1}$ is a convergent sequence). Note that it also follows from the analyses above that Newtons method will fail if the derivative at the root, $f^\\prime(x^*)$, is zero.\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 5: Compare Newtons, Bisection and the Fixed Point method\n",
    "\n",
    "Find the root of $f(x)=x^2-e^{-x}$ using bisection, fixed point,  and Newtons method, start at $x=0$. How many iterations do you need to use reach a precision of $10^{-8}$? What happens if you widen the search domain or start further away from the root?\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "The root is located at $x^*=0.70346742$.\n",
    "* Fixed point method: we saw earlier that using $g(x)=x-f(x)$ used 174 iterations, and $g(x)=\\sqrt{x^2-f(x)}$ used 19 iterations. If we start at $x=-100$, $g(x)=x-f(x)$ fails, and  $g(x)=\\sqrt{x^2-f(x)}$ uses only 21 iterations, and at $x=100$ we use 20 iterations.\n",
    "\n",
    "* Bisection method: it use 25 iterations for $a=0$, and $b=1$ (implementation shown earlier in the chapter). Choosing $a=-b=-100$ we use 33 iterations.\n",
    "\n",
    "* Newtons method: it use only 5 function evaluations (implementation above) starting at  $x=0$. Starting at $x=-100$, it uses 106 iterations. Newtons method is slow in this case because the function is very steep around the starting point, see [figure 7](#fig:nlin:newton_bad). Starting at $x=100$, we only use 10 iterations.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/newton_bad.png, width=400 frac=1.0] Newtons method performs poorly far away due to the shape of the function close to $x=-100$, bisection performs much better while the fixed point method fails. <div id=\"fig:nlin:newton_bad\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:newton_bad\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/newton_bad.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 7: Newtons method performs poorly far away due to the shape of the function close to $x=-100$, bisection performs much better while the fixed point method fails.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "**A good starting point is crucial.**\n",
    "\n",
    "Note that it is not given which method is best, but if we are ''close'' to the root Newtons method is usually superior. If we are far away, other methods might work better. In many cases one uses a more stable method far away from the root, and then ''polish up'' the root by a couple of Newton iterations [[press2001]](#press2001). See also Brents method which combines bisection and linear interpolation (secant method) [[press2001]](#press2001).\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "# Secant method\n",
    "The Newtons method is very good if you can choose a good starting point, and you can give in an analytical formula for the derivative. In some cases it is not possible to calculate the derivative analytically, then a very good method of choice is the secant method. It can be derived by simply replacing the derivative in Newtons method by the finite difference approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d583321",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:sec1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f^\\prime(x_k)\\to \\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}.\n",
    "\\label{eq:nlin:sec1} \\tag{35}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1251d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting this equation into equation ([29](#eq:nlin:newton)), we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50affeb8",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{k+1}=x_k-f(x_k)\\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}{\\nonumber}\n",
    "\\label{_auto3} \\tag{36}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63677e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:sec2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "       =\\frac{x_{k-1}f(x_k)-x_kf(x_{k-1})}{f(x_k)-f(x_{k-1})}. \\label{eq:nlin:sec2} \\tag{37}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6917d61",
   "metadata": {
    "editable": true
   },
   "source": [
    "For a graphical illustration see [figure 8](#fig:nlin:secant)\n",
    "<!-- dom:FIGURE: [fig-nlin/secant.png, width=400 frac=1.0] A graphical illustration of the secant method. Note that the starting points $x_0$ and $x_1$ do not need to be close. The next point is where the (secant) line crosses the $x$-axis. <div id=\"fig:nlin:secant\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:secant\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/secant.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 8: A graphical illustration of the secant method. Note that the starting points $x_0$ and $x_1$ do not need to be close. The next point is where the (secant) line crosses the $x$-axis.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "## Rate of convergence\n",
    "The derivation of the rate of convergence for the secant method is a bit more involved. To simplify the notation we introduce the notation $\\varepsilon_k\\equiv x_k-x^*$, where $x^*$ is the exact solution. Subtracting $x^*$ from each side of equation ([37](#eq:nlin:sec2)) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a6496",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\varepsilon_{k+1}=x_{k+1}-x^*=\\frac{x_{k-1}f(x_k)-x_kf(x_{k-1})}{f(x_k)-f(x_{k-1})}-x^*, {\\nonumber}\n",
    "\\label{_auto4} \\tag{38}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9d1b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:sec3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\varepsilon_{k+1}=\\frac{\\varepsilon_{k-1}f(x_k)-\\varepsilon_k f(x_{k-1})}{f(x_k)-f(x_{k-1})},\n",
    "\\label{eq:nlin:sec3} \\tag{39}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39969ebc",
   "metadata": {
    "editable": true
   },
   "source": [
    "we now make a Taylor expansion of $f(x_k)$ and $f(x_{k-1})$ about the root $x^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e163466",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x_k) =f(x^*)+f^\\prime(x^*)(x_k-x^*)+\\frac{1}{2}f^{\\prime\\prime}(x^*)(x_k-x^*)^2+\\cdots ,{\\nonumber}\n",
    "\\label{_auto5} \\tag{40}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e84366",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "       =f^\\prime(x^*)\\varepsilon_k+\\frac{1}{2}f^{\\prime\\prime}(x^*)\\varepsilon_k^2+\\cdots .\n",
    "\\label{_auto6} \\tag{41}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8b189",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "f(x_{k-1}) =f(x^*)+f^\\prime(x^*)(x_{k-1}-x^*)+\\frac{1}{2}f^{\\prime\\prime}(x^*)(x_{k-1}-x^*)^2+\\cdots,{\\nonumber}\n",
    "\\label{_auto7} \\tag{42}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826c6cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "       =f^\\prime(x^*)\\varepsilon_{k-1}+\\frac{1}{2}f^{\\prime\\prime}(x^*)\\varepsilon_{k-1}^2+\\cdots ,\n",
    "\\label{_auto8} \\tag{43}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9633e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the fact that $f(x^*)=0$. Inserting these equations into equation ([39](#eq:nlin:sec3)) and neglecting terms of order $\\varepsilon_k^3$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fe19f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\varepsilon_{k+1}=\\frac{\\varepsilon_{k-1}\\left[f^\\prime(x^*)\\varepsilon_k+\\frac{1}{2}f^{\\prime\\prime}(x^*)\\varepsilon_k^2\\right] -\\varepsilon_k\\left[ f^\\prime(x^*)\\varepsilon_{k-1}+\\frac{1}{2}f^{\\prime\\prime}(x^*)\\varepsilon_{k-1}^2\\right]}{f^\\prime(x^*)\\varepsilon_k+\\frac{1}{2}f^{\\prime\\prime}(x^*)\\varepsilon_k^2-\\left[ f^\\prime(x^*)\\varepsilon_{k-1}+\\frac{1}{2}f^{\\prime\\prime}(x^*)\\varepsilon_{k-1}^2\\right]},{\\nonumber}\n",
    "\\label{_auto9} \\tag{44}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a0c81",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "=\\frac{\\varepsilon_k\\varepsilon_{k-1}\\left[\\varepsilon_k-\\varepsilon_{k-1}\\right]}{\\left[f^\\prime(x^*)+\\frac{1}{2}f^{\\prime\\prime}(x^*)(\\varepsilon_k+\\varepsilon_{k-1})\\right](\\varepsilon_k-\\varepsilon_{k-1})},{\\nonumber}\n",
    "\\label{_auto10} \\tag{45}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf74fd5",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:sec4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "=\\frac{f^{\\prime\\prime}(x^*)}{2f^\\prime(x^*)}\\varepsilon_k\\varepsilon_{k-1},\\label{eq:nlin:sec4} \\tag{46}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437ad20",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have neglected higher powers of $\\varepsilon$. We are searching for a solution of the form $\\varepsilon_{k+1}=K\\varepsilon_k^q$, $q$ is the rate of convergence. We can invert this equation to get $\\varepsilon_k=K^{-1/q}\\varepsilon_{k+1}^{1/q}$, or alternatively $\\varepsilon_{k-1}=K^{-1/q}\\varepsilon_{k}^{1/q}$ (just set $k\\to k-1$). Inserting these equations into equation ([46](#eq:nlin:sec4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b849d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:sec5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\varepsilon_k^q=\\frac{f^{\\prime\\prime}(x^*)}{2f^\\prime(x^*)}\\varepsilon_kK^{-1/q}\\varepsilon_{k}^{1/q}.\n",
    "\\label{eq:nlin:sec5} \\tag{47}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedc836",
   "metadata": {
    "editable": true
   },
   "source": [
    "Clearly, if this equation is to have a solution we must have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13b5df",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{f^{\\prime\\prime}(x^*)}{2f^\\prime(x^*)}K^{-1/q} =1{\\nonumber}\n",
    "\\label{_auto11} \\tag{48}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a32671",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\varepsilon_k^q=\\varepsilon_k\\varepsilon_{k}^{1/q}=\\varepsilon_{k}^{1+1/q},\n",
    "\\label{_auto12} \\tag{49}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ab0f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "or $q=1+1/q$. Solving this equation we get $q=(1\\pm\\sqrt{5})/2$, neglecting the negative solution, we find the rate of convergence for the secant method $q=(1+\\sqrt{5})/2\\simeq 1.618$.\n",
    "\n",
    "# Newton Rapson method\n",
    "The derivation of Newtons method, equation ([29](#eq:nlin:newton)), done in the previous section was based on [figure 5](#fig:nlin:newton). We will now derive it using a slightly different approach, but which lends itself easier to extend Newtons method to higher dimensions. The starting point is to expand the function around $x_k$, using Taylors formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b27ce",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:nt\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x)=f(x_k)+f^\\prime(x_k)(x-x_k) + \\cdots\\,.\n",
    "\\label{eq:nlin:nt} \\tag{50}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be59641",
   "metadata": {
    "editable": true
   },
   "source": [
    "Equation ([29](#eq:nlin:newton)) can be derived from equation ([50](#eq:nlin:nt)) by simply demanding that we keep the linear terms, and that the next point $x_{k+1}$ is located where the linear approximation intersects the $x$-axis, i.e. simply set $f(x)=0$, and $x=x_{k+1}$ in equation ([50](#eq:nlin:nt)).\n",
    "\n",
    "In higher order dimensions, we solve equation ([3](#eq:nlin:fvec)), and equation ([50](#eq:nlin:nt)) is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878369c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:ntd\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{f}(\\mathbf{x})=\\mathbf{f}(\\mathbf{x}_k)+ \\mathbf{J}(\\mathbf{x}_k)(\\mathbf{x}-\\mathbf{x}_k) + \\cdots\\,.\n",
    "\\label{eq:nlin:ntd} \\tag{51}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80af9d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$\\mathbf{J}(\\mathbf{x}_k)$ is the Jacobian. As before, we simply set  $\\mathbf{f}(\\mathbf{x})=\\mathbf{0}$, $\\mathbf{x}=\\mathbf{x}_{k+1}$, and keep the linear terms, hence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9855131",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:ntd2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{k+1}=\\mathbf{x}_k-\\mathbf{J}^{-1}(\\mathbf{x}_k)\\mathbf{f}(\\mathbf{x}_k). \n",
    "\\label{eq:nlin:ntd2} \\tag{52}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94172e50",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make the mathematics a bit more clear, let us specify to $2D$. Assume that\n",
    "$\\mathbf{f}(\\mathbf{x})=[f_x(x,y),f_y(x,y)]$, then the Jacobian is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21567cdc",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:jac\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{J}(\\mathbf{x}_k)=\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial f_x}{\\partial x}&\\frac{\\partial f_x}{\\partial y}\\\\ \n",
    "\\frac{\\partial f_y}{\\partial x}&\\frac{\\partial f_y}{\\partial y}\n",
    "\\end{array}\n",
    "\\right).\n",
    "\\label{eq:nlin:jac} \\tag{53}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2bc4e",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Gradient Descent\n",
    "This method used is to minimize functions (does not work for root finding). In many nonlinear problems, we would like to minimize (or maximize) a function. An ideal 2D example is shown in [figure 9](#fig:nlin:grad). The algorithm moves in the direction of steepest descent. Note that the step size might change towards the search. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/steepest_descent.png, width=400 frac=1.0] A very simple example of the gradient descent method. <div id=\"fig:nlin:grad\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:grad\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/steepest_descent.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 9: A very simple example of the gradient descent method.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "Assume that we have a function $\\mathbf{f}(\\mathbf{x})$, that we would like to minimize. The gradient descent algorithm is simply to update parameters according to the derivative (gradient) of $\\mathbf{f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eacbbc",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:stpdc\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\gamma\\nabla\\mathbf{f}.\n",
    "\\label{eq:nlin:stpdc} \\tag{54}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35988de0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$\\gamma$ is the learning rate, and a good choice of $\\gamma$ is important. $\\gamma$ might also change from one iteration to the other, and does not have to be constant.  \n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 6: Gradient descent solution of linear regression\n",
    "\n",
    "A very typical example is if we have a model and we would like to fit some parameters of the model to a data set (e.g. linear regression). Assume that we have observations $(x_i,y_i)$ and model predictions $f(x_i,\\mathbf{\\beta})$, the model parameters are contained in the vector $\\mathbf{\\beta}$. The *least square*, $S$, is the square of the sum of all the *residuals*, i.e. the difference between the observations and model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0da17",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:lsq\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "S=\\sum_i(y_i-f(x_i,\\mathbf{\\beta}))^2.\n",
    "\\label{eq:nlin:lsq} \\tag{55}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0084d2c",
   "metadata": {
    "editable": true
   },
   "source": [
    "Specializing to linear regression, we choose the model to be linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323b052",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:lin\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x_i,\\mathbf{\\beta})=b_0+b_1x_i.\n",
    "\\label{eq:nlin:lin} \\tag{56}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14acbf7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Equation ([55](#eq:nlin:lsq)) now takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f322e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:lsq2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "S=\\sum_i(y_i-b_0+b_1x_i)^2.\n",
    "\\label{eq:nlin:lsq2} \\tag{57}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683bfb04",
   "metadata": {
    "editable": true
   },
   "source": [
    "The gradients are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e8e9b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S}{\\partial b_0}=-2\\sum_i(y_i-b_0+b_1x_i),{\\nonumber}\n",
    "\\label{_auto13} \\tag{58}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb94a2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:dlsq\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{\\partial S}{\\partial b_1}=-2\\sum_i(y_i-b_0+b_1x_i)x_i,.\n",
    "\\label{eq:nlin:dlsq} \\tag{59}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f064c",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Implement the gradient descent method using a constant learning rate of $10^{-3}$, to minimize the least square function\n",
    "\n",
    "* Test the linear regression on the data set $x_i=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]$, and $y=[1, 3, 2, 5, 7, 8, 8, 9, 10, 12]$, choose a starting value $(b_0,b_1)=(0,0)$. What happens if you increase the learning rate?\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "Below is an implementation of the gradient descent method with a constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2cf45ce",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f,x,df, g=.001, prec=1e-8,MAXIT=10):\n",
    "    '''Minimize f(x) by gradient descent.\n",
    "    f   : min(f(x))\n",
    "    x   : starting point \n",
    "    df  : derivative of f(x)\n",
    "    g   : learning rate\n",
    "    prec: desired precision\n",
    "    \n",
    "    Returns x when it is closer than eps to the root, \n",
    "    unless MAXIT are not exceeded\n",
    "    '''\n",
    "    x_old = x\n",
    "    for n in range(MAXIT):\n",
    "        plot_regression_line(x_old)  \n",
    "        x_new = x_old - g*df(x_old)\n",
    "        if(abs(np.max(x_new-x_old))<prec):\n",
    "            print('Found solution:', x_new, \n",
    "                  ', after:', n, 'iterations.' )\n",
    "            return x_new\n",
    "        x_old=x_new\n",
    "    print('Max number of iterations: ', MAXIT, ' reached.') \n",
    "    print('Try to increase MAXIT or decrease prec')\n",
    "    print('Returning best guess, value of function is: ', f(x_new))\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd4f1f",
   "metadata": {
    "editable": true
   },
   "source": [
    "The linear regression is implemented as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643c1d2c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_obs_ = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
    "y_obs_ = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12]) \n",
    "def plot_regression_line(b,x=x_obs_, y=y_obs_): \n",
    "    global N_\n",
    "    # plotting the actual points as scatter plot \n",
    "    plt.scatter(x, y, color = \"m\", \n",
    "               marker = \"o\", s = 30,label=\"data\") \n",
    "  \n",
    "    # predicted response vector \n",
    "    y_pred = b[0] + b[1]*x\n",
    "  \n",
    "    # plotting the regression line\n",
    "    if(len(b)>1):\n",
    "#        plt.plot(x, y_pred, color = \"g\", label = \"R-squared = {0:.3f}\".format(b[2]))\n",
    "        plt.plot(x, y_pred, color = \"g\", label = \"iteration:\" + str(N_) +\", (b[0],b[1])= ({0:.3f}\".format(b[0]) + \", {0:.3f})\".format(b[1]))\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(x, y_pred, color = \"g\")\n",
    "  \n",
    "    # putting labels \n",
    "    plt.xlabel('x') \n",
    "    plt.ylabel('y') \n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "#    plt.savefig('../fig-nlin/stdec'+str(N_)+'.png', bbox_inches='tight',transparent=True)\n",
    "    N_=N_+1  \n",
    "    # function to show plot \n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def Jacobian(x,f,dx=1e-5):\n",
    "    N=len(x)\n",
    "    x0=np.copy(x)\n",
    "    f0=f(x)\n",
    "    J=np.zeros(shape=(N,N))\n",
    "    for j in range(N):\n",
    "        x[j] = x[j] +  dx\n",
    "        for i in range(N):   \n",
    "            J[i][j] = (f(x)[i]-f0[i])/dx\n",
    "        x[j] = x[j] -  dx\n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def newton_rapson(x,f,J=None, jacobian=False, prec=1e-8,MAXIT=100):\n",
    "    '''Approximate solution of f(x)=0 by Newtons method.\n",
    "    The derivative of the function is calculated numerically\n",
    "    f   : f(x)=0.\n",
    "    J   : Jacobian\n",
    "    x   : starting point  \n",
    "    eps : desired precision\n",
    "    \n",
    "    Returns x when it is closer than eps to the root, \n",
    "    unless MAX_ITERATIONS are not exceeded\n",
    "    '''\n",
    "    MAX_ITERATIONS=MAXIT\n",
    "    x_old = np.copy(x)\n",
    "    for n in range(MAX_ITERATIONS):\n",
    "        plot_regression_line(x_old) \n",
    "        if not jacobian:\n",
    "            J_=Jacobian(x_old,f)\n",
    "        else:\n",
    "            J_=J(x_old)\n",
    "        z=np.linalg.solve(J_,-f(x_old))\n",
    "        x_new=x_old+z\n",
    "        if(np.sum(abs(x_new-x_old))<prec):\n",
    "            print('Found solution:', x_new, \n",
    "                  ', after:', n, 'iterations.' )\n",
    "            return x_new\n",
    "        x_old=np.copy(x_new)\n",
    "    print('Max number of iterations: ', MAXIT, ' reached.') \n",
    "    print('Try to increase MAXIT or decrease prec')\n",
    "    print('Returning best guess, value of function is: ', f(x_new))\n",
    "    return x_new\n",
    "\n",
    "\n",
    "def gradient_descent(f,x,df, g=.001, prec=1e-8,MAXIT=10):\n",
    "    '''Minimize f(x) by gradient descent.\n",
    "    f   : min(f(x))\n",
    "    x   : starting point \n",
    "    df  : derivative of f(x)\n",
    "    g   : learning rate\n",
    "    prec: desired precision\n",
    "    \n",
    "    Returns x when it is closer than eps to the root, \n",
    "    unless MAXIT are not exceeded\n",
    "    '''\n",
    "    x_old = x\n",
    "    for n in range(MAXIT):\n",
    "        plot_regression_line(x_old)  \n",
    "        x_new = x_old - g*df(x_old)\n",
    "        if(abs(np.max(x_new-x_old))<prec):\n",
    "            print('Found solution:', x_new, \n",
    "                  ', after:', n, 'iterations.' )\n",
    "            return x_new\n",
    "        x_old=x_new\n",
    "    print('Max number of iterations: ', MAXIT, ' reached.') \n",
    "    print('Try to increase MAXIT or decrease prec')\n",
    "    print('Returning best guess, value of function is: ', f(x_new))\n",
    "    return x_new\n",
    "#end\n",
    "\n",
    "def S(b,x=x_obs_,y=y_obs_):\n",
    "    return np.sum((y-b[0]-b[1]*x)**2)\n",
    "\n",
    "def dS(b,x=x_obs_,y=y_obs_):\n",
    "    return np.array([-2*np.sum(y-b[0]-b[1]*x),\n",
    "                     -2*np.sum((y-b[0]-b[1]*x)*x)])\n",
    "\n",
    "def J(b,x=x_obs_,y=y_obs_):\n",
    "    N=len(b)\n",
    "    J=np.zeros(shape=(N,N))\n",
    "    xs=np.sum(x)\n",
    "    J[0][0]=2*len(x)\n",
    "    J[0][1]=2*xs\n",
    "    J[1][0]=2*xs\n",
    "    J[1][1]=2*np.sum(x*x)\n",
    "    return J\n",
    "N_=0\n",
    "print('Gradient ')\n",
    "b=np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cc18e",
   "metadata": {
    "editable": true
   },
   "source": [
    "The first four iterations are shown in [figure 10](#fig:nlin:grsc). If we choose a learning rate that is too high, we will move past the minimum, and the solution will oscillate. This can be avoided by lowering the learning rate as we iterate, by e.g. replacing `g` with `g/(n+1)` in the implementation above.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-nlin/stdec_comb.png, width=400 frac=1.0] First four iterations of the gradient descent solution of linear regression. <div id=\"fig:nlin:grsc\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:grsc\"></div>\n",
    "\n",
    "<img src=\"fig-nlin/stdec_comb.png\" width=400><p style=\"font-size: 0.9em\"><i>Figure 10: First four iterations of the gradient descent solution of linear regression.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "# Other Useful Methods\n",
    "\n",
    "In this chapter we have covered the *basic*, but you should now be well equipped to dive into other methods. We highly recommend [[press2001]](#press2001) as a starting point, although the code examples are written in C++, the theory is presented in a very accurate, but informal way.\n",
    "* Brents method:  uses root bracketing, bisection, and inverse quadratic interpolation. The 1D method of choice if the function and not its derivative is known\n",
    "\n",
    "# References\n",
    "\n",
    "1. <div id=\"press2001\"></div> **W. H. Press, W. T. Vetterling, S. A. Teukolsky and B. P. Flannery**.  *Numerical Recipes in C++: the Art of Scientific Computing*, 2nd edition, Cambridge University Press, 2002.\n",
    "\n",
    "2. <div id=\"peng1976new\"></div> **D.-Y. Peng and D. B. Robinson**.  A New Two-Constant Equation of State, *Industrial & Engineering Chemistry Fundamentals*, 15(1), pp. 59-64, 1976.\n",
    "\n",
    "3. <div id=\"newman2013\"></div> **M. Newman**.  *Computational Physics*, CreateSpace Independent Publ., 2013."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
